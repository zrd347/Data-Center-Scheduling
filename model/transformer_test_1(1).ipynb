{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T07:56:50.089791300Z",
     "start_time": "2024-07-14T07:56:50.074843Z"
    }
   },
   "id": "5e23e1d749eb07b4"
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T07:59:23.239620500Z",
     "start_time": "2024-07-14T07:59:23.215489200Z"
    }
   },
   "id": "9cea3091a46a2bac"
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, nhead, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(input_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(input_dim, nhead, dim_feedforward=512)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.encoder = nn.Linear(input_dim, input_dim)\n",
    "        self.decoder = nn.Linear(input_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, src):\n",
    "        # print(f\"Initial shape: {src.shape}\")\n",
    "        # 预期的输入形状为 (batch_size, sequence_length, input_dim)\n",
    "        batch_size, sequence_length, input_dim = src.size()\n",
    "\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        # output = torch.relu(self.decoder(output))\n",
    "        # 调整回 (batch_size, 240, 200, input_dim, 2, 60)\n",
    "        output = output.permute(1, 0, 2).contiguous()\n",
    "        output = output.view(batch_size, 200, input_dim, 2, 60)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "    \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T07:59:25.087466Z",
     "start_time": "2024-07-14T07:59:25.062609700Z"
    }
   },
   "id": "485621554cd3718e"
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[:len(div_term)//2])  # 修改这一行，确保 div_term 维度正确\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T07:59:25.820683Z",
     "start_time": "2024-07-14T07:59:25.794572200Z"
    }
   },
   "id": "bd4b057cf36f36f"
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "def int_to_array(value):\n",
    "    mapping = {\n",
    "        0: [0, 0, 0, 0, 0],\n",
    "        1: [1, 0, 0, 0, 0],\n",
    "        2: [1, 1, 0, 0, 0],\n",
    "        3: [1, 1, 1, 0, 0],\n",
    "        4: [1, 1, 1, 1, 0],\n",
    "        5: [1, 1, 1, 1, 1]\n",
    "    }\n",
    "    return mapping.get(value, [0, 0, 0, 0, 0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T07:59:27.488429300Z",
     "start_time": "2024-07-14T07:59:27.466519900Z"
    }
   },
   "id": "49de0d0bd99bf8f7"
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10752\\678390873.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  in_seq = np.array([int_to_array(int(row[i])) for i in range(1, 1 + in_seq_length)])  # 控制每个in_seq的长度\n"
     ]
    }
   ],
   "source": [
    "in_seq_length = 1000\n",
    "df = pd.read_csv('in_seq.csv')\n",
    "in_seqs = []\n",
    "for idx, row in df.iterrows():\n",
    "    in_seq = np.array([int_to_array(int(row[i])) for i in range(1, 1 + in_seq_length)])  # 控制每个in_seq的长度\n",
    "    in_seqs.append(in_seq)\n",
    "in_seqs = np.array(in_seqs)\n",
    "in_seqs = np.transpose(in_seqs, (1, 0, 2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T07:59:45.880677400Z",
     "start_time": "2024-07-14T07:59:41.540527700Z"
    }
   },
   "id": "380b28e1c6bda59d"
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 200, 5)\n"
     ]
    }
   ],
   "source": [
    "print(in_seqs.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T07:59:57.383768Z",
     "start_time": "2024-07-14T07:59:57.368622700Z"
    }
   },
   "id": "217d32c60c6e681b"
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "def load_and_pad_npy(file_path, target_shape):\n",
    "    if os.path.exists(file_path):\n",
    "        loaded_data = np.load(file_path)\n",
    "        padded_data = np.zeros(target_shape)\n",
    "        padded_data[:loaded_data.shape[0], :, :] = loaded_data\n",
    "        return padded_data\n",
    "    else:\n",
    "        return np.zeros(target_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T07:59:58.254460900Z",
     "start_time": "2024-07-14T07:59:58.233550400Z"
    }
   },
   "id": "faad09b15aad5a67"
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 200, 5, 2, 60)\n"
     ]
    }
   ],
   "source": [
    "out_seq_length = 1000\n",
    "out_seqs = []\n",
    "for i in range(out_seq_length):\n",
    "    file_path = f\"Npy_Output/result_{i+1:03}.npy\"\n",
    "    target_shape = (600, 2, 60)\n",
    "    data = load_and_pad_npy(file_path, target_shape)\n",
    "    order = 0\n",
    "    out_seq = []\n",
    "    # 根据 in_seqs中的值决定加载的数据\n",
    "    for j in range(200):\n",
    "        tmp_seq = []\n",
    "        for k in range(5):\n",
    "            tmp = in_seqs[i, j, k]\n",
    "            if tmp == 1:\n",
    "                # tmp_seq.append(torch.tensor(data[order]))\n",
    "                tmp_seq.append(data[order])\n",
    "                order = order + 1\n",
    "            else:\n",
    "                # tmp_seq.append(torch.tensor(np.zeros((2, 60))))\n",
    "                tmp_seq.append(np.zeros((2, 60)))\n",
    "        out_seq.append(tmp_seq)\n",
    "    out_seqs.append(out_seq)\n",
    "# out_seqs = np.array(out_seqs, dtype=object)\n",
    "out_seqs = np.array(out_seqs)\n",
    "print(out_seqs.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T08:00:20.951980900Z",
     "start_time": "2024-07-14T07:59:59.962565100Z"
    }
   },
   "id": "67ecae793bcd43a5"
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 800\n",
      "Validation set size: 200\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集为训练集、验证集和测试集\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(in_seqs, out_seqs, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "# 转换为 torch 张量\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "Y_val = torch.tensor(Y_val, dtype=torch.float32)\n",
    "# X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "# Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "# print(f\"Test set size: {len(X_test)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T08:00:35.852448100Z",
     "start_time": "2024-07-14T08:00:35.340659900Z"
    }
   },
   "id": "b51a75043d28660"
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\ECE6258\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "input_dim = 5\n",
    "output_dim = 5 * 2 * 60\n",
    "nhead = 5\n",
    "num_layers = 2\n",
    "lr = 0.001\n",
    "epochs = 200\n",
    "\n",
    "model = TransformerModel(input_dim, output_dim, nhead, num_layers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T12:41:08.501107Z",
     "start_time": "2024-07-22T12:41:08.403048400Z"
    }
   },
   "id": "21117b7a5b983913"
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "# # 定义损失函数和优化器\n",
    "# criterion = nn.MSELoss()\n",
    "# \n",
    "# def custom_loss(output, target):\n",
    "#     mse_loss = torch.mean((output - target)**2)  # 计算均方误差\n",
    "#     complement_product_sum = torch.mean(target * (1 - target))  # 计算目标张量中每个元素与其补数的乘积的总和\n",
    "#     weighted_loss = mse_loss + complement_product_sum\n",
    "#     return weighted_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T12:41:08.551139200Z",
     "start_time": "2024-07-22T12:41:08.442041Z"
    }
   },
   "id": "e65ff146e1877e48"
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, zero_weight=0.1, non_zero_weight=50):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.zero_weight = zero_weight\n",
    "        self.non_zero_weight = non_zero_weight\n",
    "        self.bce_criterion = nn.BCELoss(reduction='none')\n",
    "        self.mse_criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        bce_loss = self.bce_criterion(outputs, targets)\n",
    "        weights = torch.where(targets == 0, self.zero_weight, self.non_zero_weight)\n",
    "        weighted_bce_loss = bce_loss * weights\n",
    "        bce_loss_mean = weighted_bce_loss.sum()\n",
    "\n",
    "        mse_loss = self.mse_criterion(outputs, targets)\n",
    "\n",
    "        # Calculate sum of (target * (1 - target)) and its mean\n",
    "        sum_target_times_one_minus_target = 20 * torch.sum(outputs * (1 - outputs))\n",
    "\n",
    "        # Combine the losses\n",
    "        final_loss = bce_loss_mean + mse_loss + sum_target_times_one_minus_target\n",
    "\n",
    "        return final_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T12:41:32.589719300Z",
     "start_time": "2024-07-22T12:41:32.356784800Z"
    }
   },
   "id": "f6a29341ebee0e85"
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "zero_weight = 0.1 # 负样本的权重\n",
    "non_zero_weight = 50 # 正样本的权重\n",
    "criterion = CustomLoss(zero_weight, non_zero_weight)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T12:41:35.247809100Z",
     "start_time": "2024-07-22T12:41:35.217427200Z"
    }
   },
   "id": "dbd5737980f60706"
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T12:41:36.120184600Z",
     "start_time": "2024-07-22T12:41:36.060438600Z"
    }
   },
   "id": "d7491f3f1b0add0a"
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 459257024.0000, Val Loss: 114558128.0000\n",
      "Epoch [2/200], Train Loss: 458289024.0000, Val Loss: 114300584.0000\n",
      "Epoch [3/200], Train Loss: 457315200.0000, Val Loss: 114045168.0000\n",
      "Epoch [4/200], Train Loss: 456350976.0000, Val Loss: 113792800.0000\n",
      "Epoch [5/200], Train Loss: 455394528.0000, Val Loss: 113545240.0000\n",
      "Epoch [6/200], Train Loss: 454441440.0000, Val Loss: 113304920.0000\n",
      "Epoch [7/200], Train Loss: 453512000.0000, Val Loss: 113075512.0000\n",
      "Epoch [8/200], Train Loss: 452598592.0000, Val Loss: 112857224.0000\n",
      "Epoch [9/200], Train Loss: 451720480.0000, Val Loss: 112648984.0000\n",
      "Epoch [10/200], Train Loss: 450884640.0000, Val Loss: 112447592.0000\n",
      "Epoch [11/200], Train Loss: 450067168.0000, Val Loss: 112247848.0000\n",
      "Epoch [12/200], Train Loss: 449275104.0000, Val Loss: 112047240.0000\n",
      "Epoch [13/200], Train Loss: 448479008.0000, Val Loss: 111844880.0000\n",
      "Epoch [14/200], Train Loss: 447681760.0000, Val Loss: 111640432.0000\n",
      "Epoch [15/200], Train Loss: 446879552.0000, Val Loss: 111433664.0000\n",
      "Epoch [16/200], Train Loss: 446066880.0000, Val Loss: 111224232.0000\n",
      "Epoch [17/200], Train Loss: 445244992.0000, Val Loss: 111011848.0000\n",
      "Epoch [18/200], Train Loss: 444411008.0000, Val Loss: 110796312.0000\n",
      "Epoch [19/200], Train Loss: 443571392.0000, Val Loss: 110577648.0000\n",
      "Epoch [20/200], Train Loss: 442714560.0000, Val Loss: 110355968.0000\n",
      "Epoch [21/200], Train Loss: 441843744.0000, Val Loss: 110131456.0000\n",
      "Epoch [22/200], Train Loss: 440962176.0000, Val Loss: 109904072.0000\n",
      "Epoch [23/200], Train Loss: 440070432.0000, Val Loss: 109673752.0000\n",
      "Epoch [24/200], Train Loss: 439160032.0000, Val Loss: 109440296.0000\n",
      "Epoch [25/200], Train Loss: 438231904.0000, Val Loss: 109203672.0000\n",
      "Epoch [26/200], Train Loss: 437297280.0000, Val Loss: 108963920.0000\n",
      "Epoch [27/200], Train Loss: 436346784.0000, Val Loss: 108721072.0000\n",
      "Epoch [28/200], Train Loss: 435377184.0000, Val Loss: 108474952.0000\n",
      "Epoch [29/200], Train Loss: 434404256.0000, Val Loss: 108225152.0000\n",
      "Epoch [30/200], Train Loss: 433405024.0000, Val Loss: 107971288.0000\n",
      "Epoch [31/200], Train Loss: 432391264.0000, Val Loss: 107713480.0000\n",
      "Epoch [32/200], Train Loss: 431364416.0000, Val Loss: 107452192.0000\n",
      "Epoch [33/200], Train Loss: 430320704.0000, Val Loss: 107187920.0000\n",
      "Epoch [34/200], Train Loss: 429259232.0000, Val Loss: 106920544.0000\n",
      "Epoch [35/200], Train Loss: 428182720.0000, Val Loss: 106649760.0000\n",
      "Epoch [36/200], Train Loss: 427085472.0000, Val Loss: 106375440.0000\n",
      "Epoch [37/200], Train Loss: 425968608.0000, Val Loss: 106097736.0000\n",
      "Epoch [38/200], Train Loss: 424839840.0000, Val Loss: 105816472.0000\n",
      "Epoch [39/200], Train Loss: 423694976.0000, Val Loss: 105531248.0000\n",
      "Epoch [40/200], Train Loss: 422519104.0000, Val Loss: 105242136.0000\n",
      "Epoch [41/200], Train Loss: 421344288.0000, Val Loss: 104949016.0000\n",
      "Epoch [42/200], Train Loss: 420142592.0000, Val Loss: 104652104.0000\n",
      "Epoch [43/200], Train Loss: 418939680.0000, Val Loss: 104351888.0000\n",
      "Epoch [44/200], Train Loss: 417722528.0000, Val Loss: 104048032.0000\n",
      "Epoch [45/200], Train Loss: 416497920.0000, Val Loss: 103741632.0000\n",
      "Epoch [46/200], Train Loss: 415272224.0000, Val Loss: 103429688.0000\n",
      "Epoch [47/200], Train Loss: 414034304.0000, Val Loss: 103114376.0000\n",
      "Epoch [48/200], Train Loss: 412786304.0000, Val Loss: 102796496.0000\n",
      "Epoch [49/200], Train Loss: 411518592.0000, Val Loss: 102474520.0000\n",
      "Epoch [50/200], Train Loss: 410228704.0000, Val Loss: 102147816.0000\n",
      "Epoch [51/200], Train Loss: 408915200.0000, Val Loss: 101818232.0000\n",
      "Epoch [52/200], Train Loss: 407590592.0000, Val Loss: 101485168.0000\n",
      "Epoch [53/200], Train Loss: 406241984.0000, Val Loss: 101147880.0000\n",
      "Epoch [54/200], Train Loss: 404893440.0000, Val Loss: 100807144.0000\n",
      "Epoch [55/200], Train Loss: 403531392.0000, Val Loss: 100462440.0000\n",
      "Epoch [56/200], Train Loss: 402153120.0000, Val Loss: 100114328.0000\n",
      "Epoch [57/200], Train Loss: 400768512.0000, Val Loss: 99762592.0000\n",
      "Epoch [58/200], Train Loss: 399364928.0000, Val Loss: 99407448.0000\n",
      "Epoch [59/200], Train Loss: 397943264.0000, Val Loss: 99048568.0000\n",
      "Epoch [60/200], Train Loss: 396508224.0000, Val Loss: 98686048.0000\n",
      "Epoch [61/200], Train Loss: 395051168.0000, Val Loss: 98320208.0000\n",
      "Epoch [62/200], Train Loss: 393584608.0000, Val Loss: 97950816.0000\n",
      "Epoch [63/200], Train Loss: 392104320.0000, Val Loss: 97578016.0000\n",
      "Epoch [64/200], Train Loss: 390607392.0000, Val Loss: 97201664.0000\n",
      "Epoch [65/200], Train Loss: 389100224.0000, Val Loss: 96822016.0000\n",
      "Epoch [66/200], Train Loss: 387585280.0000, Val Loss: 96438936.0000\n",
      "Epoch [67/200], Train Loss: 386054752.0000, Val Loss: 96052240.0000\n",
      "Epoch [68/200], Train Loss: 384504512.0000, Val Loss: 95662400.0000\n",
      "Epoch [69/200], Train Loss: 382945696.0000, Val Loss: 95269352.0000\n",
      "Epoch [70/200], Train Loss: 381367552.0000, Val Loss: 94872752.0000\n",
      "Epoch [71/200], Train Loss: 379776768.0000, Val Loss: 94473168.0000\n",
      "Epoch [72/200], Train Loss: 378177280.0000, Val Loss: 94070376.0000\n",
      "Epoch [73/200], Train Loss: 376565376.0000, Val Loss: 93664368.0000\n",
      "Epoch [74/200], Train Loss: 374943136.0000, Val Loss: 93255448.0000\n",
      "Epoch [75/200], Train Loss: 373305152.0000, Val Loss: 92843328.0000\n",
      "Epoch [76/200], Train Loss: 371654720.0000, Val Loss: 92428176.0000\n",
      "Epoch [77/200], Train Loss: 369991392.0000, Val Loss: 92010240.0000\n",
      "Epoch [78/200], Train Loss: 368315936.0000, Val Loss: 91589360.0000\n",
      "Epoch [79/200], Train Loss: 366630368.0000, Val Loss: 91165568.0000\n",
      "Epoch [80/200], Train Loss: 364936384.0000, Val Loss: 90739216.0000\n",
      "Epoch [81/200], Train Loss: 363232000.0000, Val Loss: 90309944.0000\n",
      "Epoch [82/200], Train Loss: 361508992.0000, Val Loss: 89878296.0000\n",
      "Epoch [83/200], Train Loss: 359785088.0000, Val Loss: 89443704.0000\n",
      "Epoch [84/200], Train Loss: 358041472.0000, Val Loss: 89007072.0000\n",
      "Epoch [85/200], Train Loss: 356292000.0000, Val Loss: 88567368.0000\n",
      "Epoch [86/200], Train Loss: 354529312.0000, Val Loss: 88125768.0000\n",
      "Epoch [87/200], Train Loss: 352760992.0000, Val Loss: 87681504.0000\n",
      "Epoch [88/200], Train Loss: 350984224.0000, Val Loss: 87235168.0000\n",
      "Epoch [89/200], Train Loss: 349194560.0000, Val Loss: 86786872.0000\n",
      "Epoch [90/200], Train Loss: 347399744.0000, Val Loss: 86336216.0000\n",
      "Epoch [91/200], Train Loss: 345596768.0000, Val Loss: 85883816.0000\n",
      "Epoch [92/200], Train Loss: 343782880.0000, Val Loss: 85429104.0000\n",
      "Epoch [93/200], Train Loss: 341964608.0000, Val Loss: 84972880.0000\n",
      "Epoch [94/200], Train Loss: 340137120.0000, Val Loss: 84514664.0000\n",
      "Epoch [95/200], Train Loss: 338305088.0000, Val Loss: 84054936.0000\n",
      "Epoch [96/200], Train Loss: 336460608.0000, Val Loss: 83593544.0000\n",
      "Epoch [97/200], Train Loss: 334614720.0000, Val Loss: 83130832.0000\n",
      "Epoch [98/200], Train Loss: 332755680.0000, Val Loss: 82666592.0000\n",
      "Epoch [99/200], Train Loss: 330904640.0000, Val Loss: 82201408.0000\n",
      "Epoch [100/200], Train Loss: 329039584.0000, Val Loss: 81734192.0000\n",
      "Epoch [101/200], Train Loss: 327169024.0000, Val Loss: 81265960.0000\n",
      "Epoch [102/200], Train Loss: 325295168.0000, Val Loss: 80796648.0000\n",
      "Epoch [103/200], Train Loss: 323416256.0000, Val Loss: 80326064.0000\n",
      "Epoch [104/200], Train Loss: 321532032.0000, Val Loss: 79855056.0000\n",
      "Epoch [105/200], Train Loss: 319639776.0000, Val Loss: 79382704.0000\n",
      "Epoch [106/200], Train Loss: 317751936.0000, Val Loss: 78910104.0000\n",
      "Epoch [107/200], Train Loss: 315860128.0000, Val Loss: 78437072.0000\n",
      "Epoch [108/200], Train Loss: 313965376.0000, Val Loss: 77962856.0000\n",
      "Epoch [109/200], Train Loss: 312068960.0000, Val Loss: 77488208.0000\n",
      "Epoch [110/200], Train Loss: 310166560.0000, Val Loss: 77013640.0000\n",
      "Epoch [111/200], Train Loss: 308268448.0000, Val Loss: 76538816.0000\n",
      "Epoch [112/200], Train Loss: 306363168.0000, Val Loss: 76063576.0000\n",
      "Epoch [113/200], Train Loss: 304462496.0000, Val Loss: 75587968.0000\n",
      "Epoch [114/200], Train Loss: 302560160.0000, Val Loss: 75112808.0000\n",
      "Epoch [115/200], Train Loss: 300656672.0000, Val Loss: 74637960.0000\n",
      "Epoch [116/200], Train Loss: 298759456.0000, Val Loss: 74162864.0000\n",
      "Epoch [117/200], Train Loss: 296855552.0000, Val Loss: 73688072.0000\n",
      "Epoch [118/200], Train Loss: 294950944.0000, Val Loss: 73213824.0000\n",
      "Epoch [119/200], Train Loss: 293056896.0000, Val Loss: 72740064.0000\n",
      "Epoch [120/200], Train Loss: 291155456.0000, Val Loss: 72266824.0000\n",
      "Epoch [121/200], Train Loss: 289261024.0000, Val Loss: 71794056.0000\n",
      "Epoch [122/200], Train Loss: 287375872.0000, Val Loss: 71322208.0000\n",
      "Epoch [123/200], Train Loss: 285485184.0000, Val Loss: 70851112.0000\n",
      "Epoch [124/200], Train Loss: 283593760.0000, Val Loss: 70380848.0000\n",
      "Epoch [125/200], Train Loss: 281717408.0000, Val Loss: 69911552.0000\n",
      "Epoch [126/200], Train Loss: 279830720.0000, Val Loss: 69443456.0000\n",
      "Epoch [127/200], Train Loss: 277958592.0000, Val Loss: 68976600.0000\n",
      "Epoch [128/200], Train Loss: 276090240.0000, Val Loss: 68510776.0000\n",
      "Epoch [129/200], Train Loss: 274223808.0000, Val Loss: 68046288.0000\n",
      "Epoch [130/200], Train Loss: 272364672.0000, Val Loss: 67583232.0000\n",
      "Epoch [131/200], Train Loss: 270511904.0000, Val Loss: 67121720.0000\n",
      "Epoch [132/200], Train Loss: 268666304.0000, Val Loss: 66661768.0000\n",
      "Epoch [133/200], Train Loss: 266819808.0000, Val Loss: 66203424.0000\n",
      "Epoch [134/200], Train Loss: 264983744.0000, Val Loss: 65746756.0000\n",
      "Epoch [135/200], Train Loss: 263152880.0000, Val Loss: 65291932.0000\n",
      "Epoch [136/200], Train Loss: 261335808.0000, Val Loss: 64838888.0000\n",
      "Epoch [137/200], Train Loss: 259519552.0000, Val Loss: 64387776.0000\n",
      "Epoch [138/200], Train Loss: 257710224.0000, Val Loss: 63938620.0000\n",
      "Epoch [139/200], Train Loss: 255914464.0000, Val Loss: 63491568.0000\n",
      "Epoch [140/200], Train Loss: 254122688.0000, Val Loss: 63046684.0000\n",
      "Epoch [141/200], Train Loss: 252342496.0000, Val Loss: 62603940.0000\n",
      "Epoch [142/200], Train Loss: 250573872.0000, Val Loss: 62163492.0000\n",
      "Epoch [143/200], Train Loss: 248805680.0000, Val Loss: 61725296.0000\n",
      "Epoch [144/200], Train Loss: 247051936.0000, Val Loss: 61289532.0000\n",
      "Epoch [145/200], Train Loss: 245302608.0000, Val Loss: 60856104.0000\n",
      "Epoch [146/200], Train Loss: 243571456.0000, Val Loss: 60425112.0000\n",
      "Epoch [147/200], Train Loss: 241846224.0000, Val Loss: 59996652.0000\n",
      "Epoch [148/200], Train Loss: 240134320.0000, Val Loss: 59570788.0000\n",
      "Epoch [149/200], Train Loss: 238423552.0000, Val Loss: 59147584.0000\n",
      "Epoch [150/200], Train Loss: 236728224.0000, Val Loss: 58727116.0000\n",
      "Epoch [151/200], Train Loss: 235045248.0000, Val Loss: 58309332.0000\n",
      "Epoch [152/200], Train Loss: 233372864.0000, Val Loss: 57894264.0000\n",
      "Epoch [153/200], Train Loss: 231708400.0000, Val Loss: 57482028.0000\n",
      "Epoch [154/200], Train Loss: 230052448.0000, Val Loss: 57072600.0000\n",
      "Epoch [155/200], Train Loss: 228417328.0000, Val Loss: 56666140.0000\n",
      "Epoch [156/200], Train Loss: 226786880.0000, Val Loss: 56262460.0000\n",
      "Epoch [157/200], Train Loss: 225173840.0000, Val Loss: 55861776.0000\n",
      "Epoch [158/200], Train Loss: 223567536.0000, Val Loss: 55464088.0000\n",
      "Epoch [159/200], Train Loss: 221969888.0000, Val Loss: 55069448.0000\n",
      "Epoch [160/200], Train Loss: 220392128.0000, Val Loss: 54677876.0000\n",
      "Epoch [161/200], Train Loss: 218824848.0000, Val Loss: 54289312.0000\n",
      "Epoch [162/200], Train Loss: 217268256.0000, Val Loss: 53903892.0000\n",
      "Epoch [163/200], Train Loss: 215724544.0000, Val Loss: 53521552.0000\n",
      "Epoch [164/200], Train Loss: 214193856.0000, Val Loss: 53142400.0000\n",
      "Epoch [165/200], Train Loss: 212673488.0000, Val Loss: 52766320.0000\n",
      "Epoch [166/200], Train Loss: 211164640.0000, Val Loss: 52393448.0000\n",
      "Epoch [167/200], Train Loss: 209670112.0000, Val Loss: 52023808.0000\n",
      "Epoch [168/200], Train Loss: 208190544.0000, Val Loss: 51657388.0000\n",
      "Epoch [169/200], Train Loss: 206723808.0000, Val Loss: 51294168.0000\n",
      "Epoch [170/200], Train Loss: 205269696.0000, Val Loss: 50934160.0000\n",
      "Epoch [171/200], Train Loss: 203829472.0000, Val Loss: 50577432.0000\n",
      "Epoch [172/200], Train Loss: 202399440.0000, Val Loss: 50223912.0000\n",
      "Epoch [173/200], Train Loss: 200978320.0000, Val Loss: 49873696.0000\n",
      "Epoch [174/200], Train Loss: 199578752.0000, Val Loss: 49526744.0000\n",
      "Epoch [175/200], Train Loss: 198186544.0000, Val Loss: 49183016.0000\n",
      "Epoch [176/200], Train Loss: 196812512.0000, Val Loss: 48842592.0000\n",
      "Epoch [177/200], Train Loss: 195448544.0000, Val Loss: 48505464.0000\n",
      "Epoch [178/200], Train Loss: 194096128.0000, Val Loss: 48171624.0000\n",
      "Epoch [179/200], Train Loss: 192760288.0000, Val Loss: 47841048.0000\n",
      "Epoch [180/200], Train Loss: 191432288.0000, Val Loss: 47513684.0000\n",
      "Epoch [181/200], Train Loss: 190125184.0000, Val Loss: 47189624.0000\n",
      "Epoch [182/200], Train Loss: 188822880.0000, Val Loss: 46868824.0000\n",
      "Epoch [183/200], Train Loss: 187543616.0000, Val Loss: 46551296.0000\n",
      "Epoch [184/200], Train Loss: 186266528.0000, Val Loss: 46237008.0000\n",
      "Epoch [185/200], Train Loss: 185006720.0000, Val Loss: 45925980.0000\n",
      "Epoch [186/200], Train Loss: 183765792.0000, Val Loss: 45618180.0000\n",
      "Epoch [187/200], Train Loss: 182529232.0000, Val Loss: 45313612.0000\n",
      "Epoch [188/200], Train Loss: 181308992.0000, Val Loss: 45012216.0000\n",
      "Epoch [189/200], Train Loss: 180101952.0000, Val Loss: 44714056.0000\n",
      "Epoch [190/200], Train Loss: 178906128.0000, Val Loss: 44419080.0000\n",
      "Epoch [191/200], Train Loss: 177723936.0000, Val Loss: 44127280.0000\n",
      "Epoch [192/200], Train Loss: 176557616.0000, Val Loss: 43838664.0000\n",
      "Epoch [193/200], Train Loss: 175401568.0000, Val Loss: 43553148.0000\n",
      "Epoch [194/200], Train Loss: 174258272.0000, Val Loss: 43270784.0000\n",
      "Epoch [195/200], Train Loss: 173126816.0000, Val Loss: 42991524.0000\n",
      "Epoch [196/200], Train Loss: 172007456.0000, Val Loss: 42715368.0000\n",
      "Epoch [197/200], Train Loss: 170900384.0000, Val Loss: 42442300.0000\n",
      "Epoch [198/200], Train Loss: 169804880.0000, Val Loss: 42172284.0000\n",
      "Epoch [199/200], Train Loss: 168720496.0000, Val Loss: 41905292.0000\n",
      "Epoch [200/200], Train Loss: 167653344.0000, Val Loss: 41641332.0000\n"
     ]
    }
   ],
   "source": [
    "# 初始化损失列表\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, Y_train)\n",
    "    # loss = custom_loss(output, Y_train)  # 如果使用自定义损失函数\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # 在验证集上评估模型\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, Y_val)\n",
    "        # val_loss = custom_loss(val_output, Y_val)  # 如果使用自定义损失函数\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T13:43:47.913378100Z",
     "start_time": "2024-07-22T12:41:50.708506100Z"
    }
   },
   "id": "f2aab5f66b5220aa"
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Train Loss: 13454432.0000, Val Loss: 3506170.0000\n",
      "Epoch [2/1000], Train Loss: 13458065.0000, Val Loss: 3504981.2500\n",
      "Epoch [3/1000], Train Loss: 13454423.0000, Val Loss: 3505377.7500\n",
      "Epoch [4/1000], Train Loss: 13455712.0000, Val Loss: 3505610.7500\n",
      "Epoch [5/1000], Train Loss: 13456046.0000, Val Loss: 3505635.2500\n",
      "Epoch [6/1000], Train Loss: 13454964.0000, Val Loss: 3505617.2500\n",
      "Epoch [7/1000], Train Loss: 13454376.0000, Val Loss: 3505662.5000\n",
      "Epoch [8/1000], Train Loss: 13454131.0000, Val Loss: 3505583.2500\n",
      "Epoch [9/1000], Train Loss: 13454513.0000, Val Loss: 3505454.7500\n",
      "Epoch [10/1000], Train Loss: 13454519.0000, Val Loss: 3505405.2500\n",
      "Epoch [11/1000], Train Loss: 13453743.0000, Val Loss: 3505483.2500\n",
      "Epoch [12/1000], Train Loss: 13454217.0000, Val Loss: 3505511.2500\n",
      "Epoch [13/1000], Train Loss: 13454395.0000, Val Loss: 3505374.5000\n",
      "Epoch [14/1000], Train Loss: 13454309.0000, Val Loss: 3505215.0000\n",
      "Epoch [15/1000], Train Loss: 13453652.0000, Val Loss: 3505160.2500\n",
      "Epoch [16/1000], Train Loss: 13453660.0000, Val Loss: 3505218.5000\n",
      "Epoch [17/1000], Train Loss: 13454009.0000, Val Loss: 3505333.0000\n",
      "Epoch [18/1000], Train Loss: 13453648.0000, Val Loss: 3505406.2500\n",
      "Epoch [19/1000], Train Loss: 13453481.0000, Val Loss: 3505414.2500\n",
      "Epoch [20/1000], Train Loss: 13453602.0000, Val Loss: 3505364.2500\n",
      "Epoch [21/1000], Train Loss: 13453617.0000, Val Loss: 3505321.2500\n",
      "Epoch [22/1000], Train Loss: 13453472.0000, Val Loss: 3505320.0000\n",
      "Epoch [23/1000], Train Loss: 13453407.0000, Val Loss: 3505347.0000\n",
      "Epoch [24/1000], Train Loss: 13453359.0000, Val Loss: 3505393.0000\n",
      "Epoch [25/1000], Train Loss: 13452946.0000, Val Loss: 3505457.0000\n",
      "Epoch [26/1000], Train Loss: 13453044.0000, Val Loss: 3505502.2500\n",
      "Epoch [27/1000], Train Loss: 13452920.0000, Val Loss: 3505490.2500\n",
      "Epoch [28/1000], Train Loss: 13453249.0000, Val Loss: 3505423.0000\n",
      "Epoch [29/1000], Train Loss: 13453198.0000, Val Loss: 3505347.7500\n",
      "Epoch [30/1000], Train Loss: 13453246.0000, Val Loss: 3505314.7500\n",
      "Epoch [31/1000], Train Loss: 13453122.0000, Val Loss: 3505349.5000\n",
      "Epoch [32/1000], Train Loss: 13453106.0000, Val Loss: 3505394.7500\n",
      "Epoch [33/1000], Train Loss: 13452773.0000, Val Loss: 3505404.2500\n",
      "Epoch [34/1000], Train Loss: 13452856.0000, Val Loss: 3505369.5000\n",
      "Epoch [35/1000], Train Loss: 13452846.0000, Val Loss: 3505337.0000\n",
      "Epoch [36/1000], Train Loss: 13453052.0000, Val Loss: 3505362.2500\n",
      "Epoch [37/1000], Train Loss: 13452775.0000, Val Loss: 3505414.7500\n",
      "Epoch [38/1000], Train Loss: 13452664.0000, Val Loss: 3505427.2500\n",
      "Epoch [39/1000], Train Loss: 13452596.0000, Val Loss: 3505426.5000\n",
      "Epoch [40/1000], Train Loss: 13452824.0000, Val Loss: 3505417.2500\n",
      "Epoch [41/1000], Train Loss: 13452576.0000, Val Loss: 3505389.5000\n",
      "Epoch [42/1000], Train Loss: 13453015.0000, Val Loss: 3505322.2500\n",
      "Epoch [43/1000], Train Loss: 13452754.0000, Val Loss: 3505297.2500\n",
      "Epoch [44/1000], Train Loss: 13452636.0000, Val Loss: 3505339.2500\n",
      "Epoch [45/1000], Train Loss: 13452602.0000, Val Loss: 3505409.7500\n",
      "Epoch [46/1000], Train Loss: 13452736.0000, Val Loss: 3505422.5000\n",
      "Epoch [47/1000], Train Loss: 13452661.0000, Val Loss: 3505358.7500\n",
      "Epoch [48/1000], Train Loss: 13452811.0000, Val Loss: 3505320.7500\n",
      "Epoch [49/1000], Train Loss: 13452566.0000, Val Loss: 3505345.2500\n",
      "Epoch [50/1000], Train Loss: 13452670.0000, Val Loss: 3505405.7500\n",
      "Epoch [51/1000], Train Loss: 13452763.0000, Val Loss: 3505404.0000\n",
      "Epoch [52/1000], Train Loss: 13452588.0000, Val Loss: 3505388.7500\n",
      "Epoch [53/1000], Train Loss: 13452666.0000, Val Loss: 3505376.2500\n",
      "Epoch [54/1000], Train Loss: 13452532.0000, Val Loss: 3505397.0000\n",
      "Epoch [55/1000], Train Loss: 13452526.0000, Val Loss: 3505393.0000\n",
      "Epoch [56/1000], Train Loss: 13452255.0000, Val Loss: 3505337.7500\n",
      "Epoch [57/1000], Train Loss: 13452444.0000, Val Loss: 3505354.7500\n",
      "Epoch [58/1000], Train Loss: 13452425.0000, Val Loss: 3505425.2500\n",
      "Epoch [59/1000], Train Loss: 13452445.0000, Val Loss: 3505474.2500\n",
      "Epoch [60/1000], Train Loss: 13452486.0000, Val Loss: 3505402.5000\n",
      "Epoch [61/1000], Train Loss: 13452400.0000, Val Loss: 3505318.5000\n",
      "Epoch [62/1000], Train Loss: 13452564.0000, Val Loss: 3505310.2500\n",
      "Epoch [63/1000], Train Loss: 13452150.0000, Val Loss: 3505385.2500\n",
      "Epoch [64/1000], Train Loss: 13452249.0000, Val Loss: 3505415.2500\n",
      "Epoch [65/1000], Train Loss: 13452510.0000, Val Loss: 3505355.5000\n",
      "Epoch [66/1000], Train Loss: 13452237.0000, Val Loss: 3505353.2500\n",
      "Epoch [67/1000], Train Loss: 13452191.0000, Val Loss: 3505385.5000\n",
      "Epoch [68/1000], Train Loss: 13452050.0000, Val Loss: 3505396.7500\n",
      "Epoch [69/1000], Train Loss: 13452216.0000, Val Loss: 3505413.5000\n",
      "Epoch [70/1000], Train Loss: 13452229.0000, Val Loss: 3505386.0000\n",
      "Epoch [71/1000], Train Loss: 13452133.0000, Val Loss: 3505393.5000\n",
      "Epoch [72/1000], Train Loss: 13452248.0000, Val Loss: 3505457.2500\n",
      "Epoch [73/1000], Train Loss: 13452167.0000, Val Loss: 3505428.5000\n",
      "Epoch [74/1000], Train Loss: 13452200.0000, Val Loss: 3505386.0000\n",
      "Epoch [75/1000], Train Loss: 13452196.0000, Val Loss: 3505347.7500\n",
      "Epoch [76/1000], Train Loss: 13452105.0000, Val Loss: 3505361.2500\n",
      "Epoch [77/1000], Train Loss: 13452157.0000, Val Loss: 3505397.2500\n",
      "Epoch [78/1000], Train Loss: 13452004.0000, Val Loss: 3505390.5000\n",
      "Epoch [79/1000], Train Loss: 13451947.0000, Val Loss: 3505347.2500\n",
      "Epoch [80/1000], Train Loss: 13452008.0000, Val Loss: 3505300.5000\n",
      "Epoch [81/1000], Train Loss: 13452270.0000, Val Loss: 3505316.5000\n",
      "Epoch [82/1000], Train Loss: 13451947.0000, Val Loss: 3505419.2500\n",
      "Epoch [83/1000], Train Loss: 13451868.0000, Val Loss: 3505475.7500\n",
      "Epoch [84/1000], Train Loss: 13451888.0000, Val Loss: 3505386.5000\n",
      "Epoch [85/1000], Train Loss: 13451927.0000, Val Loss: 3505362.7500\n",
      "Epoch [86/1000], Train Loss: 13452134.0000, Val Loss: 3505428.7500\n",
      "Epoch [87/1000], Train Loss: 13452076.0000, Val Loss: 3505429.5000\n",
      "Epoch [88/1000], Train Loss: 13452076.0000, Val Loss: 3505382.5000\n",
      "Epoch [89/1000], Train Loss: 13451854.0000, Val Loss: 3505343.5000\n",
      "Epoch [90/1000], Train Loss: 13451785.0000, Val Loss: 3505375.0000\n",
      "Epoch [91/1000], Train Loss: 13452148.0000, Val Loss: 3505445.7500\n",
      "Epoch [92/1000], Train Loss: 13452054.0000, Val Loss: 3505464.2500\n",
      "Epoch [93/1000], Train Loss: 13451790.0000, Val Loss: 3505458.0000\n",
      "Epoch [94/1000], Train Loss: 13452001.0000, Val Loss: 3505400.5000\n",
      "Epoch [95/1000], Train Loss: 13452041.0000, Val Loss: 3505372.2500\n",
      "Epoch [96/1000], Train Loss: 13452160.0000, Val Loss: 3505473.0000\n",
      "Epoch [97/1000], Train Loss: 13451880.0000, Val Loss: 3505532.5000\n",
      "Epoch [98/1000], Train Loss: 13451778.0000, Val Loss: 3505497.5000\n",
      "Epoch [99/1000], Train Loss: 13452156.0000, Val Loss: 3505402.5000\n",
      "Epoch [100/1000], Train Loss: 13451799.0000, Val Loss: 3505396.2500\n",
      "Epoch [101/1000], Train Loss: 13451683.0000, Val Loss: 3505513.5000\n",
      "Epoch [102/1000], Train Loss: 13451770.0000, Val Loss: 3505556.7500\n",
      "Epoch [103/1000], Train Loss: 13451848.0000, Val Loss: 3505456.0000\n",
      "Epoch [104/1000], Train Loss: 13451650.0000, Val Loss: 3505380.0000\n",
      "Epoch [105/1000], Train Loss: 13451800.0000, Val Loss: 3505435.2500\n",
      "Epoch [106/1000], Train Loss: 13451865.0000, Val Loss: 3505511.2500\n",
      "Epoch [107/1000], Train Loss: 13451886.0000, Val Loss: 3505523.7500\n",
      "Epoch [108/1000], Train Loss: 13451736.0000, Val Loss: 3505484.0000\n",
      "Epoch [109/1000], Train Loss: 13451969.0000, Val Loss: 3505483.7500\n",
      "Epoch [110/1000], Train Loss: 13451684.0000, Val Loss: 3505506.2500\n",
      "Epoch [111/1000], Train Loss: 13451682.0000, Val Loss: 3505464.0000\n",
      "Epoch [112/1000], Train Loss: 13451733.0000, Val Loss: 3505461.5000\n",
      "Epoch [113/1000], Train Loss: 13451720.0000, Val Loss: 3505524.7500\n",
      "Epoch [114/1000], Train Loss: 13451642.0000, Val Loss: 3505571.5000\n",
      "Epoch [115/1000], Train Loss: 13451498.0000, Val Loss: 3505454.5000\n",
      "Epoch [116/1000], Train Loss: 13451520.0000, Val Loss: 3505365.5000\n",
      "Epoch [117/1000], Train Loss: 13451729.0000, Val Loss: 3505452.7500\n",
      "Epoch [118/1000], Train Loss: 13451581.0000, Val Loss: 3505676.5000\n",
      "Epoch [119/1000], Train Loss: 13451703.0000, Val Loss: 3505591.2500\n",
      "Epoch [120/1000], Train Loss: 13451584.0000, Val Loss: 3505398.2500\n",
      "Epoch [121/1000], Train Loss: 13451514.0000, Val Loss: 3505463.7500\n",
      "Epoch [122/1000], Train Loss: 13451306.0000, Val Loss: 3505633.7500\n",
      "Epoch [123/1000], Train Loss: 13451361.0000, Val Loss: 3505575.5000\n",
      "Epoch [124/1000], Train Loss: 13451324.0000, Val Loss: 3505479.5000\n",
      "Epoch [125/1000], Train Loss: 13451365.0000, Val Loss: 3505549.2500\n",
      "Epoch [126/1000], Train Loss: 13451503.0000, Val Loss: 3505631.0000\n",
      "Epoch [127/1000], Train Loss: 13451447.0000, Val Loss: 3505548.2500\n",
      "Epoch [128/1000], Train Loss: 13451587.0000, Val Loss: 3505526.2500\n",
      "Epoch [129/1000], Train Loss: 13451310.0000, Val Loss: 3505597.5000\n",
      "Epoch [130/1000], Train Loss: 13451217.0000, Val Loss: 3505665.0000\n",
      "Epoch [131/1000], Train Loss: 13451423.0000, Val Loss: 3505624.2500\n",
      "Epoch [132/1000], Train Loss: 13451111.0000, Val Loss: 3505575.0000\n",
      "Epoch [133/1000], Train Loss: 13451426.0000, Val Loss: 3505611.2500\n",
      "Epoch [134/1000], Train Loss: 13451213.0000, Val Loss: 3505719.0000\n",
      "Epoch [135/1000], Train Loss: 13451429.0000, Val Loss: 3505670.7500\n",
      "Epoch [136/1000], Train Loss: 13451108.0000, Val Loss: 3505598.0000\n",
      "Epoch [137/1000], Train Loss: 13451434.0000, Val Loss: 3505663.7500\n",
      "Epoch [138/1000], Train Loss: 13451074.0000, Val Loss: 3505771.2500\n",
      "Epoch [139/1000], Train Loss: 13450942.0000, Val Loss: 3505785.0000\n",
      "Epoch [140/1000], Train Loss: 13451171.0000, Val Loss: 3505604.7500\n",
      "Epoch [141/1000], Train Loss: 13451262.0000, Val Loss: 3505615.2500\n",
      "Epoch [142/1000], Train Loss: 13451110.0000, Val Loss: 3505729.0000\n",
      "Epoch [143/1000], Train Loss: 13451158.0000, Val Loss: 3505773.7500\n",
      "Epoch [144/1000], Train Loss: 13451042.0000, Val Loss: 3505686.5000\n",
      "Epoch [145/1000], Train Loss: 13451215.0000, Val Loss: 3505702.7500\n",
      "Epoch [146/1000], Train Loss: 13450757.0000, Val Loss: 3505876.2500\n",
      "Epoch [147/1000], Train Loss: 13451020.0000, Val Loss: 3505909.0000\n",
      "Epoch [148/1000], Train Loss: 13451018.0000, Val Loss: 3505756.7500\n",
      "Epoch [149/1000], Train Loss: 13450715.0000, Val Loss: 3505748.7500\n",
      "Epoch [150/1000], Train Loss: 13450779.0000, Val Loss: 3505956.7500\n",
      "Epoch [151/1000], Train Loss: 13450710.0000, Val Loss: 3506044.2500\n",
      "Epoch [152/1000], Train Loss: 13450880.0000, Val Loss: 3505884.7500\n",
      "Epoch [153/1000], Train Loss: 13450785.0000, Val Loss: 3505842.7500\n",
      "Epoch [154/1000], Train Loss: 13450741.0000, Val Loss: 3505986.2500\n",
      "Epoch [155/1000], Train Loss: 13450597.0000, Val Loss: 3506009.2500\n",
      "Epoch [156/1000], Train Loss: 13450774.0000, Val Loss: 3505875.0000\n",
      "Epoch [157/1000], Train Loss: 13450939.0000, Val Loss: 3505925.0000\n",
      "Epoch [158/1000], Train Loss: 13450568.0000, Val Loss: 3505998.5000\n",
      "Epoch [159/1000], Train Loss: 13450960.0000, Val Loss: 3505910.7500\n",
      "Epoch [160/1000], Train Loss: 13450665.0000, Val Loss: 3505829.2500\n",
      "Epoch [161/1000], Train Loss: 13450545.0000, Val Loss: 3505925.0000\n",
      "Epoch [162/1000], Train Loss: 13450686.0000, Val Loss: 3505980.5000\n",
      "Epoch [163/1000], Train Loss: 13450630.0000, Val Loss: 3505875.2500\n",
      "Epoch [164/1000], Train Loss: 13450773.0000, Val Loss: 3505866.2500\n",
      "Epoch [165/1000], Train Loss: 13450510.0000, Val Loss: 3505995.2500\n",
      "Epoch [166/1000], Train Loss: 13450550.0000, Val Loss: 3506156.5000\n",
      "Epoch [167/1000], Train Loss: 13450444.0000, Val Loss: 3506128.5000\n",
      "Epoch [168/1000], Train Loss: 13450664.0000, Val Loss: 3506021.7500\n",
      "Epoch [169/1000], Train Loss: 13450400.0000, Val Loss: 3506093.5000\n",
      "Epoch [170/1000], Train Loss: 13450565.0000, Val Loss: 3506090.5000\n",
      "Epoch [171/1000], Train Loss: 13450455.0000, Val Loss: 3506177.5000\n",
      "Epoch [172/1000], Train Loss: 13450556.0000, Val Loss: 3506113.0000\n",
      "Epoch [173/1000], Train Loss: 13450489.0000, Val Loss: 3506085.7500\n",
      "Epoch [174/1000], Train Loss: 13450168.0000, Val Loss: 3506125.5000\n",
      "Epoch [175/1000], Train Loss: 13450253.0000, Val Loss: 3506004.2500\n",
      "Epoch [176/1000], Train Loss: 13450124.0000, Val Loss: 3506183.7500\n",
      "Epoch [177/1000], Train Loss: 13450277.0000, Val Loss: 3506374.7500\n",
      "Epoch [178/1000], Train Loss: 13450043.0000, Val Loss: 3506278.7500\n",
      "Epoch [179/1000], Train Loss: 13450400.0000, Val Loss: 3506097.0000\n",
      "Epoch [180/1000], Train Loss: 13449983.0000, Val Loss: 3506373.7500\n",
      "Epoch [181/1000], Train Loss: 13450009.0000, Val Loss: 3506647.2500\n",
      "Epoch [182/1000], Train Loss: 13450206.0000, Val Loss: 3506235.0000\n",
      "Epoch [183/1000], Train Loss: 13450244.0000, Val Loss: 3506107.7500\n",
      "Epoch [184/1000], Train Loss: 13449780.0000, Val Loss: 3506640.7500\n",
      "Epoch [185/1000], Train Loss: 13449742.0000, Val Loss: 3506616.5000\n",
      "Epoch [186/1000], Train Loss: 13449478.0000, Val Loss: 3506201.0000\n",
      "Epoch [187/1000], Train Loss: 13449583.0000, Val Loss: 3506390.0000\n",
      "Epoch [188/1000], Train Loss: 13449675.0000, Val Loss: 3506838.5000\n",
      "Epoch [189/1000], Train Loss: 13449778.0000, Val Loss: 3506519.0000\n",
      "Epoch [190/1000], Train Loss: 13449615.0000, Val Loss: 3506161.0000\n",
      "Epoch [191/1000], Train Loss: 13449706.0000, Val Loss: 3506762.2500\n",
      "Epoch [192/1000], Train Loss: 13449504.0000, Val Loss: 3506854.7500\n",
      "Epoch [193/1000], Train Loss: 13449658.0000, Val Loss: 3506291.2500\n",
      "Epoch [194/1000], Train Loss: 13449594.0000, Val Loss: 3506379.2500\n",
      "Epoch [195/1000], Train Loss: 13449490.0000, Val Loss: 3506875.0000\n",
      "Epoch [196/1000], Train Loss: 13449550.0000, Val Loss: 3506626.7500\n",
      "Epoch [197/1000], Train Loss: 13449007.0000, Val Loss: 3506361.5000\n",
      "Epoch [198/1000], Train Loss: 13449606.0000, Val Loss: 3506608.2500\n",
      "Epoch [199/1000], Train Loss: 13449275.0000, Val Loss: 3507029.5000\n",
      "Epoch [200/1000], Train Loss: 13449129.0000, Val Loss: 3506630.5000\n",
      "Epoch [201/1000], Train Loss: 13449329.0000, Val Loss: 3506574.2500\n",
      "Epoch [202/1000], Train Loss: 13449144.0000, Val Loss: 3507020.0000\n",
      "Epoch [203/1000], Train Loss: 13449514.0000, Val Loss: 3506892.7500\n",
      "Epoch [204/1000], Train Loss: 13448622.0000, Val Loss: 3506720.2500\n",
      "Epoch [205/1000], Train Loss: 13449223.0000, Val Loss: 3506772.7500\n",
      "Epoch [206/1000], Train Loss: 13448540.0000, Val Loss: 3507045.0000\n",
      "Epoch [207/1000], Train Loss: 13448676.0000, Val Loss: 3506934.2500\n",
      "Epoch [208/1000], Train Loss: 13448823.0000, Val Loss: 3506813.2500\n",
      "Epoch [209/1000], Train Loss: 13448949.0000, Val Loss: 3506881.5000\n",
      "Epoch [210/1000], Train Loss: 13448734.0000, Val Loss: 3507088.2500\n",
      "Epoch [211/1000], Train Loss: 13448351.0000, Val Loss: 3507084.2500\n",
      "Epoch [212/1000], Train Loss: 13448957.0000, Val Loss: 3506949.7500\n",
      "Epoch [213/1000], Train Loss: 13448706.0000, Val Loss: 3507237.7500\n",
      "Epoch [214/1000], Train Loss: 13448868.0000, Val Loss: 3507170.2500\n",
      "Epoch [215/1000], Train Loss: 13448230.0000, Val Loss: 3507177.7500\n",
      "Epoch [216/1000], Train Loss: 13448388.0000, Val Loss: 3507244.7500\n",
      "Epoch [217/1000], Train Loss: 13448692.0000, Val Loss: 3507389.2500\n",
      "Epoch [218/1000], Train Loss: 13448498.0000, Val Loss: 3507118.2500\n",
      "Epoch [219/1000], Train Loss: 13448290.0000, Val Loss: 3507081.2500\n",
      "Epoch [220/1000], Train Loss: 13448106.0000, Val Loss: 3507356.2500\n",
      "Epoch [221/1000], Train Loss: 13448280.0000, Val Loss: 3507295.7500\n",
      "Epoch [222/1000], Train Loss: 13448284.0000, Val Loss: 3507027.2500\n",
      "Epoch [223/1000], Train Loss: 13447963.0000, Val Loss: 3507306.7500\n",
      "Epoch [224/1000], Train Loss: 13447795.0000, Val Loss: 3507537.0000\n",
      "Epoch [225/1000], Train Loss: 13447909.0000, Val Loss: 3507280.2500\n",
      "Epoch [226/1000], Train Loss: 13448086.0000, Val Loss: 3507482.0000\n",
      "Epoch [227/1000], Train Loss: 13447507.0000, Val Loss: 3507798.7500\n",
      "Epoch [228/1000], Train Loss: 13447311.0000, Val Loss: 3507637.7500\n",
      "Epoch [229/1000], Train Loss: 13447813.0000, Val Loss: 3507664.5000\n",
      "Epoch [230/1000], Train Loss: 13447530.0000, Val Loss: 3507826.7500\n",
      "Epoch [231/1000], Train Loss: 13447508.0000, Val Loss: 3507913.5000\n",
      "Epoch [232/1000], Train Loss: 13447681.0000, Val Loss: 3507344.7500\n",
      "Epoch [233/1000], Train Loss: 13447567.0000, Val Loss: 3507581.5000\n",
      "Epoch [234/1000], Train Loss: 13447005.0000, Val Loss: 3507973.2500\n",
      "Epoch [235/1000], Train Loss: 13447276.0000, Val Loss: 3507545.0000\n",
      "Epoch [236/1000], Train Loss: 13447341.0000, Val Loss: 3507487.7500\n",
      "Epoch [237/1000], Train Loss: 13447284.0000, Val Loss: 3508022.5000\n",
      "Epoch [238/1000], Train Loss: 13447580.0000, Val Loss: 3507796.5000\n",
      "Epoch [239/1000], Train Loss: 13447085.0000, Val Loss: 3507497.5000\n",
      "Epoch [240/1000], Train Loss: 13446924.0000, Val Loss: 3507814.5000\n",
      "Epoch [241/1000], Train Loss: 13446529.0000, Val Loss: 3507969.7500\n",
      "Epoch [242/1000], Train Loss: 13446487.0000, Val Loss: 3507762.7500\n",
      "Epoch [243/1000], Train Loss: 13446725.0000, Val Loss: 3507714.2500\n",
      "Epoch [244/1000], Train Loss: 13446726.0000, Val Loss: 3508219.7500\n",
      "Epoch [245/1000], Train Loss: 13446301.0000, Val Loss: 3507952.0000\n",
      "Epoch [246/1000], Train Loss: 13446539.0000, Val Loss: 3507864.5000\n",
      "Epoch [247/1000], Train Loss: 13445896.0000, Val Loss: 3508388.7500\n",
      "Epoch [248/1000], Train Loss: 13446267.0000, Val Loss: 3508056.5000\n",
      "Epoch [249/1000], Train Loss: 13445976.0000, Val Loss: 3508143.2500\n",
      "Epoch [250/1000], Train Loss: 13446199.0000, Val Loss: 3508281.0000\n",
      "Epoch [251/1000], Train Loss: 13445671.0000, Val Loss: 3507747.2500\n",
      "Epoch [252/1000], Train Loss: 13445741.0000, Val Loss: 3508583.2500\n",
      "Epoch [253/1000], Train Loss: 13445656.0000, Val Loss: 3508987.0000\n",
      "Epoch [254/1000], Train Loss: 13445978.0000, Val Loss: 3507864.5000\n",
      "Epoch [255/1000], Train Loss: 13445762.0000, Val Loss: 3508063.7500\n",
      "Epoch [256/1000], Train Loss: 13445416.0000, Val Loss: 3508588.0000\n",
      "Epoch [257/1000], Train Loss: 13445672.0000, Val Loss: 3507839.2500\n",
      "Epoch [258/1000], Train Loss: 13444968.0000, Val Loss: 3507655.2500\n",
      "Epoch [259/1000], Train Loss: 13445218.0000, Val Loss: 3508199.5000\n",
      "Epoch [260/1000], Train Loss: 13444967.0000, Val Loss: 3507999.0000\n",
      "Epoch [261/1000], Train Loss: 13445281.0000, Val Loss: 3507698.2500\n",
      "Epoch [262/1000], Train Loss: 13445135.0000, Val Loss: 3508722.0000\n",
      "Epoch [263/1000], Train Loss: 13445243.0000, Val Loss: 3508200.7500\n",
      "Epoch [264/1000], Train Loss: 13444050.0000, Val Loss: 3508329.0000\n",
      "Epoch [265/1000], Train Loss: 13444906.0000, Val Loss: 3508815.5000\n",
      "Epoch [266/1000], Train Loss: 13444620.0000, Val Loss: 3508281.2500\n",
      "Epoch [267/1000], Train Loss: 13444244.0000, Val Loss: 3508314.0000\n",
      "Epoch [268/1000], Train Loss: 13443747.0000, Val Loss: 3508726.7500\n",
      "Epoch [269/1000], Train Loss: 13444297.0000, Val Loss: 3508131.2500\n",
      "Epoch [270/1000], Train Loss: 13443769.0000, Val Loss: 3507958.2500\n",
      "Epoch [271/1000], Train Loss: 13444432.0000, Val Loss: 3508612.7500\n",
      "Epoch [272/1000], Train Loss: 13443748.0000, Val Loss: 3508329.0000\n",
      "Epoch [273/1000], Train Loss: 13443621.0000, Val Loss: 3507615.2500\n",
      "Epoch [274/1000], Train Loss: 13444175.0000, Val Loss: 3508891.0000\n",
      "Epoch [275/1000], Train Loss: 13443894.0000, Val Loss: 3508710.7500\n",
      "Epoch [276/1000], Train Loss: 13443156.0000, Val Loss: 3507847.7500\n",
      "Epoch [277/1000], Train Loss: 13443910.0000, Val Loss: 3509149.0000\n",
      "Epoch [278/1000], Train Loss: 13442714.0000, Val Loss: 3509270.5000\n",
      "Epoch [279/1000], Train Loss: 13443615.0000, Val Loss: 3508415.7500\n",
      "Epoch [280/1000], Train Loss: 13443113.0000, Val Loss: 3508681.5000\n",
      "Epoch [281/1000], Train Loss: 13443176.0000, Val Loss: 3509148.2500\n",
      "Epoch [282/1000], Train Loss: 13442844.0000, Val Loss: 3508381.2500\n",
      "Epoch [283/1000], Train Loss: 13443068.0000, Val Loss: 3507899.2500\n",
      "Epoch [284/1000], Train Loss: 13442503.0000, Val Loss: 3508484.7500\n",
      "Epoch [285/1000], Train Loss: 13443156.0000, Val Loss: 3508207.2500\n",
      "Epoch [286/1000], Train Loss: 13442471.0000, Val Loss: 3507800.7500\n",
      "Epoch [287/1000], Train Loss: 13441902.0000, Val Loss: 3508602.7500\n",
      "Epoch [288/1000], Train Loss: 13441800.0000, Val Loss: 3508779.2500\n",
      "Epoch [289/1000], Train Loss: 13441649.0000, Val Loss: 3508342.0000\n",
      "Epoch [290/1000], Train Loss: 13442374.0000, Val Loss: 3508931.2500\n",
      "Epoch [291/1000], Train Loss: 13441757.0000, Val Loss: 3509488.5000\n",
      "Epoch [292/1000], Train Loss: 13441760.0000, Val Loss: 3508319.0000\n",
      "Epoch [293/1000], Train Loss: 13441800.0000, Val Loss: 3509284.5000\n",
      "Epoch [294/1000], Train Loss: 13441034.0000, Val Loss: 3509485.0000\n",
      "Epoch [295/1000], Train Loss: 13441505.0000, Val Loss: 3508346.7500\n",
      "Epoch [296/1000], Train Loss: 13441199.0000, Val Loss: 3509065.7500\n",
      "Epoch [297/1000], Train Loss: 13441264.0000, Val Loss: 3508986.2500\n",
      "Epoch [298/1000], Train Loss: 13440256.0000, Val Loss: 3507841.5000\n",
      "Epoch [299/1000], Train Loss: 13441228.0000, Val Loss: 3508872.0000\n",
      "Epoch [300/1000], Train Loss: 13440847.0000, Val Loss: 3508869.2500\n",
      "Epoch [301/1000], Train Loss: 13440080.0000, Val Loss: 3507946.7500\n",
      "Epoch [302/1000], Train Loss: 13440541.0000, Val Loss: 3509592.2500\n",
      "Epoch [303/1000], Train Loss: 13440769.0000, Val Loss: 3509566.7500\n",
      "Epoch [304/1000], Train Loss: 13440154.0000, Val Loss: 3508150.2500\n",
      "Epoch [305/1000], Train Loss: 13440301.0000, Val Loss: 3508866.5000\n",
      "Epoch [306/1000], Train Loss: 13439549.0000, Val Loss: 3509215.0000\n",
      "Epoch [307/1000], Train Loss: 13440156.0000, Val Loss: 3507975.2500\n",
      "Epoch [308/1000], Train Loss: 13439895.0000, Val Loss: 3508626.0000\n",
      "Epoch [309/1000], Train Loss: 13439205.0000, Val Loss: 3509714.2500\n",
      "Epoch [310/1000], Train Loss: 13439509.0000, Val Loss: 3508797.2500\n",
      "Epoch [311/1000], Train Loss: 13439219.0000, Val Loss: 3508857.7500\n",
      "Epoch [312/1000], Train Loss: 13438937.0000, Val Loss: 3509583.0000\n",
      "Epoch [313/1000], Train Loss: 13438628.0000, Val Loss: 3508629.0000\n",
      "Epoch [314/1000], Train Loss: 13439399.0000, Val Loss: 3508445.7500\n",
      "Epoch [315/1000], Train Loss: 13438817.0000, Val Loss: 3510060.2500\n",
      "Epoch [316/1000], Train Loss: 13438596.0000, Val Loss: 3509071.5000\n",
      "Epoch [317/1000], Train Loss: 13437797.0000, Val Loss: 3508813.7500\n",
      "Epoch [318/1000], Train Loss: 13438127.0000, Val Loss: 3509738.7500\n",
      "Epoch [319/1000], Train Loss: 13437670.0000, Val Loss: 3508505.0000\n",
      "Epoch [320/1000], Train Loss: 13438635.0000, Val Loss: 3508420.2500\n",
      "Epoch [321/1000], Train Loss: 13437718.0000, Val Loss: 3509752.0000\n",
      "Epoch [322/1000], Train Loss: 13437694.0000, Val Loss: 3509205.5000\n",
      "Epoch [323/1000], Train Loss: 13437493.0000, Val Loss: 3509120.7500\n",
      "Epoch [324/1000], Train Loss: 13437450.0000, Val Loss: 3509739.5000\n",
      "Epoch [325/1000], Train Loss: 13436476.0000, Val Loss: 3509187.0000\n",
      "Epoch [326/1000], Train Loss: 13436539.0000, Val Loss: 3508458.2500\n",
      "Epoch [327/1000], Train Loss: 13437240.0000, Val Loss: 3509537.2500\n",
      "Epoch [328/1000], Train Loss: 13436825.0000, Val Loss: 3509339.5000\n",
      "Epoch [329/1000], Train Loss: 13436518.0000, Val Loss: 3508809.5000\n",
      "Epoch [330/1000], Train Loss: 13436444.0000, Val Loss: 3509427.5000\n",
      "Epoch [331/1000], Train Loss: 13435690.0000, Val Loss: 3509585.2500\n",
      "Epoch [332/1000], Train Loss: 13435607.0000, Val Loss: 3508719.0000\n",
      "Epoch [333/1000], Train Loss: 13435634.0000, Val Loss: 3508357.7500\n",
      "Epoch [334/1000], Train Loss: 13436056.0000, Val Loss: 3509973.5000\n",
      "Epoch [335/1000], Train Loss: 13435516.0000, Val Loss: 3509150.7500\n",
      "Epoch [336/1000], Train Loss: 13435106.0000, Val Loss: 3509560.7500\n",
      "Epoch [337/1000], Train Loss: 13435503.0000, Val Loss: 3509390.2500\n",
      "Epoch [338/1000], Train Loss: 13434062.0000, Val Loss: 3509564.5000\n",
      "Epoch [339/1000], Train Loss: 13434207.0000, Val Loss: 3509646.7500\n",
      "Epoch [340/1000], Train Loss: 13434749.0000, Val Loss: 3508223.7500\n",
      "Epoch [341/1000], Train Loss: 13435976.0000, Val Loss: 3509274.7500\n",
      "Epoch [342/1000], Train Loss: 13435584.0000, Val Loss: 3510038.0000\n",
      "Epoch [343/1000], Train Loss: 13434787.0000, Val Loss: 3509973.2500\n",
      "Epoch [344/1000], Train Loss: 13433264.0000, Val Loss: 3510583.2500\n",
      "Epoch [345/1000], Train Loss: 13435022.0000, Val Loss: 3508788.2500\n",
      "Epoch [346/1000], Train Loss: 13433825.0000, Val Loss: 3508150.0000\n",
      "Epoch [347/1000], Train Loss: 13433864.0000, Val Loss: 3509483.7500\n",
      "Epoch [348/1000], Train Loss: 13433448.0000, Val Loss: 3509729.0000\n",
      "Epoch [349/1000], Train Loss: 13433170.0000, Val Loss: 3509621.2500\n",
      "Epoch [350/1000], Train Loss: 13434114.0000, Val Loss: 3511712.0000\n",
      "Epoch [351/1000], Train Loss: 13433615.0000, Val Loss: 3510656.7500\n",
      "Epoch [352/1000], Train Loss: 13432749.0000, Val Loss: 3509753.0000\n",
      "Epoch [353/1000], Train Loss: 13432684.0000, Val Loss: 3510522.2500\n",
      "Epoch [354/1000], Train Loss: 13433239.0000, Val Loss: 3508338.2500\n",
      "Epoch [355/1000], Train Loss: 13432673.0000, Val Loss: 3508881.2500\n",
      "Epoch [356/1000], Train Loss: 13433295.0000, Val Loss: 3511358.5000\n",
      "Epoch [357/1000], Train Loss: 13433058.0000, Val Loss: 3510080.2500\n",
      "Epoch [358/1000], Train Loss: 13432750.0000, Val Loss: 3509919.2500\n",
      "Epoch [359/1000], Train Loss: 13432494.0000, Val Loss: 3510835.2500\n",
      "Epoch [360/1000], Train Loss: 13431809.0000, Val Loss: 3509406.7500\n",
      "Epoch [361/1000], Train Loss: 13431157.0000, Val Loss: 3508373.5000\n",
      "Epoch [362/1000], Train Loss: 13431976.0000, Val Loss: 3510270.2500\n",
      "Epoch [363/1000], Train Loss: 13431650.0000, Val Loss: 3510438.2500\n",
      "Epoch [364/1000], Train Loss: 13432455.0000, Val Loss: 3510139.2500\n",
      "Epoch [365/1000], Train Loss: 13431547.0000, Val Loss: 3511619.2500\n",
      "Epoch [366/1000], Train Loss: 13431682.0000, Val Loss: 3512207.7500\n",
      "Epoch [367/1000], Train Loss: 13431651.0000, Val Loss: 3509921.0000\n",
      "Epoch [368/1000], Train Loss: 13430182.0000, Val Loss: 3509770.7500\n",
      "Epoch [369/1000], Train Loss: 13429377.0000, Val Loss: 3510822.7500\n",
      "Epoch [370/1000], Train Loss: 13432989.0000, Val Loss: 3508040.0000\n",
      "Epoch [371/1000], Train Loss: 13432584.0000, Val Loss: 3509961.5000\n",
      "Epoch [372/1000], Train Loss: 13429663.0000, Val Loss: 3513800.5000\n",
      "Epoch [373/1000], Train Loss: 13431436.0000, Val Loss: 3512015.2500\n",
      "Epoch [374/1000], Train Loss: 13430143.0000, Val Loss: 3509676.0000\n",
      "Epoch [375/1000], Train Loss: 13431158.0000, Val Loss: 3510345.2500\n",
      "Epoch [376/1000], Train Loss: 13429186.0000, Val Loss: 3512022.7500\n",
      "Epoch [377/1000], Train Loss: 13429173.0000, Val Loss: 3510457.5000\n",
      "Epoch [378/1000], Train Loss: 13428912.0000, Val Loss: 3509592.2500\n",
      "Epoch [379/1000], Train Loss: 13430104.0000, Val Loss: 3511512.0000\n",
      "Epoch [380/1000], Train Loss: 13428754.0000, Val Loss: 3514115.0000\n",
      "Epoch [381/1000], Train Loss: 13429748.0000, Val Loss: 3511925.0000\n",
      "Epoch [382/1000], Train Loss: 13428468.0000, Val Loss: 3509796.5000\n",
      "Epoch [383/1000], Train Loss: 13430098.0000, Val Loss: 3510181.7500\n",
      "Epoch [384/1000], Train Loss: 13427510.0000, Val Loss: 3512105.0000\n",
      "Epoch [385/1000], Train Loss: 13428502.0000, Val Loss: 3511383.2500\n",
      "Epoch [386/1000], Train Loss: 13428536.0000, Val Loss: 3511463.2500\n",
      "Epoch [387/1000], Train Loss: 13428139.0000, Val Loss: 3512377.0000\n",
      "Epoch [388/1000], Train Loss: 13426908.0000, Val Loss: 3512562.0000\n",
      "Epoch [389/1000], Train Loss: 13428278.0000, Val Loss: 3510702.7500\n",
      "Epoch [390/1000], Train Loss: 13427534.0000, Val Loss: 3511442.5000\n",
      "Epoch [391/1000], Train Loss: 13427238.0000, Val Loss: 3512338.7500\n",
      "Epoch [392/1000], Train Loss: 13427712.0000, Val Loss: 3511816.0000\n",
      "Epoch [393/1000], Train Loss: 13426839.0000, Val Loss: 3512348.7500\n",
      "Epoch [394/1000], Train Loss: 13427876.0000, Val Loss: 3513370.7500\n",
      "Epoch [395/1000], Train Loss: 13427783.0000, Val Loss: 3511721.7500\n",
      "Epoch [396/1000], Train Loss: 13426062.0000, Val Loss: 3510638.0000\n",
      "Epoch [397/1000], Train Loss: 13426907.0000, Val Loss: 3512032.0000\n",
      "Epoch [398/1000], Train Loss: 13426964.0000, Val Loss: 3511493.0000\n",
      "Epoch [399/1000], Train Loss: 13427667.0000, Val Loss: 3511119.7500\n",
      "Epoch [400/1000], Train Loss: 13427124.0000, Val Loss: 3512973.7500\n",
      "Epoch [401/1000], Train Loss: 13425689.0000, Val Loss: 3514321.2500\n",
      "Epoch [402/1000], Train Loss: 13425957.0000, Val Loss: 3512790.2500\n",
      "Epoch [403/1000], Train Loss: 13425467.0000, Val Loss: 3511470.2500\n",
      "Epoch [404/1000], Train Loss: 13427063.0000, Val Loss: 3512574.0000\n",
      "Epoch [405/1000], Train Loss: 13424789.0000, Val Loss: 3512817.7500\n",
      "Epoch [406/1000], Train Loss: 13424007.0000, Val Loss: 3511641.0000\n",
      "Epoch [407/1000], Train Loss: 13424671.0000, Val Loss: 3511837.7500\n",
      "Epoch [408/1000], Train Loss: 13425326.0000, Val Loss: 3513269.5000\n",
      "Epoch [409/1000], Train Loss: 13424477.0000, Val Loss: 3513162.5000\n",
      "Epoch [410/1000], Train Loss: 13425756.0000, Val Loss: 3510624.7500\n",
      "Epoch [411/1000], Train Loss: 13424883.0000, Val Loss: 3510923.5000\n",
      "Epoch [412/1000], Train Loss: 13423125.0000, Val Loss: 3513736.0000\n",
      "Epoch [413/1000], Train Loss: 13428028.0000, Val Loss: 3511197.2500\n",
      "Epoch [414/1000], Train Loss: 13423812.0000, Val Loss: 3511073.7500\n",
      "Epoch [415/1000], Train Loss: 13425800.0000, Val Loss: 3514289.7500\n",
      "Epoch [416/1000], Train Loss: 13426008.0000, Val Loss: 3512970.7500\n",
      "Epoch [417/1000], Train Loss: 13424153.0000, Val Loss: 3510694.5000\n",
      "Epoch [418/1000], Train Loss: 13424775.0000, Val Loss: 3511803.7500\n",
      "Epoch [419/1000], Train Loss: 13422734.0000, Val Loss: 3514294.7500\n",
      "Epoch [420/1000], Train Loss: 13424390.0000, Val Loss: 3513029.7500\n",
      "Epoch [421/1000], Train Loss: 13423510.0000, Val Loss: 3511587.0000\n",
      "Epoch [422/1000], Train Loss: 13423172.0000, Val Loss: 3512207.2500\n",
      "Epoch [423/1000], Train Loss: 13422029.0000, Val Loss: 3513454.7500\n",
      "Epoch [424/1000], Train Loss: 13423654.0000, Val Loss: 3511848.7500\n",
      "Epoch [425/1000], Train Loss: 13422596.0000, Val Loss: 3509577.7500\n",
      "Epoch [426/1000], Train Loss: 13426400.0000, Val Loss: 3510879.2500\n",
      "Epoch [427/1000], Train Loss: 13422580.0000, Val Loss: 3514277.5000\n",
      "Epoch [428/1000], Train Loss: 13422988.0000, Val Loss: 3513134.5000\n",
      "Epoch [429/1000], Train Loss: 13421984.0000, Val Loss: 3511844.0000\n",
      "Epoch [430/1000], Train Loss: 13423168.0000, Val Loss: 3512658.7500\n",
      "Epoch [431/1000], Train Loss: 13420680.0000, Val Loss: 3513321.7500\n",
      "Epoch [432/1000], Train Loss: 13421695.0000, Val Loss: 3511672.0000\n",
      "Epoch [433/1000], Train Loss: 13419993.0000, Val Loss: 3510874.7500\n",
      "Epoch [434/1000], Train Loss: 13422758.0000, Val Loss: 3512277.7500\n",
      "Epoch [435/1000], Train Loss: 13420760.0000, Val Loss: 3511793.0000\n",
      "Epoch [436/1000], Train Loss: 13422313.0000, Val Loss: 3511061.5000\n",
      "Epoch [437/1000], Train Loss: 13421474.0000, Val Loss: 3512283.2500\n",
      "Epoch [438/1000], Train Loss: 13418424.0000, Val Loss: 3513055.7500\n",
      "Epoch [439/1000], Train Loss: 13420299.0000, Val Loss: 3511513.2500\n",
      "Epoch [440/1000], Train Loss: 13419697.0000, Val Loss: 3511412.0000\n",
      "Epoch [441/1000], Train Loss: 13421350.0000, Val Loss: 3512902.2500\n",
      "Epoch [442/1000], Train Loss: 13418567.0000, Val Loss: 3511839.0000\n",
      "Epoch [443/1000], Train Loss: 13418529.0000, Val Loss: 3510299.5000\n",
      "Epoch [444/1000], Train Loss: 13419684.0000, Val Loss: 3511406.7500\n",
      "Epoch [445/1000], Train Loss: 13419437.0000, Val Loss: 3511922.0000\n",
      "Epoch [446/1000], Train Loss: 13417946.0000, Val Loss: 3511649.0000\n",
      "Epoch [447/1000], Train Loss: 13418350.0000, Val Loss: 3511448.0000\n",
      "Epoch [448/1000], Train Loss: 13417544.0000, Val Loss: 3512610.5000\n",
      "Epoch [449/1000], Train Loss: 13418631.0000, Val Loss: 3510693.0000\n",
      "Epoch [450/1000], Train Loss: 13417199.0000, Val Loss: 3509576.2500\n",
      "Epoch [451/1000], Train Loss: 13419090.0000, Val Loss: 3510644.2500\n",
      "Epoch [452/1000], Train Loss: 13418048.0000, Val Loss: 3511580.2500\n",
      "Epoch [453/1000], Train Loss: 13419731.0000, Val Loss: 3509794.7500\n",
      "Epoch [454/1000], Train Loss: 13417916.0000, Val Loss: 3511221.0000\n",
      "Epoch [455/1000], Train Loss: 13417594.0000, Val Loss: 3512806.2500\n",
      "Epoch [456/1000], Train Loss: 13419610.0000, Val Loss: 3510864.7500\n",
      "Epoch [457/1000], Train Loss: 13417406.0000, Val Loss: 3510684.0000\n",
      "Epoch [458/1000], Train Loss: 13419243.0000, Val Loss: 3513079.5000\n",
      "Epoch [459/1000], Train Loss: 13417117.0000, Val Loss: 3512402.5000\n",
      "Epoch [460/1000], Train Loss: 13417049.0000, Val Loss: 3509763.7500\n",
      "Epoch [461/1000], Train Loss: 13415789.0000, Val Loss: 3509600.0000\n",
      "Epoch [462/1000], Train Loss: 13416598.0000, Val Loss: 3511383.0000\n",
      "Epoch [463/1000], Train Loss: 13419214.0000, Val Loss: 3509207.7500\n",
      "Epoch [464/1000], Train Loss: 13415827.0000, Val Loss: 3507055.2500\n",
      "Epoch [465/1000], Train Loss: 13418706.0000, Val Loss: 3509571.5000\n",
      "Epoch [466/1000], Train Loss: 13414778.0000, Val Loss: 3511941.2500\n",
      "Epoch [467/1000], Train Loss: 13417809.0000, Val Loss: 3508975.0000\n",
      "Epoch [468/1000], Train Loss: 13418272.0000, Val Loss: 3508483.2500\n",
      "Epoch [469/1000], Train Loss: 13418546.0000, Val Loss: 3510906.2500\n",
      "Epoch [470/1000], Train Loss: 13418422.0000, Val Loss: 3510008.2500\n",
      "Epoch [471/1000], Train Loss: 13413725.0000, Val Loss: 3508297.0000\n",
      "Epoch [472/1000], Train Loss: 13416991.0000, Val Loss: 3508377.7500\n",
      "Epoch [473/1000], Train Loss: 13414103.0000, Val Loss: 3509954.2500\n",
      "Epoch [474/1000], Train Loss: 13416735.0000, Val Loss: 3508705.7500\n",
      "Epoch [475/1000], Train Loss: 13413776.0000, Val Loss: 3507174.7500\n",
      "Epoch [476/1000], Train Loss: 13414393.0000, Val Loss: 3507473.5000\n",
      "Epoch [477/1000], Train Loss: 13413970.0000, Val Loss: 3509735.7500\n",
      "Epoch [478/1000], Train Loss: 13418257.0000, Val Loss: 3509836.0000\n",
      "Epoch [479/1000], Train Loss: 13416495.0000, Val Loss: 3507836.2500\n",
      "Epoch [480/1000], Train Loss: 13415606.0000, Val Loss: 3507000.0000\n",
      "Epoch [481/1000], Train Loss: 13415423.0000, Val Loss: 3508402.7500\n",
      "Epoch [482/1000], Train Loss: 13412621.0000, Val Loss: 3508657.7500\n",
      "Epoch [483/1000], Train Loss: 13414068.0000, Val Loss: 3506781.7500\n",
      "Epoch [484/1000], Train Loss: 13412251.0000, Val Loss: 3506204.5000\n",
      "Epoch [485/1000], Train Loss: 13412337.0000, Val Loss: 3507974.0000\n",
      "Epoch [486/1000], Train Loss: 13413096.0000, Val Loss: 3507022.2500\n",
      "Epoch [487/1000], Train Loss: 13410190.0000, Val Loss: 3505161.2500\n",
      "Epoch [488/1000], Train Loss: 13411836.0000, Val Loss: 3506246.0000\n",
      "Epoch [489/1000], Train Loss: 13410966.0000, Val Loss: 3507759.2500\n",
      "Epoch [490/1000], Train Loss: 13413009.0000, Val Loss: 3507278.2500\n",
      "Epoch [491/1000], Train Loss: 13411324.0000, Val Loss: 3506366.7500\n",
      "Epoch [492/1000], Train Loss: 13410489.0000, Val Loss: 3506558.0000\n",
      "Epoch [493/1000], Train Loss: 13411379.0000, Val Loss: 3505905.0000\n",
      "Epoch [494/1000], Train Loss: 13406960.0000, Val Loss: 3505489.2500\n",
      "Epoch [495/1000], Train Loss: 13408563.0000, Val Loss: 3506112.7500\n",
      "Epoch [496/1000], Train Loss: 13408606.0000, Val Loss: 3506278.0000\n",
      "Epoch [497/1000], Train Loss: 13407473.0000, Val Loss: 3505829.0000\n",
      "Epoch [498/1000], Train Loss: 13410371.0000, Val Loss: 3505934.0000\n",
      "Epoch [499/1000], Train Loss: 13409112.0000, Val Loss: 3506294.7500\n",
      "Epoch [500/1000], Train Loss: 13408259.0000, Val Loss: 3504978.2500\n",
      "Epoch [501/1000], Train Loss: 13404862.0000, Val Loss: 3504708.5000\n",
      "Epoch [502/1000], Train Loss: 13405662.0000, Val Loss: 3505120.5000\n",
      "Epoch [503/1000], Train Loss: 13409473.0000, Val Loss: 3504746.7500\n",
      "Epoch [504/1000], Train Loss: 13406906.0000, Val Loss: 3504580.7500\n",
      "Epoch [505/1000], Train Loss: 13407032.0000, Val Loss: 3505437.2500\n",
      "Epoch [506/1000], Train Loss: 13406999.0000, Val Loss: 3505162.7500\n",
      "Epoch [507/1000], Train Loss: 13405836.0000, Val Loss: 3504186.2500\n",
      "Epoch [508/1000], Train Loss: 13407168.0000, Val Loss: 3504193.2500\n",
      "Epoch [509/1000], Train Loss: 13407221.0000, Val Loss: 3504823.5000\n",
      "Epoch [510/1000], Train Loss: 13405436.0000, Val Loss: 3502614.7500\n",
      "Epoch [511/1000], Train Loss: 13408842.0000, Val Loss: 3503418.7500\n",
      "Epoch [512/1000], Train Loss: 13403746.0000, Val Loss: 3503873.2500\n",
      "Epoch [513/1000], Train Loss: 13411746.0000, Val Loss: 3501848.7500\n",
      "Epoch [514/1000], Train Loss: 13411512.0000, Val Loss: 3502363.2500\n",
      "Epoch [515/1000], Train Loss: 13405099.0000, Val Loss: 3503268.5000\n",
      "Epoch [516/1000], Train Loss: 13403844.0000, Val Loss: 3502788.5000\n",
      "Epoch [517/1000], Train Loss: 13400148.0000, Val Loss: 3502865.5000\n",
      "Epoch [518/1000], Train Loss: 13407424.0000, Val Loss: 3504680.7500\n",
      "Epoch [519/1000], Train Loss: 13406388.0000, Val Loss: 3503599.2500\n",
      "Epoch [520/1000], Train Loss: 13403832.0000, Val Loss: 3503179.0000\n",
      "Epoch [521/1000], Train Loss: 13402345.0000, Val Loss: 3503060.7500\n",
      "Epoch [522/1000], Train Loss: 13403580.0000, Val Loss: 3502051.7500\n",
      "Epoch [523/1000], Train Loss: 13401068.0000, Val Loss: 3500261.7500\n",
      "Epoch [524/1000], Train Loss: 13405019.0000, Val Loss: 3500585.5000\n",
      "Epoch [525/1000], Train Loss: 13400783.0000, Val Loss: 3500979.7500\n",
      "Epoch [526/1000], Train Loss: 13403184.0000, Val Loss: 3501115.7500\n",
      "Epoch [527/1000], Train Loss: 13403852.0000, Val Loss: 3502447.0000\n",
      "Epoch [528/1000], Train Loss: 13401396.0000, Val Loss: 3502621.2500\n",
      "Epoch [529/1000], Train Loss: 13399419.0000, Val Loss: 3501506.2500\n",
      "Epoch [530/1000], Train Loss: 13402802.0000, Val Loss: 3501149.2500\n",
      "Epoch [531/1000], Train Loss: 13397932.0000, Val Loss: 3502112.7500\n",
      "Epoch [532/1000], Train Loss: 13405611.0000, Val Loss: 3502155.0000\n",
      "Epoch [533/1000], Train Loss: 13399755.0000, Val Loss: 3501157.5000\n",
      "Epoch [534/1000], Train Loss: 13404976.0000, Val Loss: 3502091.7500\n",
      "Epoch [535/1000], Train Loss: 13405742.0000, Val Loss: 3500213.2500\n",
      "Epoch [536/1000], Train Loss: 13399281.0000, Val Loss: 3498436.2500\n",
      "Epoch [537/1000], Train Loss: 13409255.0000, Val Loss: 3499945.0000\n",
      "Epoch [538/1000], Train Loss: 13397769.0000, Val Loss: 3501552.5000\n",
      "Epoch [539/1000], Train Loss: 13404178.0000, Val Loss: 3500726.2500\n",
      "Epoch [540/1000], Train Loss: 13400573.0000, Val Loss: 3500118.7500\n",
      "Epoch [541/1000], Train Loss: 13408589.0000, Val Loss: 3500878.7500\n",
      "Epoch [542/1000], Train Loss: 13400374.0000, Val Loss: 3500966.7500\n",
      "Epoch [543/1000], Train Loss: 13400984.0000, Val Loss: 3499495.0000\n",
      "Epoch [544/1000], Train Loss: 13400600.0000, Val Loss: 3499126.7500\n",
      "Epoch [545/1000], Train Loss: 13395506.0000, Val Loss: 3500312.7500\n",
      "Epoch [546/1000], Train Loss: 13402280.0000, Val Loss: 3499871.2500\n",
      "Epoch [547/1000], Train Loss: 13397797.0000, Val Loss: 3498525.0000\n",
      "Epoch [548/1000], Train Loss: 13401587.0000, Val Loss: 3498322.5000\n",
      "Epoch [549/1000], Train Loss: 13395046.0000, Val Loss: 3498787.2500\n",
      "Epoch [550/1000], Train Loss: 13399217.0000, Val Loss: 3498197.7500\n",
      "Epoch [551/1000], Train Loss: 13392934.0000, Val Loss: 3497442.0000\n",
      "Epoch [552/1000], Train Loss: 13402164.0000, Val Loss: 3498364.7500\n",
      "Epoch [553/1000], Train Loss: 13395639.0000, Val Loss: 3500020.0000\n",
      "Epoch [554/1000], Train Loss: 13402723.0000, Val Loss: 3499383.5000\n",
      "Epoch [555/1000], Train Loss: 13391671.0000, Val Loss: 3498247.0000\n",
      "Epoch [556/1000], Train Loss: 13399446.0000, Val Loss: 3497911.7500\n",
      "Epoch [557/1000], Train Loss: 13397226.0000, Val Loss: 3499052.0000\n",
      "Epoch [558/1000], Train Loss: 13400157.0000, Val Loss: 3498882.7500\n",
      "Epoch [559/1000], Train Loss: 13395230.0000, Val Loss: 3498377.2500\n",
      "Epoch [560/1000], Train Loss: 13393877.0000, Val Loss: 3498185.2500\n",
      "Epoch [561/1000], Train Loss: 13395758.0000, Val Loss: 3499050.7500\n",
      "Epoch [562/1000], Train Loss: 13397061.0000, Val Loss: 3498703.5000\n",
      "Epoch [563/1000], Train Loss: 13391610.0000, Val Loss: 3497482.7500\n",
      "Epoch [564/1000], Train Loss: 13398862.0000, Val Loss: 3497326.2500\n",
      "Epoch [565/1000], Train Loss: 13394310.0000, Val Loss: 3498506.7500\n",
      "Epoch [566/1000], Train Loss: 13396915.0000, Val Loss: 3498415.5000\n",
      "Epoch [567/1000], Train Loss: 13394476.0000, Val Loss: 3497140.7500\n",
      "Epoch [568/1000], Train Loss: 13394639.0000, Val Loss: 3496838.0000\n",
      "Epoch [569/1000], Train Loss: 13392742.0000, Val Loss: 3497738.7500\n",
      "Epoch [570/1000], Train Loss: 13394800.0000, Val Loss: 3497363.0000\n",
      "Epoch [571/1000], Train Loss: 13394219.0000, Val Loss: 3495908.7500\n",
      "Epoch [572/1000], Train Loss: 13392024.0000, Val Loss: 3495467.7500\n",
      "Epoch [573/1000], Train Loss: 13396768.0000, Val Loss: 3496226.7500\n",
      "Epoch [574/1000], Train Loss: 13389789.0000, Val Loss: 3497265.5000\n",
      "Epoch [575/1000], Train Loss: 13396071.0000, Val Loss: 3497053.7500\n",
      "Epoch [576/1000], Train Loss: 13387460.0000, Val Loss: 3496934.7500\n",
      "Epoch [577/1000], Train Loss: 13393113.0000, Val Loss: 3496980.0000\n",
      "Epoch [578/1000], Train Loss: 13385839.0000, Val Loss: 3496937.2500\n",
      "Epoch [579/1000], Train Loss: 13390450.0000, Val Loss: 3495786.5000\n",
      "Epoch [580/1000], Train Loss: 13384937.0000, Val Loss: 3495071.0000\n",
      "Epoch [581/1000], Train Loss: 13389406.0000, Val Loss: 3495501.7500\n",
      "Epoch [582/1000], Train Loss: 13386074.0000, Val Loss: 3496352.7500\n",
      "Epoch [583/1000], Train Loss: 13388317.0000, Val Loss: 3496574.0000\n",
      "Epoch [584/1000], Train Loss: 13385894.0000, Val Loss: 3496117.0000\n",
      "Epoch [585/1000], Train Loss: 13385169.0000, Val Loss: 3495699.2500\n",
      "Epoch [586/1000], Train Loss: 13388418.0000, Val Loss: 3495743.5000\n",
      "Epoch [587/1000], Train Loss: 13383486.0000, Val Loss: 3495270.2500\n",
      "Epoch [588/1000], Train Loss: 13386250.0000, Val Loss: 3495609.2500\n",
      "Epoch [589/1000], Train Loss: 13384908.0000, Val Loss: 3495943.5000\n",
      "Epoch [590/1000], Train Loss: 13386109.0000, Val Loss: 3496289.5000\n",
      "Epoch [591/1000], Train Loss: 13383603.0000, Val Loss: 3496165.2500\n",
      "Epoch [592/1000], Train Loss: 13381539.0000, Val Loss: 3495053.0000\n",
      "Epoch [593/1000], Train Loss: 13382129.0000, Val Loss: 3494462.7500\n",
      "Epoch [594/1000], Train Loss: 13383463.0000, Val Loss: 3495607.2500\n",
      "Epoch [595/1000], Train Loss: 13383285.0000, Val Loss: 3496823.7500\n",
      "Epoch [596/1000], Train Loss: 13380696.0000, Val Loss: 3497789.0000\n",
      "Epoch [597/1000], Train Loss: 13380927.0000, Val Loss: 3496830.7500\n",
      "Epoch [598/1000], Train Loss: 13382000.0000, Val Loss: 3496222.0000\n",
      "Epoch [599/1000], Train Loss: 13381218.0000, Val Loss: 3494824.2500\n",
      "Epoch [600/1000], Train Loss: 13379638.0000, Val Loss: 3494054.0000\n",
      "Epoch [601/1000], Train Loss: 13379587.0000, Val Loss: 3494675.7500\n",
      "Epoch [602/1000], Train Loss: 13378879.0000, Val Loss: 3496054.7500\n",
      "Epoch [603/1000], Train Loss: 13381799.0000, Val Loss: 3496761.2500\n",
      "Epoch [604/1000], Train Loss: 13383721.0000, Val Loss: 3496407.0000\n",
      "Epoch [605/1000], Train Loss: 13380330.0000, Val Loss: 3495677.2500\n",
      "Epoch [606/1000], Train Loss: 13378162.0000, Val Loss: 3495355.2500\n",
      "Epoch [607/1000], Train Loss: 13379204.0000, Val Loss: 3495045.5000\n",
      "Epoch [608/1000], Train Loss: 13378261.0000, Val Loss: 3494505.2500\n",
      "Epoch [609/1000], Train Loss: 13375906.0000, Val Loss: 3493727.5000\n",
      "Epoch [610/1000], Train Loss: 13381210.0000, Val Loss: 3493943.5000\n",
      "Epoch [611/1000], Train Loss: 13377479.0000, Val Loss: 3493556.5000\n",
      "Epoch [612/1000], Train Loss: 13377949.0000, Val Loss: 3493162.5000\n",
      "Epoch [613/1000], Train Loss: 13377023.0000, Val Loss: 3494081.2500\n",
      "Epoch [614/1000], Train Loss: 13376809.0000, Val Loss: 3495385.2500\n",
      "Epoch [615/1000], Train Loss: 13378307.0000, Val Loss: 3495445.5000\n",
      "Epoch [616/1000], Train Loss: 13376261.0000, Val Loss: 3495389.7500\n",
      "Epoch [617/1000], Train Loss: 13376827.0000, Val Loss: 3495771.0000\n",
      "Epoch [618/1000], Train Loss: 13376664.0000, Val Loss: 3494540.5000\n",
      "Epoch [619/1000], Train Loss: 13372802.0000, Val Loss: 3492450.0000\n",
      "Epoch [620/1000], Train Loss: 13375466.0000, Val Loss: 3492442.5000\n",
      "Epoch [621/1000], Train Loss: 13371916.0000, Val Loss: 3493264.5000\n",
      "Epoch [622/1000], Train Loss: 13374722.0000, Val Loss: 3493495.5000\n",
      "Epoch [623/1000], Train Loss: 13373292.0000, Val Loss: 3494379.7500\n",
      "Epoch [624/1000], Train Loss: 13370039.0000, Val Loss: 3495380.2500\n",
      "Epoch [625/1000], Train Loss: 13371447.0000, Val Loss: 3494725.0000\n",
      "Epoch [626/1000], Train Loss: 13373304.0000, Val Loss: 3493071.2500\n",
      "Epoch [627/1000], Train Loss: 13378088.0000, Val Loss: 3493186.7500\n",
      "Epoch [628/1000], Train Loss: 13368758.0000, Val Loss: 3493263.5000\n",
      "Epoch [629/1000], Train Loss: 13369967.0000, Val Loss: 3492275.0000\n",
      "Epoch [630/1000], Train Loss: 13371688.0000, Val Loss: 3492085.5000\n",
      "Epoch [631/1000], Train Loss: 13368439.0000, Val Loss: 3492927.5000\n",
      "Epoch [632/1000], Train Loss: 13374136.0000, Val Loss: 3493347.2500\n",
      "Epoch [633/1000], Train Loss: 13366872.0000, Val Loss: 3493107.5000\n",
      "Epoch [634/1000], Train Loss: 13372218.0000, Val Loss: 3493085.2500\n",
      "Epoch [635/1000], Train Loss: 13365294.0000, Val Loss: 3493229.2500\n",
      "Epoch [636/1000], Train Loss: 13372160.0000, Val Loss: 3491648.5000\n",
      "Epoch [637/1000], Train Loss: 13368619.0000, Val Loss: 3490932.5000\n",
      "Epoch [638/1000], Train Loss: 13363965.0000, Val Loss: 3491491.7500\n",
      "Epoch [639/1000], Train Loss: 13368409.0000, Val Loss: 3492365.7500\n",
      "Epoch [640/1000], Train Loss: 13369163.0000, Val Loss: 3491824.0000\n",
      "Epoch [641/1000], Train Loss: 13371867.0000, Val Loss: 3492963.7500\n",
      "Epoch [642/1000], Train Loss: 13366347.0000, Val Loss: 3494025.0000\n",
      "Epoch [643/1000], Train Loss: 13366838.0000, Val Loss: 3492591.2500\n",
      "Epoch [644/1000], Train Loss: 13362600.0000, Val Loss: 3491673.2500\n",
      "Epoch [645/1000], Train Loss: 13363419.0000, Val Loss: 3492074.2500\n",
      "Epoch [646/1000], Train Loss: 13364002.0000, Val Loss: 3491220.5000\n",
      "Epoch [647/1000], Train Loss: 13359791.0000, Val Loss: 3490915.0000\n",
      "Epoch [648/1000], Train Loss: 13366391.0000, Val Loss: 3491730.2500\n",
      "Epoch [649/1000], Train Loss: 13360112.0000, Val Loss: 3492613.0000\n",
      "Epoch [650/1000], Train Loss: 13360502.0000, Val Loss: 3491746.0000\n",
      "Epoch [651/1000], Train Loss: 13364193.0000, Val Loss: 3491635.0000\n",
      "Epoch [652/1000], Train Loss: 13360802.0000, Val Loss: 3491901.7500\n",
      "Epoch [653/1000], Train Loss: 13361046.0000, Val Loss: 3490683.5000\n",
      "Epoch [654/1000], Train Loss: 13363802.0000, Val Loss: 3490374.0000\n",
      "Epoch [655/1000], Train Loss: 13360761.0000, Val Loss: 3490986.7500\n",
      "Epoch [656/1000], Train Loss: 13358858.0000, Val Loss: 3490931.7500\n",
      "Epoch [657/1000], Train Loss: 13359522.0000, Val Loss: 3491182.7500\n",
      "Epoch [658/1000], Train Loss: 13358637.0000, Val Loss: 3492605.7500\n",
      "Epoch [659/1000], Train Loss: 13356373.0000, Val Loss: 3492271.5000\n",
      "Epoch [660/1000], Train Loss: 13357796.0000, Val Loss: 3492050.0000\n",
      "Epoch [661/1000], Train Loss: 13359864.0000, Val Loss: 3492956.0000\n",
      "Epoch [662/1000], Train Loss: 13357383.0000, Val Loss: 3492842.7500\n",
      "Epoch [663/1000], Train Loss: 13356639.0000, Val Loss: 3491816.7500\n",
      "Epoch [664/1000], Train Loss: 13357625.0000, Val Loss: 3490950.0000\n",
      "Epoch [665/1000], Train Loss: 13353077.0000, Val Loss: 3489693.7500\n",
      "Epoch [666/1000], Train Loss: 13353820.0000, Val Loss: 3488313.7500\n",
      "Epoch [667/1000], Train Loss: 13358807.0000, Val Loss: 3489532.5000\n",
      "Epoch [668/1000], Train Loss: 13356772.0000, Val Loss: 3491291.2500\n",
      "Epoch [669/1000], Train Loss: 13356767.0000, Val Loss: 3491163.0000\n",
      "Epoch [670/1000], Train Loss: 13359190.0000, Val Loss: 3492067.0000\n",
      "Epoch [671/1000], Train Loss: 13355544.0000, Val Loss: 3491719.0000\n",
      "Epoch [672/1000], Train Loss: 13354139.0000, Val Loss: 3490156.7500\n",
      "Epoch [673/1000], Train Loss: 13354282.0000, Val Loss: 3490240.2500\n",
      "Epoch [674/1000], Train Loss: 13348971.0000, Val Loss: 3490372.7500\n",
      "Epoch [675/1000], Train Loss: 13350544.0000, Val Loss: 3490468.7500\n",
      "Epoch [676/1000], Train Loss: 13355344.0000, Val Loss: 3490815.7500\n",
      "Epoch [677/1000], Train Loss: 13352204.0000, Val Loss: 3491479.5000\n",
      "Epoch [678/1000], Train Loss: 13352885.0000, Val Loss: 3490395.7500\n",
      "Epoch [679/1000], Train Loss: 13352685.0000, Val Loss: 3490151.5000\n",
      "Epoch [680/1000], Train Loss: 13350301.0000, Val Loss: 3489741.2500\n",
      "Epoch [681/1000], Train Loss: 13353005.0000, Val Loss: 3490005.0000\n",
      "Epoch [682/1000], Train Loss: 13349378.0000, Val Loss: 3490550.7500\n",
      "Epoch [683/1000], Train Loss: 13351539.0000, Val Loss: 3489734.0000\n",
      "Epoch [684/1000], Train Loss: 13352924.0000, Val Loss: 3490042.2500\n",
      "Epoch [685/1000], Train Loss: 13348352.0000, Val Loss: 3489502.5000\n",
      "Epoch [686/1000], Train Loss: 13349422.0000, Val Loss: 3488711.0000\n",
      "Epoch [687/1000], Train Loss: 13343596.0000, Val Loss: 3489277.2500\n",
      "Epoch [688/1000], Train Loss: 13348190.0000, Val Loss: 3490087.5000\n",
      "Epoch [689/1000], Train Loss: 13346153.0000, Val Loss: 3490678.2500\n",
      "Epoch [690/1000], Train Loss: 13350880.0000, Val Loss: 3492185.0000\n",
      "Epoch [691/1000], Train Loss: 13346294.0000, Val Loss: 3491495.5000\n",
      "Epoch [692/1000], Train Loss: 13347956.0000, Val Loss: 3491273.2500\n",
      "Epoch [693/1000], Train Loss: 13344513.0000, Val Loss: 3491136.5000\n",
      "Epoch [694/1000], Train Loss: 13349260.0000, Val Loss: 3488944.5000\n",
      "Epoch [695/1000], Train Loss: 13351235.0000, Val Loss: 3489939.7500\n",
      "Epoch [696/1000], Train Loss: 13346086.0000, Val Loss: 3490360.0000\n",
      "Epoch [697/1000], Train Loss: 13348248.0000, Val Loss: 3489645.0000\n",
      "Epoch [698/1000], Train Loss: 13340791.0000, Val Loss: 3489152.7500\n",
      "Epoch [699/1000], Train Loss: 13351292.0000, Val Loss: 3490557.5000\n",
      "Epoch [700/1000], Train Loss: 13348657.0000, Val Loss: 3489506.7500\n",
      "Epoch [701/1000], Train Loss: 13342463.0000, Val Loss: 3489816.7500\n",
      "Epoch [702/1000], Train Loss: 13343367.0000, Val Loss: 3490394.2500\n",
      "Epoch [703/1000], Train Loss: 13344730.0000, Val Loss: 3489831.0000\n",
      "Epoch [704/1000], Train Loss: 13342112.0000, Val Loss: 3488606.5000\n",
      "Epoch [705/1000], Train Loss: 13342779.0000, Val Loss: 3488894.7500\n",
      "Epoch [706/1000], Train Loss: 13340929.0000, Val Loss: 3489843.7500\n",
      "Epoch [707/1000], Train Loss: 13340242.0000, Val Loss: 3490420.2500\n",
      "Epoch [708/1000], Train Loss: 13341561.0000, Val Loss: 3490324.0000\n",
      "Epoch [709/1000], Train Loss: 13340244.0000, Val Loss: 3491561.0000\n",
      "Epoch [710/1000], Train Loss: 13341731.0000, Val Loss: 3491207.7500\n",
      "Epoch [711/1000], Train Loss: 13340778.0000, Val Loss: 3488673.2500\n",
      "Epoch [712/1000], Train Loss: 13354476.0000, Val Loss: 3489572.5000\n",
      "Epoch [713/1000], Train Loss: 13341776.0000, Val Loss: 3488504.7500\n",
      "Epoch [714/1000], Train Loss: 13343497.0000, Val Loss: 3487487.5000\n",
      "Epoch [715/1000], Train Loss: 13350046.0000, Val Loss: 3489446.2500\n",
      "Epoch [716/1000], Train Loss: 13342385.0000, Val Loss: 3490003.0000\n",
      "Epoch [717/1000], Train Loss: 13341260.0000, Val Loss: 3488869.7500\n",
      "Epoch [718/1000], Train Loss: 13341133.0000, Val Loss: 3489705.2500\n",
      "Epoch [719/1000], Train Loss: 13336914.0000, Val Loss: 3490530.0000\n",
      "Epoch [720/1000], Train Loss: 13341899.0000, Val Loss: 3489405.0000\n",
      "Epoch [721/1000], Train Loss: 13337442.0000, Val Loss: 3488942.2500\n",
      "Epoch [722/1000], Train Loss: 13341110.0000, Val Loss: 3489409.5000\n",
      "Epoch [723/1000], Train Loss: 13340523.0000, Val Loss: 3489317.2500\n",
      "Epoch [724/1000], Train Loss: 13339446.0000, Val Loss: 3488327.2500\n",
      "Epoch [725/1000], Train Loss: 13340912.0000, Val Loss: 3489477.5000\n",
      "Epoch [726/1000], Train Loss: 13337634.0000, Val Loss: 3489131.0000\n",
      "Epoch [727/1000], Train Loss: 13336905.0000, Val Loss: 3487899.7500\n",
      "Epoch [728/1000], Train Loss: 13338578.0000, Val Loss: 3488526.0000\n",
      "Epoch [729/1000], Train Loss: 13333128.0000, Val Loss: 3488622.7500\n",
      "Epoch [730/1000], Train Loss: 13335642.0000, Val Loss: 3487242.7500\n",
      "Epoch [731/1000], Train Loss: 13340327.0000, Val Loss: 3489061.5000\n",
      "Epoch [732/1000], Train Loss: 13338086.0000, Val Loss: 3490000.0000\n",
      "Epoch [733/1000], Train Loss: 13333860.0000, Val Loss: 3489684.2500\n",
      "Epoch [734/1000], Train Loss: 13338004.0000, Val Loss: 3491309.7500\n",
      "Epoch [735/1000], Train Loss: 13337161.0000, Val Loss: 3492561.5000\n",
      "Epoch [736/1000], Train Loss: 13342254.0000, Val Loss: 3489944.2500\n",
      "Epoch [737/1000], Train Loss: 13338526.0000, Val Loss: 3489097.2500\n",
      "Epoch [738/1000], Train Loss: 13337305.0000, Val Loss: 3489602.5000\n",
      "Epoch [739/1000], Train Loss: 13338782.0000, Val Loss: 3488715.5000\n",
      "Epoch [740/1000], Train Loss: 13333615.0000, Val Loss: 3487932.7500\n",
      "Epoch [741/1000], Train Loss: 13343991.0000, Val Loss: 3490020.0000\n",
      "Epoch [742/1000], Train Loss: 13334384.0000, Val Loss: 3490803.0000\n",
      "Epoch [743/1000], Train Loss: 13336722.0000, Val Loss: 3489143.2500\n",
      "Epoch [744/1000], Train Loss: 13336256.0000, Val Loss: 3489160.7500\n",
      "Epoch [745/1000], Train Loss: 13331541.0000, Val Loss: 3490010.7500\n",
      "Epoch [746/1000], Train Loss: 13334732.0000, Val Loss: 3488951.7500\n",
      "Epoch [747/1000], Train Loss: 13331950.0000, Val Loss: 3488297.2500\n",
      "Epoch [748/1000], Train Loss: 13330036.0000, Val Loss: 3488079.5000\n",
      "Epoch [749/1000], Train Loss: 13333234.0000, Val Loss: 3487827.5000\n",
      "Epoch [750/1000], Train Loss: 13330179.0000, Val Loss: 3487076.7500\n",
      "Epoch [751/1000], Train Loss: 13333386.0000, Val Loss: 3487877.7500\n",
      "Epoch [752/1000], Train Loss: 13331251.0000, Val Loss: 3490303.7500\n",
      "Epoch [753/1000], Train Loss: 13334503.0000, Val Loss: 3489665.7500\n",
      "Epoch [754/1000], Train Loss: 13331814.0000, Val Loss: 3489775.5000\n",
      "Epoch [755/1000], Train Loss: 13327798.0000, Val Loss: 3490038.5000\n",
      "Epoch [756/1000], Train Loss: 13332288.0000, Val Loss: 3488867.5000\n",
      "Epoch [757/1000], Train Loss: 13328347.0000, Val Loss: 3488686.2500\n",
      "Epoch [758/1000], Train Loss: 13329510.0000, Val Loss: 3489802.5000\n",
      "Epoch [759/1000], Train Loss: 13329110.0000, Val Loss: 3490118.0000\n",
      "Epoch [760/1000], Train Loss: 13330598.0000, Val Loss: 3488844.7500\n",
      "Epoch [761/1000], Train Loss: 13328929.0000, Val Loss: 3489050.7500\n",
      "Epoch [762/1000], Train Loss: 13324787.0000, Val Loss: 3488700.5000\n",
      "Epoch [763/1000], Train Loss: 13328727.0000, Val Loss: 3488350.7500\n",
      "Epoch [764/1000], Train Loss: 13332631.0000, Val Loss: 3489262.2500\n",
      "Epoch [765/1000], Train Loss: 13325245.0000, Val Loss: 3490957.2500\n",
      "Epoch [766/1000], Train Loss: 13331191.0000, Val Loss: 3490275.7500\n",
      "Epoch [767/1000], Train Loss: 13332560.0000, Val Loss: 3490396.2500\n",
      "Epoch [768/1000], Train Loss: 13328475.0000, Val Loss: 3491052.2500\n",
      "Epoch [769/1000], Train Loss: 13329978.0000, Val Loss: 3489035.7500\n",
      "Epoch [770/1000], Train Loss: 13326980.0000, Val Loss: 3488162.7500\n",
      "Epoch [771/1000], Train Loss: 13327628.0000, Val Loss: 3488950.2500\n",
      "Epoch [772/1000], Train Loss: 13328446.0000, Val Loss: 3488590.7500\n",
      "Epoch [773/1000], Train Loss: 13327573.0000, Val Loss: 3487560.5000\n",
      "Epoch [774/1000], Train Loss: 13328122.0000, Val Loss: 3487843.0000\n",
      "Epoch [775/1000], Train Loss: 13321149.0000, Val Loss: 3487280.5000\n",
      "Epoch [776/1000], Train Loss: 13321530.0000, Val Loss: 3486854.7500\n",
      "Epoch [777/1000], Train Loss: 13324723.0000, Val Loss: 3487753.0000\n",
      "Epoch [778/1000], Train Loss: 13323512.0000, Val Loss: 3488020.2500\n",
      "Epoch [779/1000], Train Loss: 13327667.0000, Val Loss: 3487504.2500\n",
      "Epoch [780/1000], Train Loss: 13322571.0000, Val Loss: 3488105.0000\n",
      "Epoch [781/1000], Train Loss: 13321526.0000, Val Loss: 3490100.2500\n",
      "Epoch [782/1000], Train Loss: 13331768.0000, Val Loss: 3488779.5000\n",
      "Epoch [783/1000], Train Loss: 13320110.0000, Val Loss: 3488770.7500\n",
      "Epoch [784/1000], Train Loss: 13322207.0000, Val Loss: 3489019.2500\n",
      "Epoch [785/1000], Train Loss: 13320441.0000, Val Loss: 3489178.5000\n",
      "Epoch [786/1000], Train Loss: 13322951.0000, Val Loss: 3489320.7500\n",
      "Epoch [787/1000], Train Loss: 13322528.0000, Val Loss: 3488594.7500\n",
      "Epoch [788/1000], Train Loss: 13320492.0000, Val Loss: 3489583.7500\n",
      "Epoch [789/1000], Train Loss: 13324255.0000, Val Loss: 3490395.5000\n",
      "Epoch [790/1000], Train Loss: 13318301.0000, Val Loss: 3490179.5000\n",
      "Epoch [791/1000], Train Loss: 13323781.0000, Val Loss: 3489249.7500\n",
      "Epoch [792/1000], Train Loss: 13320917.0000, Val Loss: 3488463.7500\n",
      "Epoch [793/1000], Train Loss: 13318355.0000, Val Loss: 3487323.2500\n",
      "Epoch [794/1000], Train Loss: 13320163.0000, Val Loss: 3486950.2500\n",
      "Epoch [795/1000], Train Loss: 13319856.0000, Val Loss: 3487195.5000\n",
      "Epoch [796/1000], Train Loss: 13318227.0000, Val Loss: 3488388.5000\n",
      "Epoch [797/1000], Train Loss: 13318540.0000, Val Loss: 3490252.5000\n",
      "Epoch [798/1000], Train Loss: 13322330.0000, Val Loss: 3489563.0000\n",
      "Epoch [799/1000], Train Loss: 13324530.0000, Val Loss: 3490694.0000\n",
      "Epoch [800/1000], Train Loss: 13323521.0000, Val Loss: 3491261.7500\n",
      "Epoch [801/1000], Train Loss: 13327339.0000, Val Loss: 3488966.2500\n",
      "Epoch [802/1000], Train Loss: 13322214.0000, Val Loss: 3488443.2500\n",
      "Epoch [803/1000], Train Loss: 13320440.0000, Val Loss: 3489713.7500\n",
      "Epoch [804/1000], Train Loss: 13323415.0000, Val Loss: 3488433.0000\n",
      "Epoch [805/1000], Train Loss: 13319157.0000, Val Loss: 3487297.0000\n",
      "Epoch [806/1000], Train Loss: 13322682.0000, Val Loss: 3488024.5000\n",
      "Epoch [807/1000], Train Loss: 13316786.0000, Val Loss: 3488342.5000\n",
      "Epoch [808/1000], Train Loss: 13319089.0000, Val Loss: 3488119.2500\n",
      "Epoch [809/1000], Train Loss: 13318953.0000, Val Loss: 3489137.7500\n",
      "Epoch [810/1000], Train Loss: 13320136.0000, Val Loss: 3490440.2500\n",
      "Epoch [811/1000], Train Loss: 13317612.0000, Val Loss: 3490125.2500\n",
      "Epoch [812/1000], Train Loss: 13320277.0000, Val Loss: 3489832.0000\n",
      "Epoch [813/1000], Train Loss: 13323446.0000, Val Loss: 3491285.2500\n",
      "Epoch [814/1000], Train Loss: 13320863.0000, Val Loss: 3490460.0000\n",
      "Epoch [815/1000], Train Loss: 13311623.0000, Val Loss: 3489967.0000\n",
      "Epoch [816/1000], Train Loss: 13319468.0000, Val Loss: 3491650.2500\n",
      "Epoch [817/1000], Train Loss: 13318677.0000, Val Loss: 3492284.7500\n",
      "Epoch [818/1000], Train Loss: 13318244.0000, Val Loss: 3491407.5000\n",
      "Epoch [819/1000], Train Loss: 13317464.0000, Val Loss: 3491447.7500\n",
      "Epoch [820/1000], Train Loss: 13319404.0000, Val Loss: 3491890.0000\n",
      "Epoch [821/1000], Train Loss: 13317692.0000, Val Loss: 3490907.0000\n",
      "Epoch [822/1000], Train Loss: 13320340.0000, Val Loss: 3489565.5000\n",
      "Epoch [823/1000], Train Loss: 13314110.0000, Val Loss: 3490214.5000\n",
      "Epoch [824/1000], Train Loss: 13313206.0000, Val Loss: 3491190.7500\n",
      "Epoch [825/1000], Train Loss: 13318926.0000, Val Loss: 3490323.2500\n",
      "Epoch [826/1000], Train Loss: 13312433.0000, Val Loss: 3490055.2500\n",
      "Epoch [827/1000], Train Loss: 13323328.0000, Val Loss: 3491466.7500\n",
      "Epoch [828/1000], Train Loss: 13317961.0000, Val Loss: 3490855.7500\n",
      "Epoch [829/1000], Train Loss: 13316054.0000, Val Loss: 3489298.5000\n",
      "Epoch [830/1000], Train Loss: 13316440.0000, Val Loss: 3489373.5000\n",
      "Epoch [831/1000], Train Loss: 13316637.0000, Val Loss: 3489718.0000\n",
      "Epoch [832/1000], Train Loss: 13315224.0000, Val Loss: 3488867.5000\n",
      "Epoch [833/1000], Train Loss: 13315896.0000, Val Loss: 3488759.0000\n",
      "Epoch [834/1000], Train Loss: 13311005.0000, Val Loss: 3488861.5000\n",
      "Epoch [835/1000], Train Loss: 13311378.0000, Val Loss: 3489195.7500\n",
      "Epoch [836/1000], Train Loss: 13313223.0000, Val Loss: 3490275.0000\n",
      "Epoch [837/1000], Train Loss: 13317584.0000, Val Loss: 3490633.2500\n",
      "Epoch [838/1000], Train Loss: 13317672.0000, Val Loss: 3491001.5000\n",
      "Epoch [839/1000], Train Loss: 13312679.0000, Val Loss: 3490513.2500\n",
      "Epoch [840/1000], Train Loss: 13312462.0000, Val Loss: 3488841.5000\n",
      "Epoch [841/1000], Train Loss: 13314002.0000, Val Loss: 3488298.7500\n",
      "Epoch [842/1000], Train Loss: 13310629.0000, Val Loss: 3488224.2500\n",
      "Epoch [843/1000], Train Loss: 13315908.0000, Val Loss: 3487017.0000\n",
      "Epoch [844/1000], Train Loss: 13311464.0000, Val Loss: 3486602.5000\n",
      "Epoch [845/1000], Train Loss: 13310104.0000, Val Loss: 3486762.2500\n",
      "Epoch [846/1000], Train Loss: 13309034.0000, Val Loss: 3487415.7500\n",
      "Epoch [847/1000], Train Loss: 13311920.0000, Val Loss: 3487841.0000\n",
      "Epoch [848/1000], Train Loss: 13310393.0000, Val Loss: 3487377.2500\n",
      "Epoch [849/1000], Train Loss: 13308549.0000, Val Loss: 3487566.5000\n",
      "Epoch [850/1000], Train Loss: 13314994.0000, Val Loss: 3488617.0000\n",
      "Epoch [851/1000], Train Loss: 13324204.0000, Val Loss: 3487509.7500\n",
      "Epoch [852/1000], Train Loss: 13316367.0000, Val Loss: 3487023.5000\n",
      "Epoch [853/1000], Train Loss: 13315622.0000, Val Loss: 3488038.2500\n",
      "Epoch [854/1000], Train Loss: 13313193.0000, Val Loss: 3488241.0000\n",
      "Epoch [855/1000], Train Loss: 13310103.0000, Val Loss: 3488285.0000\n",
      "Epoch [856/1000], Train Loss: 13309941.0000, Val Loss: 3488735.5000\n",
      "Epoch [857/1000], Train Loss: 13309318.0000, Val Loss: 3488802.2500\n",
      "Epoch [858/1000], Train Loss: 13315044.0000, Val Loss: 3487887.7500\n",
      "Epoch [859/1000], Train Loss: 13309456.0000, Val Loss: 3487269.7500\n",
      "Epoch [860/1000], Train Loss: 13306722.0000, Val Loss: 3486240.7500\n",
      "Epoch [861/1000], Train Loss: 13305705.0000, Val Loss: 3486192.2500\n",
      "Epoch [862/1000], Train Loss: 13309974.0000, Val Loss: 3487212.5000\n",
      "Epoch [863/1000], Train Loss: 13314160.0000, Val Loss: 3487021.0000\n",
      "Epoch [864/1000], Train Loss: 13310725.0000, Val Loss: 3488127.2500\n",
      "Epoch [865/1000], Train Loss: 13312698.0000, Val Loss: 3490452.0000\n",
      "Epoch [866/1000], Train Loss: 13311489.0000, Val Loss: 3490737.5000\n",
      "Epoch [867/1000], Train Loss: 13309840.0000, Val Loss: 3490584.0000\n",
      "Epoch [868/1000], Train Loss: 13312795.0000, Val Loss: 3491347.2500\n",
      "Epoch [869/1000], Train Loss: 13310954.0000, Val Loss: 3490081.7500\n",
      "Epoch [870/1000], Train Loss: 13305301.0000, Val Loss: 3489137.0000\n",
      "Epoch [871/1000], Train Loss: 13308416.0000, Val Loss: 3488947.2500\n",
      "Epoch [872/1000], Train Loss: 13307660.0000, Val Loss: 3487658.5000\n",
      "Epoch [873/1000], Train Loss: 13311379.0000, Val Loss: 3487275.2500\n",
      "Epoch [874/1000], Train Loss: 13309730.0000, Val Loss: 3487668.2500\n",
      "Epoch [875/1000], Train Loss: 13308023.0000, Val Loss: 3488000.2500\n",
      "Epoch [876/1000], Train Loss: 13306153.0000, Val Loss: 3488906.7500\n",
      "Epoch [877/1000], Train Loss: 13308357.0000, Val Loss: 3489919.5000\n",
      "Epoch [878/1000], Train Loss: 13306330.0000, Val Loss: 3489961.2500\n",
      "Epoch [879/1000], Train Loss: 13307050.0000, Val Loss: 3488561.2500\n",
      "Epoch [880/1000], Train Loss: 13311661.0000, Val Loss: 3488006.2500\n",
      "Epoch [881/1000], Train Loss: 13311298.0000, Val Loss: 3488247.0000\n",
      "Epoch [882/1000], Train Loss: 13312211.0000, Val Loss: 3487088.5000\n",
      "Epoch [883/1000], Train Loss: 13310445.0000, Val Loss: 3487010.2500\n",
      "Epoch [884/1000], Train Loss: 13308766.0000, Val Loss: 3488567.0000\n",
      "Epoch [885/1000], Train Loss: 13310226.0000, Val Loss: 3488971.2500\n",
      "Epoch [886/1000], Train Loss: 13309351.0000, Val Loss: 3488742.5000\n",
      "Epoch [887/1000], Train Loss: 13306874.0000, Val Loss: 3488956.2500\n",
      "Epoch [888/1000], Train Loss: 13312346.0000, Val Loss: 3489717.5000\n",
      "Epoch [889/1000], Train Loss: 13310754.0000, Val Loss: 3488665.2500\n",
      "Epoch [890/1000], Train Loss: 13310160.0000, Val Loss: 3488222.0000\n",
      "Epoch [891/1000], Train Loss: 13306781.0000, Val Loss: 3488446.2500\n",
      "Epoch [892/1000], Train Loss: 13304194.0000, Val Loss: 3488404.0000\n",
      "Epoch [893/1000], Train Loss: 13307176.0000, Val Loss: 3487964.7500\n",
      "Epoch [894/1000], Train Loss: 13309730.0000, Val Loss: 3486992.0000\n",
      "Epoch [895/1000], Train Loss: 13305224.0000, Val Loss: 3487055.0000\n",
      "Epoch [896/1000], Train Loss: 13302696.0000, Val Loss: 3486625.2500\n",
      "Epoch [897/1000], Train Loss: 13309065.0000, Val Loss: 3486464.7500\n",
      "Epoch [898/1000], Train Loss: 13305627.0000, Val Loss: 3487280.5000\n",
      "Epoch [899/1000], Train Loss: 13305061.0000, Val Loss: 3487974.5000\n",
      "Epoch [900/1000], Train Loss: 13307267.0000, Val Loss: 3488136.5000\n",
      "Epoch [901/1000], Train Loss: 13306838.0000, Val Loss: 3488191.0000\n",
      "Epoch [902/1000], Train Loss: 13309062.0000, Val Loss: 3489798.7500\n",
      "Epoch [903/1000], Train Loss: 13307682.0000, Val Loss: 3490363.7500\n",
      "Epoch [904/1000], Train Loss: 13304688.0000, Val Loss: 3490452.2500\n",
      "Epoch [905/1000], Train Loss: 13304428.0000, Val Loss: 3491081.2500\n",
      "Epoch [906/1000], Train Loss: 13306509.0000, Val Loss: 3490203.7500\n",
      "Epoch [907/1000], Train Loss: 13302467.0000, Val Loss: 3489507.0000\n",
      "Epoch [908/1000], Train Loss: 13306728.0000, Val Loss: 3488839.2500\n",
      "Epoch [909/1000], Train Loss: 13302828.0000, Val Loss: 3489012.5000\n",
      "Epoch [910/1000], Train Loss: 13303396.0000, Val Loss: 3489080.0000\n",
      "Epoch [911/1000], Train Loss: 13301705.0000, Val Loss: 3490052.7500\n",
      "Epoch [912/1000], Train Loss: 13304270.0000, Val Loss: 3490793.5000\n",
      "Epoch [913/1000], Train Loss: 13302540.0000, Val Loss: 3490311.5000\n",
      "Epoch [914/1000], Train Loss: 13302198.0000, Val Loss: 3490364.0000\n",
      "Epoch [915/1000], Train Loss: 13303070.0000, Val Loss: 3489682.7500\n",
      "Epoch [916/1000], Train Loss: 13305977.0000, Val Loss: 3489085.2500\n",
      "Epoch [917/1000], Train Loss: 13301468.0000, Val Loss: 3488881.0000\n",
      "Epoch [918/1000], Train Loss: 13308207.0000, Val Loss: 3488969.7500\n",
      "Epoch [919/1000], Train Loss: 13306915.0000, Val Loss: 3487874.7500\n",
      "Epoch [920/1000], Train Loss: 13303278.0000, Val Loss: 3488277.5000\n",
      "Epoch [921/1000], Train Loss: 13299724.0000, Val Loss: 3488527.0000\n",
      "Epoch [922/1000], Train Loss: 13299424.0000, Val Loss: 3489056.7500\n",
      "Epoch [923/1000], Train Loss: 13301589.0000, Val Loss: 3489360.7500\n",
      "Epoch [924/1000], Train Loss: 13308149.0000, Val Loss: 3490790.7500\n",
      "Epoch [925/1000], Train Loss: 13305110.0000, Val Loss: 3490426.5000\n",
      "Epoch [926/1000], Train Loss: 13304643.0000, Val Loss: 3488712.0000\n",
      "Epoch [927/1000], Train Loss: 13311930.0000, Val Loss: 3489388.2500\n",
      "Epoch [928/1000], Train Loss: 13306504.0000, Val Loss: 3489119.2500\n",
      "Epoch [929/1000], Train Loss: 13305577.0000, Val Loss: 3487796.7500\n",
      "Epoch [930/1000], Train Loss: 13307828.0000, Val Loss: 3488848.5000\n",
      "Epoch [931/1000], Train Loss: 13302078.0000, Val Loss: 3489237.0000\n",
      "Epoch [932/1000], Train Loss: 13298040.0000, Val Loss: 3488888.0000\n",
      "Epoch [933/1000], Train Loss: 13305986.0000, Val Loss: 3489949.5000\n",
      "Epoch [934/1000], Train Loss: 13308216.0000, Val Loss: 3489386.2500\n",
      "Epoch [935/1000], Train Loss: 13302351.0000, Val Loss: 3487999.2500\n",
      "Epoch [936/1000], Train Loss: 13308720.0000, Val Loss: 3488199.7500\n",
      "Epoch [937/1000], Train Loss: 13305326.0000, Val Loss: 3487302.0000\n",
      "Epoch [938/1000], Train Loss: 13303900.0000, Val Loss: 3485587.2500\n",
      "Epoch [939/1000], Train Loss: 13306598.0000, Val Loss: 3485363.5000\n",
      "Epoch [940/1000], Train Loss: 13299577.0000, Val Loss: 3485283.5000\n",
      "Epoch [941/1000], Train Loss: 13307363.0000, Val Loss: 3484327.5000\n",
      "Epoch [942/1000], Train Loss: 13306715.0000, Val Loss: 3485748.0000\n",
      "Epoch [943/1000], Train Loss: 13297765.0000, Val Loss: 3488174.0000\n",
      "Epoch [944/1000], Train Loss: 13311460.0000, Val Loss: 3488496.7500\n",
      "Epoch [945/1000], Train Loss: 13298514.0000, Val Loss: 3488993.2500\n",
      "Epoch [946/1000], Train Loss: 13303366.0000, Val Loss: 3490189.0000\n",
      "Epoch [947/1000], Train Loss: 13302359.0000, Val Loss: 3490565.2500\n",
      "Epoch [948/1000], Train Loss: 13303489.0000, Val Loss: 3489429.0000\n",
      "Epoch [949/1000], Train Loss: 13305122.0000, Val Loss: 3488629.0000\n",
      "Epoch [950/1000], Train Loss: 13297036.0000, Val Loss: 3488707.7500\n",
      "Epoch [951/1000], Train Loss: 13304092.0000, Val Loss: 3488189.5000\n",
      "Epoch [952/1000], Train Loss: 13303393.0000, Val Loss: 3488492.5000\n",
      "Epoch [953/1000], Train Loss: 13304082.0000, Val Loss: 3489612.5000\n",
      "Epoch [954/1000], Train Loss: 13298095.0000, Val Loss: 3490203.0000\n",
      "Epoch [955/1000], Train Loss: 13305916.0000, Val Loss: 3489150.0000\n",
      "Epoch [956/1000], Train Loss: 13303525.0000, Val Loss: 3489206.5000\n",
      "Epoch [957/1000], Train Loss: 13297592.0000, Val Loss: 3489399.5000\n",
      "Epoch [958/1000], Train Loss: 13303298.0000, Val Loss: 3487677.5000\n",
      "Epoch [959/1000], Train Loss: 13301953.0000, Val Loss: 3486912.5000\n",
      "Epoch [960/1000], Train Loss: 13304628.0000, Val Loss: 3487731.5000\n",
      "Epoch [961/1000], Train Loss: 13311342.0000, Val Loss: 3486795.0000\n",
      "Epoch [962/1000], Train Loss: 13300291.0000, Val Loss: 3486765.7500\n",
      "Epoch [963/1000], Train Loss: 13306304.0000, Val Loss: 3489218.2500\n",
      "Epoch [964/1000], Train Loss: 13303645.0000, Val Loss: 3490312.7500\n",
      "Epoch [965/1000], Train Loss: 13301770.0000, Val Loss: 3489360.7500\n",
      "Epoch [966/1000], Train Loss: 13301956.0000, Val Loss: 3489013.0000\n",
      "Epoch [967/1000], Train Loss: 13302531.0000, Val Loss: 3489815.0000\n",
      "Epoch [968/1000], Train Loss: 13309912.0000, Val Loss: 3488656.0000\n",
      "Epoch [969/1000], Train Loss: 13305533.0000, Val Loss: 3487163.2500\n",
      "Epoch [970/1000], Train Loss: 13303105.0000, Val Loss: 3487503.5000\n",
      "Epoch [971/1000], Train Loss: 13297534.0000, Val Loss: 3488025.2500\n",
      "Epoch [972/1000], Train Loss: 13298815.0000, Val Loss: 3487322.2500\n",
      "Epoch [973/1000], Train Loss: 13300241.0000, Val Loss: 3486717.0000\n",
      "Epoch [974/1000], Train Loss: 13302316.0000, Val Loss: 3487748.7500\n",
      "Epoch [975/1000], Train Loss: 13300048.0000, Val Loss: 3489174.2500\n",
      "Epoch [976/1000], Train Loss: 13302068.0000, Val Loss: 3489375.7500\n",
      "Epoch [977/1000], Train Loss: 13297261.0000, Val Loss: 3489181.5000\n",
      "Epoch [978/1000], Train Loss: 13301443.0000, Val Loss: 3490141.2500\n",
      "Epoch [979/1000], Train Loss: 13300598.0000, Val Loss: 3490133.2500\n",
      "Epoch [980/1000], Train Loss: 13300722.0000, Val Loss: 3488601.5000\n",
      "Epoch [981/1000], Train Loss: 13297687.0000, Val Loss: 3488302.0000\n",
      "Epoch [982/1000], Train Loss: 13304088.0000, Val Loss: 3487893.7500\n",
      "Epoch [983/1000], Train Loss: 13299055.0000, Val Loss: 3487066.5000\n",
      "Epoch [984/1000], Train Loss: 13299042.0000, Val Loss: 3486473.5000\n",
      "Epoch [985/1000], Train Loss: 13299448.0000, Val Loss: 3486128.2500\n",
      "Epoch [986/1000], Train Loss: 13304045.0000, Val Loss: 3486686.5000\n",
      "Epoch [987/1000], Train Loss: 13298507.0000, Val Loss: 3487025.5000\n",
      "Epoch [988/1000], Train Loss: 13299086.0000, Val Loss: 3486531.0000\n",
      "Epoch [989/1000], Train Loss: 13297306.0000, Val Loss: 3487127.5000\n",
      "Epoch [990/1000], Train Loss: 13299166.0000, Val Loss: 3488343.7500\n",
      "Epoch [991/1000], Train Loss: 13306198.0000, Val Loss: 3487869.7500\n",
      "Epoch [992/1000], Train Loss: 13298757.0000, Val Loss: 3487894.7500\n",
      "Epoch [993/1000], Train Loss: 13300046.0000, Val Loss: 3488752.7500\n",
      "Epoch [994/1000], Train Loss: 13297489.0000, Val Loss: 3488304.7500\n",
      "Epoch [995/1000], Train Loss: 13297746.0000, Val Loss: 3487712.2500\n",
      "Epoch [996/1000], Train Loss: 13300518.0000, Val Loss: 3488084.5000\n",
      "Epoch [997/1000], Train Loss: 13297856.0000, Val Loss: 3487872.2500\n",
      "Epoch [998/1000], Train Loss: 13301129.0000, Val Loss: 3487132.0000\n",
      "Epoch [999/1000], Train Loss: 13298054.0000, Val Loss: 3486803.7500\n",
      "Epoch [1000/1000], Train Loss: 13296754.0000, Val Loss: 3486130.5000\n"
     ]
    }
   ],
   "source": [
    "# 初始化损失列表\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, Y_train)\n",
    "    # loss = custom_loss(output, Y_train)  # 如果使用自定义损失函数\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # 在验证集上评估模型\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, Y_val)\n",
    "        # val_loss = custom_loss(val_output, Y_val)  # 如果使用自定义损失函数\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T16:25:50.574830800Z",
     "start_time": "2024-07-17T11:37:30.907088500Z"
    }
   },
   "id": "fd386e50da0a6de1"
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Train Loss: 21892048.0000, Val Loss: 5463589.5000\n",
      "Epoch [2/1000], Train Loss: 21794032.0000, Val Loss: 5441181.5000\n",
      "Epoch [3/1000], Train Loss: 21698898.0000, Val Loss: 5418489.5000\n",
      "Epoch [4/1000], Train Loss: 21611694.0000, Val Loss: 5396117.0000\n",
      "Epoch [5/1000], Train Loss: 21528316.0000, Val Loss: 5374543.0000\n",
      "Epoch [6/1000], Train Loss: 21445600.0000, Val Loss: 5354088.0000\n",
      "Epoch [7/1000], Train Loss: 21368988.0000, Val Loss: 5336194.5000\n",
      "Epoch [8/1000], Train Loss: 21311250.0000, Val Loss: 5320557.0000\n",
      "Epoch [9/1000], Train Loss: 21248264.0000, Val Loss: 5306673.0000\n",
      "Epoch [10/1000], Train Loss: 21198022.0000, Val Loss: 5293772.5000\n",
      "Epoch [11/1000], Train Loss: 21143108.0000, Val Loss: 5281340.0000\n",
      "Epoch [12/1000], Train Loss: 21096304.0000, Val Loss: 5269110.5000\n",
      "Epoch [13/1000], Train Loss: 21048158.0000, Val Loss: 5256948.5000\n",
      "Epoch [14/1000], Train Loss: 20997406.0000, Val Loss: 5244780.0000\n",
      "Epoch [15/1000], Train Loss: 20947648.0000, Val Loss: 5232600.5000\n",
      "Epoch [16/1000], Train Loss: 20897808.0000, Val Loss: 5220385.5000\n",
      "Epoch [17/1000], Train Loss: 20848372.0000, Val Loss: 5208143.0000\n",
      "Epoch [18/1000], Train Loss: 20799722.0000, Val Loss: 5195877.5000\n",
      "Epoch [19/1000], Train Loss: 20749800.0000, Val Loss: 5183614.0000\n",
      "Epoch [20/1000], Train Loss: 20700478.0000, Val Loss: 5171366.5000\n",
      "Epoch [21/1000], Train Loss: 20652836.0000, Val Loss: 5159137.0000\n",
      "Epoch [22/1000], Train Loss: 20601688.0000, Val Loss: 5146945.0000\n",
      "Epoch [23/1000], Train Loss: 20555918.0000, Val Loss: 5134782.5000\n",
      "Epoch [24/1000], Train Loss: 20506020.0000, Val Loss: 5122662.5000\n",
      "Epoch [25/1000], Train Loss: 20454434.0000, Val Loss: 5110580.5000\n",
      "Epoch [26/1000], Train Loss: 20401752.0000, Val Loss: 5098540.5000\n",
      "Epoch [27/1000], Train Loss: 20359030.0000, Val Loss: 5086551.0000\n",
      "Epoch [28/1000], Train Loss: 20308692.0000, Val Loss: 5074618.0000\n",
      "Epoch [29/1000], Train Loss: 20263850.0000, Val Loss: 5062755.5000\n",
      "Epoch [30/1000], Train Loss: 20213652.0000, Val Loss: 5050960.0000\n",
      "Epoch [31/1000], Train Loss: 20162706.0000, Val Loss: 5039229.5000\n",
      "Epoch [32/1000], Train Loss: 20118656.0000, Val Loss: 5027560.5000\n",
      "Epoch [33/1000], Train Loss: 20063584.0000, Val Loss: 5015940.0000\n",
      "Epoch [34/1000], Train Loss: 20020798.0000, Val Loss: 5004379.5000\n",
      "Epoch [35/1000], Train Loss: 19973760.0000, Val Loss: 4992871.5000\n",
      "Epoch [36/1000], Train Loss: 19927736.0000, Val Loss: 4981424.5000\n",
      "Epoch [37/1000], Train Loss: 19879080.0000, Val Loss: 4970046.0000\n",
      "Epoch [38/1000], Train Loss: 19832634.0000, Val Loss: 4958755.0000\n",
      "Epoch [39/1000], Train Loss: 19785268.0000, Val Loss: 4947552.5000\n",
      "Epoch [40/1000], Train Loss: 19741252.0000, Val Loss: 4936444.5000\n",
      "Epoch [41/1000], Train Loss: 19694266.0000, Val Loss: 4925425.0000\n",
      "Epoch [42/1000], Train Loss: 19648910.0000, Val Loss: 4914495.0000\n",
      "Epoch [43/1000], Train Loss: 19606446.0000, Val Loss: 4903647.0000\n",
      "Epoch [44/1000], Train Loss: 19556578.0000, Val Loss: 4892872.5000\n",
      "Epoch [45/1000], Train Loss: 19510168.0000, Val Loss: 4882165.0000\n",
      "Epoch [46/1000], Train Loss: 19468842.0000, Val Loss: 4871525.5000\n",
      "Epoch [47/1000], Train Loss: 19422842.0000, Val Loss: 4860932.5000\n",
      "Epoch [48/1000], Train Loss: 19380372.0000, Val Loss: 4850401.5000\n",
      "Epoch [49/1000], Train Loss: 19334990.0000, Val Loss: 4839933.0000\n",
      "Epoch [50/1000], Train Loss: 19291860.0000, Val Loss: 4829544.5000\n",
      "Epoch [51/1000], Train Loss: 19247068.0000, Val Loss: 4819229.0000\n",
      "Epoch [52/1000], Train Loss: 19205952.0000, Val Loss: 4808989.0000\n",
      "Epoch [53/1000], Train Loss: 19162266.0000, Val Loss: 4798823.5000\n",
      "Epoch [54/1000], Train Loss: 19117744.0000, Val Loss: 4788740.5000\n",
      "Epoch [55/1000], Train Loss: 19076766.0000, Val Loss: 4778730.5000\n",
      "Epoch [56/1000], Train Loss: 19032524.0000, Val Loss: 4768768.5000\n",
      "Epoch [57/1000], Train Loss: 18989006.0000, Val Loss: 4758823.0000\n",
      "Epoch [58/1000], Train Loss: 18947964.0000, Val Loss: 4748910.0000\n",
      "Epoch [59/1000], Train Loss: 18905766.0000, Val Loss: 4739061.5000\n",
      "Epoch [60/1000], Train Loss: 18865784.0000, Val Loss: 4729314.5000\n",
      "Epoch [61/1000], Train Loss: 18821272.0000, Val Loss: 4719643.0000\n",
      "Epoch [62/1000], Train Loss: 18779804.0000, Val Loss: 4710039.0000\n",
      "Epoch [63/1000], Train Loss: 18739434.0000, Val Loss: 4700488.0000\n",
      "Epoch [64/1000], Train Loss: 18700204.0000, Val Loss: 4690943.5000\n",
      "Epoch [65/1000], Train Loss: 18659002.0000, Val Loss: 4681411.0000\n",
      "Epoch [66/1000], Train Loss: 18620920.0000, Val Loss: 4671966.5000\n",
      "Epoch [67/1000], Train Loss: 18580900.0000, Val Loss: 4662614.0000\n",
      "Epoch [68/1000], Train Loss: 18541724.0000, Val Loss: 4653365.5000\n",
      "Epoch [69/1000], Train Loss: 18501984.0000, Val Loss: 4644059.0000\n",
      "Epoch [70/1000], Train Loss: 18462758.0000, Val Loss: 4634764.0000\n",
      "Epoch [71/1000], Train Loss: 18425172.0000, Val Loss: 4625521.5000\n",
      "Epoch [72/1000], Train Loss: 18385432.0000, Val Loss: 4616391.0000\n",
      "Epoch [73/1000], Train Loss: 18347624.0000, Val Loss: 4607369.5000\n",
      "Epoch [74/1000], Train Loss: 18308030.0000, Val Loss: 4598338.0000\n",
      "Epoch [75/1000], Train Loss: 18269848.0000, Val Loss: 4589255.5000\n",
      "Epoch [76/1000], Train Loss: 18229384.0000, Val Loss: 4580234.5000\n",
      "Epoch [77/1000], Train Loss: 18193578.0000, Val Loss: 4571318.5000\n",
      "Epoch [78/1000], Train Loss: 18156152.0000, Val Loss: 4562501.5000\n",
      "Epoch [79/1000], Train Loss: 18118520.0000, Val Loss: 4553708.0000\n",
      "Epoch [80/1000], Train Loss: 18080274.0000, Val Loss: 4544869.0000\n",
      "Epoch [81/1000], Train Loss: 18045734.0000, Val Loss: 4536069.5000\n",
      "Epoch [82/1000], Train Loss: 18008606.0000, Val Loss: 4527358.0000\n",
      "Epoch [83/1000], Train Loss: 17970284.0000, Val Loss: 4518724.0000\n",
      "Epoch [84/1000], Train Loss: 17932914.0000, Val Loss: 4510165.5000\n",
      "Epoch [85/1000], Train Loss: 17897004.0000, Val Loss: 4501588.0000\n",
      "Epoch [86/1000], Train Loss: 17861440.0000, Val Loss: 4492987.0000\n",
      "Epoch [87/1000], Train Loss: 17824486.0000, Val Loss: 4484449.0000\n",
      "Epoch [88/1000], Train Loss: 17788502.0000, Val Loss: 4475990.0000\n",
      "Epoch [89/1000], Train Loss: 17753970.0000, Val Loss: 4467637.0000\n",
      "Epoch [90/1000], Train Loss: 17716694.0000, Val Loss: 4459316.5000\n",
      "Epoch [91/1000], Train Loss: 17682644.0000, Val Loss: 4450910.0000\n",
      "Epoch [92/1000], Train Loss: 17645856.0000, Val Loss: 4442565.5000\n",
      "Epoch [93/1000], Train Loss: 17610636.0000, Val Loss: 4434315.0000\n",
      "Epoch [94/1000], Train Loss: 17575922.0000, Val Loss: 4426162.0000\n",
      "Epoch [95/1000], Train Loss: 17541568.0000, Val Loss: 4417978.5000\n",
      "Epoch [96/1000], Train Loss: 17505428.0000, Val Loss: 4409811.5000\n",
      "Epoch [97/1000], Train Loss: 17472008.0000, Val Loss: 4401711.5000\n",
      "Epoch [98/1000], Train Loss: 17437992.0000, Val Loss: 4393649.0000\n",
      "Epoch [99/1000], Train Loss: 17404092.0000, Val Loss: 4385639.5000\n",
      "Epoch [100/1000], Train Loss: 17369658.0000, Val Loss: 4377637.0000\n",
      "Epoch [101/1000], Train Loss: 17334752.0000, Val Loss: 4369700.5000\n",
      "Epoch [102/1000], Train Loss: 17300944.0000, Val Loss: 4361808.5000\n",
      "Epoch [103/1000], Train Loss: 17267572.0000, Val Loss: 4353941.5000\n",
      "Epoch [104/1000], Train Loss: 17234686.0000, Val Loss: 4346131.5000\n",
      "Epoch [105/1000], Train Loss: 17201282.0000, Val Loss: 4338375.5000\n",
      "Epoch [106/1000], Train Loss: 17169572.0000, Val Loss: 4330626.5000\n",
      "Epoch [107/1000], Train Loss: 17136032.0000, Val Loss: 4322901.5000\n",
      "Epoch [108/1000], Train Loss: 17102760.0000, Val Loss: 4315255.5000\n",
      "Epoch [109/1000], Train Loss: 17069552.0000, Val Loss: 4307637.5000\n",
      "Epoch [110/1000], Train Loss: 17038882.0000, Val Loss: 4300062.0000\n",
      "Epoch [111/1000], Train Loss: 17005542.0000, Val Loss: 4292533.0000\n",
      "Epoch [112/1000], Train Loss: 16973760.0000, Val Loss: 4285050.5000\n",
      "Epoch [113/1000], Train Loss: 16941316.0000, Val Loss: 4277600.5000\n",
      "Epoch [114/1000], Train Loss: 16909972.0000, Val Loss: 4270203.5000\n",
      "Epoch [115/1000], Train Loss: 16878522.0000, Val Loss: 4262826.5000\n",
      "Epoch [116/1000], Train Loss: 16846156.0000, Val Loss: 4255477.5000\n",
      "Epoch [117/1000], Train Loss: 16814994.0000, Val Loss: 4248240.5000\n",
      "Epoch [118/1000], Train Loss: 16784082.0000, Val Loss: 4241047.5000\n",
      "Epoch [119/1000], Train Loss: 16753454.0000, Val Loss: 4233803.5000\n",
      "Epoch [120/1000], Train Loss: 16722601.0000, Val Loss: 4226597.0000\n",
      "Epoch [121/1000], Train Loss: 16692536.0000, Val Loss: 4219522.5000\n",
      "Epoch [122/1000], Train Loss: 16661532.0000, Val Loss: 4212549.5000\n",
      "Epoch [123/1000], Train Loss: 16631051.0000, Val Loss: 4205478.0000\n",
      "Epoch [124/1000], Train Loss: 16601352.0000, Val Loss: 4198442.5000\n",
      "Epoch [125/1000], Train Loss: 16572056.0000, Val Loss: 4191509.0000\n",
      "Epoch [126/1000], Train Loss: 16541932.0000, Val Loss: 4184709.7500\n",
      "Epoch [127/1000], Train Loss: 16512311.0000, Val Loss: 4177896.2500\n",
      "Epoch [128/1000], Train Loss: 16483289.0000, Val Loss: 4171021.5000\n",
      "Epoch [129/1000], Train Loss: 16453750.0000, Val Loss: 4164237.7500\n",
      "Epoch [130/1000], Train Loss: 16425311.0000, Val Loss: 4157560.7500\n",
      "Epoch [131/1000], Train Loss: 16396013.0000, Val Loss: 4150957.7500\n",
      "Epoch [132/1000], Train Loss: 16367513.0000, Val Loss: 4144306.2500\n",
      "Epoch [133/1000], Train Loss: 16339239.0000, Val Loss: 4137690.5000\n",
      "Epoch [134/1000], Train Loss: 16310761.0000, Val Loss: 4131169.0000\n",
      "Epoch [135/1000], Train Loss: 16281536.0000, Val Loss: 4124726.2500\n",
      "Epoch [136/1000], Train Loss: 16254492.0000, Val Loss: 4118329.2500\n",
      "Epoch [137/1000], Train Loss: 16227099.0000, Val Loss: 4111897.7500\n",
      "Epoch [138/1000], Train Loss: 16199595.0000, Val Loss: 4105515.7500\n",
      "Epoch [139/1000], Train Loss: 16172264.0000, Val Loss: 4099238.0000\n",
      "Epoch [140/1000], Train Loss: 16145054.0000, Val Loss: 4093052.2500\n",
      "Epoch [141/1000], Train Loss: 16118173.0000, Val Loss: 4086817.7500\n",
      "Epoch [142/1000], Train Loss: 16091475.0000, Val Loss: 4080601.5000\n",
      "Epoch [143/1000], Train Loss: 16064222.0000, Val Loss: 4074509.7500\n",
      "Epoch [144/1000], Train Loss: 16039012.0000, Val Loss: 4068491.2500\n",
      "Epoch [145/1000], Train Loss: 16012858.0000, Val Loss: 4062495.0000\n",
      "Epoch [146/1000], Train Loss: 15986256.0000, Val Loss: 4056452.7500\n",
      "Epoch [147/1000], Train Loss: 15961105.0000, Val Loss: 4050502.7500\n",
      "Epoch [148/1000], Train Loss: 15935029.0000, Val Loss: 4044645.5000\n",
      "Epoch [149/1000], Train Loss: 15910079.0000, Val Loss: 4038874.2500\n",
      "Epoch [150/1000], Train Loss: 15883672.0000, Val Loss: 4033067.5000\n",
      "Epoch [151/1000], Train Loss: 15859806.0000, Val Loss: 4027274.5000\n",
      "Epoch [152/1000], Train Loss: 15833467.0000, Val Loss: 4021557.5000\n",
      "Epoch [153/1000], Train Loss: 15809333.0000, Val Loss: 4015971.2500\n",
      "Epoch [154/1000], Train Loss: 15785747.0000, Val Loss: 4010412.5000\n",
      "Epoch [155/1000], Train Loss: 15760475.0000, Val Loss: 4004793.5000\n",
      "Epoch [156/1000], Train Loss: 15737097.0000, Val Loss: 3999226.2500\n",
      "Epoch [157/1000], Train Loss: 15712788.0000, Val Loss: 3993794.5000\n",
      "Epoch [158/1000], Train Loss: 15688910.0000, Val Loss: 3988439.2500\n",
      "Epoch [159/1000], Train Loss: 15665634.0000, Val Loss: 3983038.0000\n",
      "Epoch [160/1000], Train Loss: 15642066.0000, Val Loss: 3977654.7500\n",
      "Epoch [161/1000], Train Loss: 15619926.0000, Val Loss: 3972365.7500\n",
      "Epoch [162/1000], Train Loss: 15596505.0000, Val Loss: 3967199.7500\n",
      "Epoch [163/1000], Train Loss: 15573239.0000, Val Loss: 3962010.7500\n",
      "Epoch [164/1000], Train Loss: 15551013.0000, Val Loss: 3956810.2500\n",
      "Epoch [165/1000], Train Loss: 15528741.0000, Val Loss: 3951700.5000\n",
      "Epoch [166/1000], Train Loss: 15506476.0000, Val Loss: 3946676.7500\n",
      "Epoch [167/1000], Train Loss: 15484904.0000, Val Loss: 3941720.2500\n",
      "Epoch [168/1000], Train Loss: 15462886.0000, Val Loss: 3936720.5000\n",
      "Epoch [169/1000], Train Loss: 15441460.0000, Val Loss: 3931744.2500\n",
      "Epoch [170/1000], Train Loss: 15419584.0000, Val Loss: 3926872.2500\n",
      "Epoch [171/1000], Train Loss: 15398534.0000, Val Loss: 3922079.7500\n",
      "Epoch [172/1000], Train Loss: 15377458.0000, Val Loss: 3917309.5000\n",
      "Epoch [173/1000], Train Loss: 15357058.0000, Val Loss: 3912529.0000\n",
      "Epoch [174/1000], Train Loss: 15336477.0000, Val Loss: 3907800.5000\n",
      "Epoch [175/1000], Train Loss: 15315345.0000, Val Loss: 3903175.0000\n",
      "Epoch [176/1000], Train Loss: 15293886.0000, Val Loss: 3898598.7500\n",
      "Epoch [177/1000], Train Loss: 15274824.0000, Val Loss: 3894005.2500\n",
      "Epoch [178/1000], Train Loss: 15255386.0000, Val Loss: 3889453.5000\n",
      "Epoch [179/1000], Train Loss: 15234986.0000, Val Loss: 3884952.0000\n",
      "Epoch [180/1000], Train Loss: 15216196.0000, Val Loss: 3880556.5000\n",
      "Epoch [181/1000], Train Loss: 15196470.0000, Val Loss: 3876198.5000\n",
      "Epoch [182/1000], Train Loss: 15177202.0000, Val Loss: 3871824.7500\n",
      "Epoch [183/1000], Train Loss: 15158056.0000, Val Loss: 3867462.7500\n",
      "Epoch [184/1000], Train Loss: 15139274.0000, Val Loss: 3863197.7500\n",
      "Epoch [185/1000], Train Loss: 15119921.0000, Val Loss: 3859017.2500\n",
      "Epoch [186/1000], Train Loss: 15101463.0000, Val Loss: 3854855.0000\n",
      "Epoch [187/1000], Train Loss: 15083100.0000, Val Loss: 3850655.0000\n",
      "Epoch [188/1000], Train Loss: 15064905.0000, Val Loss: 3846539.2500\n",
      "Epoch [189/1000], Train Loss: 15046540.0000, Val Loss: 3842514.0000\n",
      "Epoch [190/1000], Train Loss: 15029097.0000, Val Loss: 3838533.0000\n",
      "Epoch [191/1000], Train Loss: 15012363.0000, Val Loss: 3834538.7500\n",
      "Epoch [192/1000], Train Loss: 14994480.0000, Val Loss: 3830557.5000\n",
      "Epoch [193/1000], Train Loss: 14976861.0000, Val Loss: 3826668.0000\n",
      "Epoch [194/1000], Train Loss: 14959359.0000, Val Loss: 3822865.2500\n",
      "Epoch [195/1000], Train Loss: 14941994.0000, Val Loss: 3819047.2500\n",
      "Epoch [196/1000], Train Loss: 14925658.0000, Val Loss: 3815238.2500\n",
      "Epoch [197/1000], Train Loss: 14909637.0000, Val Loss: 3811478.7500\n",
      "Epoch [198/1000], Train Loss: 14892702.0000, Val Loss: 3807794.5000\n",
      "Epoch [199/1000], Train Loss: 14876032.0000, Val Loss: 3804165.0000\n",
      "Epoch [200/1000], Train Loss: 14860039.0000, Val Loss: 3800532.2500\n",
      "Epoch [201/1000], Train Loss: 14843542.0000, Val Loss: 3796936.2500\n",
      "Epoch [202/1000], Train Loss: 14827671.0000, Val Loss: 3793397.0000\n",
      "Epoch [203/1000], Train Loss: 14812318.0000, Val Loss: 3789897.7500\n",
      "Epoch [204/1000], Train Loss: 14796210.0000, Val Loss: 3786443.7500\n",
      "Epoch [205/1000], Train Loss: 14781807.0000, Val Loss: 3782989.5000\n",
      "Epoch [206/1000], Train Loss: 14765890.0000, Val Loss: 3779600.7500\n",
      "Epoch [207/1000], Train Loss: 14751298.0000, Val Loss: 3776250.0000\n",
      "Epoch [208/1000], Train Loss: 14736641.0000, Val Loss: 3772935.7500\n",
      "Epoch [209/1000], Train Loss: 14721832.0000, Val Loss: 3769657.2500\n",
      "Epoch [210/1000], Train Loss: 14707061.0000, Val Loss: 3766375.2500\n",
      "Epoch [211/1000], Train Loss: 14692138.0000, Val Loss: 3763154.2500\n",
      "Epoch [212/1000], Train Loss: 14677698.0000, Val Loss: 3759999.0000\n",
      "Epoch [213/1000], Train Loss: 14664017.0000, Val Loss: 3756879.7500\n",
      "Epoch [214/1000], Train Loss: 14649841.0000, Val Loss: 3753762.2500\n",
      "Epoch [215/1000], Train Loss: 14635860.0000, Val Loss: 3750657.0000\n",
      "Epoch [216/1000], Train Loss: 14623061.0000, Val Loss: 3747607.0000\n",
      "Epoch [217/1000], Train Loss: 14609216.0000, Val Loss: 3744615.0000\n",
      "Epoch [218/1000], Train Loss: 14595563.0000, Val Loss: 3741679.2500\n",
      "Epoch [219/1000], Train Loss: 14581619.0000, Val Loss: 3738716.7500\n",
      "Epoch [220/1000], Train Loss: 14569194.0000, Val Loss: 3735779.5000\n",
      "Epoch [221/1000], Train Loss: 14556348.0000, Val Loss: 3732922.2500\n",
      "Epoch [222/1000], Train Loss: 14543257.0000, Val Loss: 3730122.2500\n",
      "Epoch [223/1000], Train Loss: 14531008.0000, Val Loss: 3727321.2500\n",
      "Epoch [224/1000], Train Loss: 14518276.0000, Val Loss: 3724505.7500\n",
      "Epoch [225/1000], Train Loss: 14505354.0000, Val Loss: 3721747.2500\n",
      "Epoch [226/1000], Train Loss: 14493002.0000, Val Loss: 3719082.2500\n",
      "Epoch [227/1000], Train Loss: 14481097.0000, Val Loss: 3716434.2500\n",
      "Epoch [228/1000], Train Loss: 14468905.0000, Val Loss: 3713765.5000\n",
      "Epoch [229/1000], Train Loss: 14457428.0000, Val Loss: 3711088.2500\n",
      "Epoch [230/1000], Train Loss: 14445083.0000, Val Loss: 3708506.5000\n",
      "Epoch [231/1000], Train Loss: 14434197.0000, Val Loss: 3706025.5000\n",
      "Epoch [232/1000], Train Loss: 14422233.0000, Val Loss: 3703514.2500\n",
      "Epoch [233/1000], Train Loss: 14410986.0000, Val Loss: 3700947.2500\n",
      "Epoch [234/1000], Train Loss: 14399916.0000, Val Loss: 3698447.0000\n",
      "Epoch [235/1000], Train Loss: 14388965.0000, Val Loss: 3696063.2500\n",
      "Epoch [236/1000], Train Loss: 14377772.0000, Val Loss: 3693697.0000\n",
      "Epoch [237/1000], Train Loss: 14366751.0000, Val Loss: 3691262.2500\n",
      "Epoch [238/1000], Train Loss: 14355820.0000, Val Loss: 3688867.5000\n",
      "Epoch [239/1000], Train Loss: 14345347.0000, Val Loss: 3686569.7500\n",
      "Epoch [240/1000], Train Loss: 14334501.0000, Val Loss: 3684305.0000\n",
      "Epoch [241/1000], Train Loss: 14324151.0000, Val Loss: 3682017.2500\n",
      "Epoch [242/1000], Train Loss: 14314648.0000, Val Loss: 3679733.0000\n",
      "Epoch [243/1000], Train Loss: 14304380.0000, Val Loss: 3677526.7500\n",
      "Epoch [244/1000], Train Loss: 14293776.0000, Val Loss: 3675361.7500\n",
      "Epoch [245/1000], Train Loss: 14283516.0000, Val Loss: 3673195.0000\n",
      "Epoch [246/1000], Train Loss: 14274802.0000, Val Loss: 3671021.0000\n",
      "Epoch [247/1000], Train Loss: 14264594.0000, Val Loss: 3668900.7500\n",
      "Epoch [248/1000], Train Loss: 14255060.0000, Val Loss: 3666850.2500\n",
      "Epoch [249/1000], Train Loss: 14245434.0000, Val Loss: 3664801.0000\n",
      "Epoch [250/1000], Train Loss: 14236424.0000, Val Loss: 3662712.7500\n",
      "Epoch [251/1000], Train Loss: 14227232.0000, Val Loss: 3660673.7500\n",
      "Epoch [252/1000], Train Loss: 14217945.0000, Val Loss: 3658725.0000\n",
      "Epoch [253/1000], Train Loss: 14208799.0000, Val Loss: 3656783.0000\n",
      "Epoch [254/1000], Train Loss: 14200158.0000, Val Loss: 3654825.2500\n",
      "Epoch [255/1000], Train Loss: 14191030.0000, Val Loss: 3652893.2500\n",
      "Epoch [256/1000], Train Loss: 14182446.0000, Val Loss: 3651026.0000\n",
      "Epoch [257/1000], Train Loss: 14173136.0000, Val Loss: 3649174.0000\n",
      "Epoch [258/1000], Train Loss: 14164474.0000, Val Loss: 3647300.7500\n",
      "Epoch [259/1000], Train Loss: 14156453.0000, Val Loss: 3645465.5000\n",
      "Epoch [260/1000], Train Loss: 14148552.0000, Val Loss: 3643690.7500\n",
      "Epoch [261/1000], Train Loss: 14140031.0000, Val Loss: 3641921.7500\n",
      "Epoch [262/1000], Train Loss: 14132462.0000, Val Loss: 3640137.7500\n",
      "Epoch [263/1000], Train Loss: 14123844.0000, Val Loss: 3638398.2500\n",
      "Epoch [264/1000], Train Loss: 14115746.0000, Val Loss: 3636706.2500\n",
      "Epoch [265/1000], Train Loss: 14107892.0000, Val Loss: 3635054.5000\n",
      "Epoch [266/1000], Train Loss: 14100391.0000, Val Loss: 3633348.2500\n",
      "Epoch [267/1000], Train Loss: 14093088.0000, Val Loss: 3631665.5000\n",
      "Epoch [268/1000], Train Loss: 14085480.0000, Val Loss: 3630024.0000\n",
      "Epoch [269/1000], Train Loss: 14077562.0000, Val Loss: 3628455.7500\n",
      "Epoch [270/1000], Train Loss: 14070796.0000, Val Loss: 3626883.5000\n",
      "Epoch [271/1000], Train Loss: 14062540.0000, Val Loss: 3625285.7500\n",
      "Epoch [272/1000], Train Loss: 14056026.0000, Val Loss: 3623667.5000\n",
      "Epoch [273/1000], Train Loss: 14048315.0000, Val Loss: 3622153.7500\n",
      "Epoch [274/1000], Train Loss: 14041820.0000, Val Loss: 3620708.5000\n",
      "Epoch [275/1000], Train Loss: 14034493.0000, Val Loss: 3619236.7500\n",
      "Epoch [276/1000], Train Loss: 14028306.0000, Val Loss: 3617705.2500\n",
      "Epoch [277/1000], Train Loss: 14020900.0000, Val Loss: 3616218.2500\n",
      "Epoch [278/1000], Train Loss: 14014242.0000, Val Loss: 3614800.7500\n",
      "Epoch [279/1000], Train Loss: 14007862.0000, Val Loss: 3613450.2500\n",
      "Epoch [280/1000], Train Loss: 14001388.0000, Val Loss: 3612017.2500\n",
      "Epoch [281/1000], Train Loss: 13994317.0000, Val Loss: 3610588.7500\n",
      "Epoch [282/1000], Train Loss: 13988083.0000, Val Loss: 3609214.7500\n",
      "Epoch [283/1000], Train Loss: 13981847.0000, Val Loss: 3607911.2500\n",
      "Epoch [284/1000], Train Loss: 13975466.0000, Val Loss: 3606638.5000\n",
      "Epoch [285/1000], Train Loss: 13968723.0000, Val Loss: 3605310.0000\n",
      "Epoch [286/1000], Train Loss: 13963053.0000, Val Loss: 3603946.7500\n",
      "Epoch [287/1000], Train Loss: 13957065.0000, Val Loss: 3602645.7500\n",
      "Epoch [288/1000], Train Loss: 13951237.0000, Val Loss: 3601422.7500\n",
      "Epoch [289/1000], Train Loss: 13945242.0000, Val Loss: 3600205.5000\n",
      "Epoch [290/1000], Train Loss: 13939876.0000, Val Loss: 3598970.5000\n",
      "Epoch [291/1000], Train Loss: 13933908.0000, Val Loss: 3597716.7500\n",
      "Epoch [292/1000], Train Loss: 13927800.0000, Val Loss: 3596488.0000\n",
      "Epoch [293/1000], Train Loss: 13923034.0000, Val Loss: 3595320.0000\n",
      "Epoch [294/1000], Train Loss: 13916875.0000, Val Loss: 3594166.5000\n",
      "Epoch [295/1000], Train Loss: 13911194.0000, Val Loss: 3593030.5000\n",
      "Epoch [296/1000], Train Loss: 13906461.0000, Val Loss: 3591883.2500\n",
      "Epoch [297/1000], Train Loss: 13900771.0000, Val Loss: 3590730.5000\n",
      "Epoch [298/1000], Train Loss: 13895446.0000, Val Loss: 3589630.2500\n",
      "Epoch [299/1000], Train Loss: 13890432.0000, Val Loss: 3588534.0000\n",
      "Epoch [300/1000], Train Loss: 13885140.0000, Val Loss: 3587461.7500\n",
      "Epoch [301/1000], Train Loss: 13880253.0000, Val Loss: 3586371.0000\n",
      "Epoch [302/1000], Train Loss: 13875405.0000, Val Loss: 3585305.0000\n",
      "Epoch [303/1000], Train Loss: 13870151.0000, Val Loss: 3584286.7500\n",
      "Epoch [304/1000], Train Loss: 13865233.0000, Val Loss: 3583271.7500\n",
      "Epoch [305/1000], Train Loss: 13860315.0000, Val Loss: 3582234.7500\n",
      "Epoch [306/1000], Train Loss: 13855867.0000, Val Loss: 3581219.5000\n",
      "Epoch [307/1000], Train Loss: 13850977.0000, Val Loss: 3580254.0000\n",
      "Epoch [308/1000], Train Loss: 13846928.0000, Val Loss: 3579287.7500\n",
      "Epoch [309/1000], Train Loss: 13841956.0000, Val Loss: 3578321.5000\n",
      "Epoch [310/1000], Train Loss: 13837290.0000, Val Loss: 3577351.0000\n",
      "Epoch [311/1000], Train Loss: 13832117.0000, Val Loss: 3576399.0000\n",
      "Epoch [312/1000], Train Loss: 13828404.0000, Val Loss: 3575496.2500\n",
      "Epoch [313/1000], Train Loss: 13823484.0000, Val Loss: 3574602.2500\n",
      "Epoch [314/1000], Train Loss: 13819104.0000, Val Loss: 3573706.2500\n",
      "Epoch [315/1000], Train Loss: 13814889.0000, Val Loss: 3572780.0000\n",
      "Epoch [316/1000], Train Loss: 13810799.0000, Val Loss: 3571871.5000\n",
      "Epoch [317/1000], Train Loss: 13806862.0000, Val Loss: 3571020.5000\n",
      "Epoch [318/1000], Train Loss: 13802436.0000, Val Loss: 3570176.5000\n",
      "Epoch [319/1000], Train Loss: 13798549.0000, Val Loss: 3569344.7500\n",
      "Epoch [320/1000], Train Loss: 13794234.0000, Val Loss: 3568509.0000\n",
      "Epoch [321/1000], Train Loss: 13790368.0000, Val Loss: 3567691.5000\n",
      "Epoch [322/1000], Train Loss: 13786329.0000, Val Loss: 3566876.2500\n",
      "Epoch [323/1000], Train Loss: 13782914.0000, Val Loss: 3566065.2500\n",
      "Epoch [324/1000], Train Loss: 13778632.0000, Val Loss: 3565248.7500\n",
      "Epoch [325/1000], Train Loss: 13775284.0000, Val Loss: 3564464.5000\n",
      "Epoch [326/1000], Train Loss: 13771405.0000, Val Loss: 3563732.2500\n",
      "Epoch [327/1000], Train Loss: 13767929.0000, Val Loss: 3562985.5000\n",
      "Epoch [328/1000], Train Loss: 13763913.0000, Val Loss: 3562197.0000\n",
      "Epoch [329/1000], Train Loss: 13760586.0000, Val Loss: 3561437.7500\n",
      "Epoch [330/1000], Train Loss: 13756900.0000, Val Loss: 3560708.0000\n",
      "Epoch [331/1000], Train Loss: 13753670.0000, Val Loss: 3559996.2500\n",
      "Epoch [332/1000], Train Loss: 13749954.0000, Val Loss: 3559301.7500\n",
      "Epoch [333/1000], Train Loss: 13746966.0000, Val Loss: 3558590.7500\n",
      "Epoch [334/1000], Train Loss: 13743338.0000, Val Loss: 3557872.2500\n",
      "Epoch [335/1000], Train Loss: 13740234.0000, Val Loss: 3557183.2500\n",
      "Epoch [336/1000], Train Loss: 13735919.0000, Val Loss: 3556519.5000\n",
      "Epoch [337/1000], Train Loss: 13733400.0000, Val Loss: 3555881.5000\n",
      "Epoch [338/1000], Train Loss: 13730140.0000, Val Loss: 3555193.5000\n",
      "Epoch [339/1000], Train Loss: 13726538.0000, Val Loss: 3554536.2500\n",
      "Epoch [340/1000], Train Loss: 13723512.0000, Val Loss: 3553886.2500\n",
      "Epoch [341/1000], Train Loss: 13720491.0000, Val Loss: 3553261.2500\n",
      "Epoch [342/1000], Train Loss: 13717318.0000, Val Loss: 3552644.7500\n",
      "Epoch [343/1000], Train Loss: 13714972.0000, Val Loss: 3552030.7500\n",
      "Epoch [344/1000], Train Loss: 13711532.0000, Val Loss: 3551419.7500\n",
      "Epoch [345/1000], Train Loss: 13708304.0000, Val Loss: 3550829.0000\n",
      "Epoch [346/1000], Train Loss: 13705820.0000, Val Loss: 3550239.2500\n",
      "Epoch [347/1000], Train Loss: 13702715.0000, Val Loss: 3549622.7500\n",
      "Epoch [348/1000], Train Loss: 13700340.0000, Val Loss: 3549041.7500\n",
      "Epoch [349/1000], Train Loss: 13697094.0000, Val Loss: 3548495.0000\n",
      "Epoch [350/1000], Train Loss: 13694772.0000, Val Loss: 3547948.0000\n",
      "Epoch [351/1000], Train Loss: 13691694.0000, Val Loss: 3547385.5000\n",
      "Epoch [352/1000], Train Loss: 13689067.0000, Val Loss: 3546826.2500\n",
      "Epoch [353/1000], Train Loss: 13686095.0000, Val Loss: 3546279.7500\n",
      "Epoch [354/1000], Train Loss: 13683494.0000, Val Loss: 3545766.7500\n",
      "Epoch [355/1000], Train Loss: 13680881.0000, Val Loss: 3545249.2500\n",
      "Epoch [356/1000], Train Loss: 13678151.0000, Val Loss: 3544730.7500\n",
      "Epoch [357/1000], Train Loss: 13675565.0000, Val Loss: 3544198.0000\n",
      "Epoch [358/1000], Train Loss: 13673153.0000, Val Loss: 3543675.7500\n",
      "Epoch [359/1000], Train Loss: 13670763.0000, Val Loss: 3543195.7500\n",
      "Epoch [360/1000], Train Loss: 13668538.0000, Val Loss: 3542711.7500\n",
      "Epoch [361/1000], Train Loss: 13666422.0000, Val Loss: 3542227.7500\n",
      "Epoch [362/1000], Train Loss: 13663505.0000, Val Loss: 3541745.0000\n",
      "Epoch [363/1000], Train Loss: 13661524.0000, Val Loss: 3541241.2500\n",
      "Epoch [364/1000], Train Loss: 13659262.0000, Val Loss: 3540753.7500\n",
      "Epoch [365/1000], Train Loss: 13656792.0000, Val Loss: 3540296.7500\n",
      "Epoch [366/1000], Train Loss: 13654319.0000, Val Loss: 3539888.5000\n",
      "Epoch [367/1000], Train Loss: 13652017.0000, Val Loss: 3539439.0000\n",
      "Epoch [368/1000], Train Loss: 13650143.0000, Val Loss: 3538970.0000\n",
      "Epoch [369/1000], Train Loss: 13648046.0000, Val Loss: 3538495.7500\n",
      "Epoch [370/1000], Train Loss: 13645134.0000, Val Loss: 3538063.2500\n",
      "Epoch [371/1000], Train Loss: 13643649.0000, Val Loss: 3537666.5000\n",
      "Epoch [372/1000], Train Loss: 13641339.0000, Val Loss: 3537267.7500\n",
      "Epoch [373/1000], Train Loss: 13639424.0000, Val Loss: 3536852.7500\n",
      "Epoch [374/1000], Train Loss: 13637190.0000, Val Loss: 3536439.7500\n",
      "Epoch [375/1000], Train Loss: 13634721.0000, Val Loss: 3536012.0000\n",
      "Epoch [376/1000], Train Loss: 13633379.0000, Val Loss: 3535630.2500\n",
      "Epoch [377/1000], Train Loss: 13631566.0000, Val Loss: 3535240.7500\n",
      "Epoch [378/1000], Train Loss: 13629012.0000, Val Loss: 3534861.7500\n",
      "Epoch [379/1000], Train Loss: 13626666.0000, Val Loss: 3534481.7500\n",
      "Epoch [380/1000], Train Loss: 13625142.0000, Val Loss: 3534076.7500\n",
      "Epoch [381/1000], Train Loss: 13623400.0000, Val Loss: 3533701.0000\n",
      "Epoch [382/1000], Train Loss: 13621573.0000, Val Loss: 3533326.2500\n",
      "Epoch [383/1000], Train Loss: 13619519.0000, Val Loss: 3532990.2500\n",
      "Epoch [384/1000], Train Loss: 13617708.0000, Val Loss: 3532648.5000\n",
      "Epoch [385/1000], Train Loss: 13615868.0000, Val Loss: 3532297.5000\n",
      "Epoch [386/1000], Train Loss: 13614347.0000, Val Loss: 3531961.0000\n",
      "Epoch [387/1000], Train Loss: 13612703.0000, Val Loss: 3531615.0000\n",
      "Epoch [388/1000], Train Loss: 13610202.0000, Val Loss: 3531237.7500\n",
      "Epoch [389/1000], Train Loss: 13609141.0000, Val Loss: 3530899.7500\n",
      "Epoch [390/1000], Train Loss: 13607468.0000, Val Loss: 3530584.0000\n",
      "Epoch [391/1000], Train Loss: 13605898.0000, Val Loss: 3530265.0000\n",
      "Epoch [392/1000], Train Loss: 13603940.0000, Val Loss: 3529958.2500\n",
      "Epoch [393/1000], Train Loss: 13601754.0000, Val Loss: 3529621.0000\n",
      "Epoch [394/1000], Train Loss: 13600695.0000, Val Loss: 3529292.5000\n",
      "Epoch [395/1000], Train Loss: 13599058.0000, Val Loss: 3528967.7500\n",
      "Epoch [396/1000], Train Loss: 13597462.0000, Val Loss: 3528683.2500\n",
      "Epoch [397/1000], Train Loss: 13595908.0000, Val Loss: 3528387.5000\n",
      "Epoch [398/1000], Train Loss: 13594354.0000, Val Loss: 3528098.0000\n",
      "Epoch [399/1000], Train Loss: 13592502.0000, Val Loss: 3527804.2500\n",
      "Epoch [400/1000], Train Loss: 13591281.0000, Val Loss: 3527509.2500\n",
      "Epoch [401/1000], Train Loss: 13589966.0000, Val Loss: 3527209.2500\n",
      "Epoch [402/1000], Train Loss: 13588306.0000, Val Loss: 3526922.7500\n",
      "Epoch [403/1000], Train Loss: 13587285.0000, Val Loss: 3526652.2500\n",
      "Epoch [404/1000], Train Loss: 13585561.0000, Val Loss: 3526378.0000\n",
      "Epoch [405/1000], Train Loss: 13584312.0000, Val Loss: 3526103.7500\n",
      "Epoch [406/1000], Train Loss: 13582641.0000, Val Loss: 3525804.0000\n",
      "Epoch [407/1000], Train Loss: 13581202.0000, Val Loss: 3525539.2500\n",
      "Epoch [408/1000], Train Loss: 13579992.0000, Val Loss: 3525302.0000\n",
      "Epoch [409/1000], Train Loss: 13578131.0000, Val Loss: 3525053.0000\n",
      "Epoch [410/1000], Train Loss: 13576827.0000, Val Loss: 3524781.0000\n",
      "Epoch [411/1000], Train Loss: 13575660.0000, Val Loss: 3524518.0000\n",
      "Epoch [412/1000], Train Loss: 13574400.0000, Val Loss: 3524257.0000\n",
      "Epoch [413/1000], Train Loss: 13573145.0000, Val Loss: 3524021.2500\n",
      "Epoch [414/1000], Train Loss: 13572000.0000, Val Loss: 3523793.0000\n",
      "Epoch [415/1000], Train Loss: 13570199.0000, Val Loss: 3523541.7500\n",
      "Epoch [416/1000], Train Loss: 13569528.0000, Val Loss: 3523295.7500\n",
      "Epoch [417/1000], Train Loss: 13568602.0000, Val Loss: 3523062.2500\n",
      "Epoch [418/1000], Train Loss: 13567245.0000, Val Loss: 3522865.0000\n",
      "Epoch [419/1000], Train Loss: 13565800.0000, Val Loss: 3522662.7500\n",
      "Epoch [420/1000], Train Loss: 13564586.0000, Val Loss: 3522379.5000\n",
      "Epoch [421/1000], Train Loss: 13563997.0000, Val Loss: 3522140.7500\n",
      "Epoch [422/1000], Train Loss: 13562352.0000, Val Loss: 3521964.7500\n",
      "Epoch [423/1000], Train Loss: 13561238.0000, Val Loss: 3521762.2500\n",
      "Epoch [424/1000], Train Loss: 13560090.0000, Val Loss: 3521543.0000\n",
      "Epoch [425/1000], Train Loss: 13559143.0000, Val Loss: 3521312.0000\n",
      "Epoch [426/1000], Train Loss: 13557714.0000, Val Loss: 3521108.7500\n",
      "Epoch [427/1000], Train Loss: 13556442.0000, Val Loss: 3520900.7500\n",
      "Epoch [428/1000], Train Loss: 13555723.0000, Val Loss: 3520691.0000\n",
      "Epoch [429/1000], Train Loss: 13554674.0000, Val Loss: 3520495.7500\n",
      "Epoch [430/1000], Train Loss: 13553540.0000, Val Loss: 3520331.7500\n",
      "Epoch [431/1000], Train Loss: 13552092.0000, Val Loss: 3520141.2500\n",
      "Epoch [432/1000], Train Loss: 13551340.0000, Val Loss: 3519937.2500\n",
      "Epoch [433/1000], Train Loss: 13550864.0000, Val Loss: 3519746.2500\n",
      "Epoch [434/1000], Train Loss: 13549480.0000, Val Loss: 3519575.0000\n",
      "Epoch [435/1000], Train Loss: 13548679.0000, Val Loss: 3519411.5000\n",
      "Epoch [436/1000], Train Loss: 13547035.0000, Val Loss: 3519221.5000\n",
      "Epoch [437/1000], Train Loss: 13546716.0000, Val Loss: 3519053.5000\n",
      "Epoch [438/1000], Train Loss: 13544917.0000, Val Loss: 3518863.2500\n",
      "Epoch [439/1000], Train Loss: 13544845.0000, Val Loss: 3518668.2500\n",
      "Epoch [440/1000], Train Loss: 13543183.0000, Val Loss: 3518495.2500\n",
      "Epoch [441/1000], Train Loss: 13542396.0000, Val Loss: 3518339.2500\n",
      "Epoch [442/1000], Train Loss: 13541738.0000, Val Loss: 3518171.5000\n",
      "Epoch [443/1000], Train Loss: 13540820.0000, Val Loss: 3518004.0000\n",
      "Epoch [444/1000], Train Loss: 13540156.0000, Val Loss: 3517852.7500\n",
      "Epoch [445/1000], Train Loss: 13538985.0000, Val Loss: 3517694.5000\n",
      "Epoch [446/1000], Train Loss: 13538384.0000, Val Loss: 3517511.2500\n",
      "Epoch [447/1000], Train Loss: 13537347.0000, Val Loss: 3517358.2500\n",
      "Epoch [448/1000], Train Loss: 13536515.0000, Val Loss: 3517210.0000\n",
      "Epoch [449/1000], Train Loss: 13535056.0000, Val Loss: 3517065.7500\n",
      "Epoch [450/1000], Train Loss: 13534566.0000, Val Loss: 3516926.0000\n",
      "Epoch [451/1000], Train Loss: 13534020.0000, Val Loss: 3516758.5000\n",
      "Epoch [452/1000], Train Loss: 13532822.0000, Val Loss: 3516583.5000\n",
      "Epoch [453/1000], Train Loss: 13532273.0000, Val Loss: 3516427.7500\n",
      "Epoch [454/1000], Train Loss: 13531564.0000, Val Loss: 3516317.7500\n",
      "Epoch [455/1000], Train Loss: 13530854.0000, Val Loss: 3516217.0000\n",
      "Epoch [456/1000], Train Loss: 13529743.0000, Val Loss: 3516074.7500\n",
      "Epoch [457/1000], Train Loss: 13529051.0000, Val Loss: 3515874.0000\n",
      "Epoch [458/1000], Train Loss: 13528514.0000, Val Loss: 3515706.7500\n",
      "Epoch [459/1000], Train Loss: 13527213.0000, Val Loss: 3515601.0000\n",
      "Epoch [460/1000], Train Loss: 13526547.0000, Val Loss: 3515518.7500\n",
      "Epoch [461/1000], Train Loss: 13525919.0000, Val Loss: 3515377.7500\n",
      "Epoch [462/1000], Train Loss: 13525207.0000, Val Loss: 3515192.0000\n",
      "Epoch [463/1000], Train Loss: 13524716.0000, Val Loss: 3515048.0000\n",
      "Epoch [464/1000], Train Loss: 13523936.0000, Val Loss: 3514972.7500\n",
      "Epoch [465/1000], Train Loss: 13523543.0000, Val Loss: 3514863.0000\n",
      "Epoch [466/1000], Train Loss: 13522681.0000, Val Loss: 3514711.7500\n",
      "Epoch [467/1000], Train Loss: 13521548.0000, Val Loss: 3514558.7500\n",
      "Epoch [468/1000], Train Loss: 13520922.0000, Val Loss: 3514449.2500\n",
      "Epoch [469/1000], Train Loss: 13520183.0000, Val Loss: 3514375.7500\n",
      "Epoch [470/1000], Train Loss: 13519841.0000, Val Loss: 3514273.2500\n",
      "Epoch [471/1000], Train Loss: 13519284.0000, Val Loss: 3514142.0000\n",
      "Epoch [472/1000], Train Loss: 13518814.0000, Val Loss: 3513984.7500\n",
      "Epoch [473/1000], Train Loss: 13518047.0000, Val Loss: 3513874.5000\n",
      "Epoch [474/1000], Train Loss: 13517370.0000, Val Loss: 3513806.2500\n",
      "Epoch [475/1000], Train Loss: 13516880.0000, Val Loss: 3513714.7500\n",
      "Epoch [476/1000], Train Loss: 13516010.0000, Val Loss: 3513571.2500\n",
      "Epoch [477/1000], Train Loss: 13515556.0000, Val Loss: 3513440.2500\n",
      "Epoch [478/1000], Train Loss: 13514754.0000, Val Loss: 3513353.0000\n",
      "Epoch [479/1000], Train Loss: 13514254.0000, Val Loss: 3513283.2500\n",
      "Epoch [480/1000], Train Loss: 13513951.0000, Val Loss: 3513200.7500\n",
      "Epoch [481/1000], Train Loss: 13512496.0000, Val Loss: 3513113.0000\n",
      "Epoch [482/1000], Train Loss: 13512310.0000, Val Loss: 3512962.0000\n",
      "Epoch [483/1000], Train Loss: 13511657.0000, Val Loss: 3512822.2500\n",
      "Epoch [484/1000], Train Loss: 13511155.0000, Val Loss: 3512755.5000\n",
      "Epoch [485/1000], Train Loss: 13510477.0000, Val Loss: 3512701.0000\n",
      "Epoch [486/1000], Train Loss: 13510209.0000, Val Loss: 3512630.5000\n",
      "Epoch [487/1000], Train Loss: 13509372.0000, Val Loss: 3512502.0000\n",
      "Epoch [488/1000], Train Loss: 13508929.0000, Val Loss: 3512362.0000\n",
      "Epoch [489/1000], Train Loss: 13508549.0000, Val Loss: 3512255.0000\n",
      "Epoch [490/1000], Train Loss: 13507719.0000, Val Loss: 3512242.2500\n",
      "Epoch [491/1000], Train Loss: 13507555.0000, Val Loss: 3512187.0000\n",
      "Epoch [492/1000], Train Loss: 13507045.0000, Val Loss: 3512032.2500\n",
      "Epoch [493/1000], Train Loss: 13506464.0000, Val Loss: 3511902.2500\n",
      "Epoch [494/1000], Train Loss: 13505903.0000, Val Loss: 3511792.0000\n",
      "Epoch [495/1000], Train Loss: 13505422.0000, Val Loss: 3511750.7500\n",
      "Epoch [496/1000], Train Loss: 13504880.0000, Val Loss: 3511728.0000\n",
      "Epoch [497/1000], Train Loss: 13504248.0000, Val Loss: 3511633.0000\n",
      "Epoch [498/1000], Train Loss: 13503899.0000, Val Loss: 3511481.2500\n",
      "Epoch [499/1000], Train Loss: 13503628.0000, Val Loss: 3511366.2500\n",
      "Epoch [500/1000], Train Loss: 13503252.0000, Val Loss: 3511349.0000\n",
      "Epoch [501/1000], Train Loss: 13502228.0000, Val Loss: 3511337.7500\n",
      "Epoch [502/1000], Train Loss: 13502186.0000, Val Loss: 3511243.7500\n",
      "Epoch [503/1000], Train Loss: 13501552.0000, Val Loss: 3511122.2500\n",
      "Epoch [504/1000], Train Loss: 13500735.0000, Val Loss: 3511000.5000\n",
      "Epoch [505/1000], Train Loss: 13500418.0000, Val Loss: 3510926.7500\n",
      "Epoch [506/1000], Train Loss: 13500117.0000, Val Loss: 3510917.7500\n",
      "Epoch [507/1000], Train Loss: 13499514.0000, Val Loss: 3510896.0000\n",
      "Epoch [508/1000], Train Loss: 13499454.0000, Val Loss: 3510807.5000\n",
      "Epoch [509/1000], Train Loss: 13499234.0000, Val Loss: 3510655.0000\n",
      "Epoch [510/1000], Train Loss: 13498418.0000, Val Loss: 3510543.7500\n",
      "Epoch [511/1000], Train Loss: 13498132.0000, Val Loss: 3510500.2500\n",
      "Epoch [512/1000], Train Loss: 13497952.0000, Val Loss: 3510533.0000\n",
      "Epoch [513/1000], Train Loss: 13497293.0000, Val Loss: 3510502.2500\n",
      "Epoch [514/1000], Train Loss: 13496515.0000, Val Loss: 3510356.2500\n",
      "Epoch [515/1000], Train Loss: 13496615.0000, Val Loss: 3510226.5000\n",
      "Epoch [516/1000], Train Loss: 13496071.0000, Val Loss: 3510178.7500\n",
      "Epoch [517/1000], Train Loss: 13495834.0000, Val Loss: 3510199.7500\n",
      "Epoch [518/1000], Train Loss: 13495342.0000, Val Loss: 3510178.0000\n",
      "Epoch [519/1000], Train Loss: 13494944.0000, Val Loss: 3510040.2500\n",
      "Epoch [520/1000], Train Loss: 13494291.0000, Val Loss: 3509897.2500\n",
      "Epoch [521/1000], Train Loss: 13493917.0000, Val Loss: 3509825.7500\n",
      "Epoch [522/1000], Train Loss: 13493720.0000, Val Loss: 3509825.0000\n",
      "Epoch [523/1000], Train Loss: 13493256.0000, Val Loss: 3509797.5000\n",
      "Epoch [524/1000], Train Loss: 13492790.0000, Val Loss: 3509737.7500\n",
      "Epoch [525/1000], Train Loss: 13492176.0000, Val Loss: 3509630.7500\n",
      "Epoch [526/1000], Train Loss: 13491865.0000, Val Loss: 3509548.0000\n",
      "Epoch [527/1000], Train Loss: 13491986.0000, Val Loss: 3509501.5000\n",
      "Epoch [528/1000], Train Loss: 13491409.0000, Val Loss: 3509493.2500\n",
      "Epoch [529/1000], Train Loss: 13490994.0000, Val Loss: 3509454.0000\n",
      "Epoch [530/1000], Train Loss: 13490749.0000, Val Loss: 3509360.2500\n",
      "Epoch [531/1000], Train Loss: 13490644.0000, Val Loss: 3509272.7500\n",
      "Epoch [532/1000], Train Loss: 13490036.0000, Val Loss: 3509254.2500\n",
      "Epoch [533/1000], Train Loss: 13489756.0000, Val Loss: 3509257.2500\n",
      "Epoch [534/1000], Train Loss: 13489659.0000, Val Loss: 3509203.0000\n",
      "Epoch [535/1000], Train Loss: 13488855.0000, Val Loss: 3509107.2500\n",
      "Epoch [536/1000], Train Loss: 13488533.0000, Val Loss: 3509029.7500\n",
      "Epoch [537/1000], Train Loss: 13488508.0000, Val Loss: 3508998.7500\n",
      "Epoch [538/1000], Train Loss: 13488280.0000, Val Loss: 3508978.7500\n",
      "Epoch [539/1000], Train Loss: 13488192.0000, Val Loss: 3508929.0000\n",
      "Epoch [540/1000], Train Loss: 13487469.0000, Val Loss: 3508854.2500\n",
      "Epoch [541/1000], Train Loss: 13487300.0000, Val Loss: 3508795.7500\n",
      "Epoch [542/1000], Train Loss: 13486790.0000, Val Loss: 3508770.7500\n",
      "Epoch [543/1000], Train Loss: 13486933.0000, Val Loss: 3508716.5000\n",
      "Epoch [544/1000], Train Loss: 13486166.0000, Val Loss: 3508635.7500\n",
      "Epoch [545/1000], Train Loss: 13486107.0000, Val Loss: 3508620.0000\n",
      "Epoch [546/1000], Train Loss: 13485886.0000, Val Loss: 3508634.2500\n",
      "Epoch [547/1000], Train Loss: 13486012.0000, Val Loss: 3508600.7500\n",
      "Epoch [548/1000], Train Loss: 13484691.0000, Val Loss: 3508509.7500\n",
      "Epoch [549/1000], Train Loss: 13484758.0000, Val Loss: 3508421.2500\n",
      "Epoch [550/1000], Train Loss: 13484740.0000, Val Loss: 3508370.2500\n",
      "Epoch [551/1000], Train Loss: 13484723.0000, Val Loss: 3508383.5000\n",
      "Epoch [552/1000], Train Loss: 13484240.0000, Val Loss: 3508380.2500\n",
      "Epoch [553/1000], Train Loss: 13484258.0000, Val Loss: 3508303.2500\n",
      "Epoch [554/1000], Train Loss: 13483641.0000, Val Loss: 3508216.2500\n",
      "Epoch [555/1000], Train Loss: 13483094.0000, Val Loss: 3508153.5000\n",
      "Epoch [556/1000], Train Loss: 13483018.0000, Val Loss: 3508140.2500\n",
      "Epoch [557/1000], Train Loss: 13482494.0000, Val Loss: 3508128.0000\n",
      "Epoch [558/1000], Train Loss: 13482329.0000, Val Loss: 3508102.7500\n",
      "Epoch [559/1000], Train Loss: 13482458.0000, Val Loss: 3508049.2500\n",
      "Epoch [560/1000], Train Loss: 13481568.0000, Val Loss: 3508012.2500\n",
      "Epoch [561/1000], Train Loss: 13481735.0000, Val Loss: 3507999.2500\n",
      "Epoch [562/1000], Train Loss: 13481439.0000, Val Loss: 3507977.7500\n",
      "Epoch [563/1000], Train Loss: 13481120.0000, Val Loss: 3507921.0000\n",
      "Epoch [564/1000], Train Loss: 13481078.0000, Val Loss: 3507896.2500\n",
      "Epoch [565/1000], Train Loss: 13480742.0000, Val Loss: 3507872.7500\n",
      "Epoch [566/1000], Train Loss: 13480567.0000, Val Loss: 3507852.7500\n",
      "Epoch [567/1000], Train Loss: 13480214.0000, Val Loss: 3507786.0000\n",
      "Epoch [568/1000], Train Loss: 13479818.0000, Val Loss: 3507702.5000\n",
      "Epoch [569/1000], Train Loss: 13479822.0000, Val Loss: 3507672.5000\n",
      "Epoch [570/1000], Train Loss: 13479536.0000, Val Loss: 3507678.0000\n",
      "Epoch [571/1000], Train Loss: 13479262.0000, Val Loss: 3507662.5000\n",
      "Epoch [572/1000], Train Loss: 13479179.0000, Val Loss: 3507618.7500\n",
      "Epoch [573/1000], Train Loss: 13479081.0000, Val Loss: 3507552.2500\n",
      "Epoch [574/1000], Train Loss: 13478580.0000, Val Loss: 3507514.2500\n",
      "Epoch [575/1000], Train Loss: 13478712.0000, Val Loss: 3507490.7500\n",
      "Epoch [576/1000], Train Loss: 13478469.0000, Val Loss: 3507487.7500\n",
      "Epoch [577/1000], Train Loss: 13477834.0000, Val Loss: 3507429.0000\n",
      "Epoch [578/1000], Train Loss: 13477612.0000, Val Loss: 3507382.2500\n",
      "Epoch [579/1000], Train Loss: 13477946.0000, Val Loss: 3507363.7500\n",
      "Epoch [580/1000], Train Loss: 13477324.0000, Val Loss: 3507357.5000\n",
      "Epoch [581/1000], Train Loss: 13477196.0000, Val Loss: 3507341.5000\n",
      "Epoch [582/1000], Train Loss: 13476874.0000, Val Loss: 3507285.0000\n",
      "Epoch [583/1000], Train Loss: 13476373.0000, Val Loss: 3507236.7500\n",
      "Epoch [584/1000], Train Loss: 13476682.0000, Val Loss: 3507233.0000\n",
      "Epoch [585/1000], Train Loss: 13476570.0000, Val Loss: 3507227.2500\n",
      "Epoch [586/1000], Train Loss: 13476010.0000, Val Loss: 3507191.7500\n",
      "Epoch [587/1000], Train Loss: 13476025.0000, Val Loss: 3507146.2500\n",
      "Epoch [588/1000], Train Loss: 13476090.0000, Val Loss: 3507148.7500\n",
      "Epoch [589/1000], Train Loss: 13475413.0000, Val Loss: 3507138.2500\n",
      "Epoch [590/1000], Train Loss: 13475680.0000, Val Loss: 3507065.0000\n",
      "Epoch [591/1000], Train Loss: 13475157.0000, Val Loss: 3507022.7500\n",
      "Epoch [592/1000], Train Loss: 13475379.0000, Val Loss: 3507003.2500\n",
      "Epoch [593/1000], Train Loss: 13474913.0000, Val Loss: 3507015.2500\n",
      "Epoch [594/1000], Train Loss: 13474742.0000, Val Loss: 3507006.7500\n",
      "Epoch [595/1000], Train Loss: 13474310.0000, Val Loss: 3506962.2500\n",
      "Epoch [596/1000], Train Loss: 13474163.0000, Val Loss: 3506899.7500\n",
      "Epoch [597/1000], Train Loss: 13474171.0000, Val Loss: 3506862.7500\n",
      "Epoch [598/1000], Train Loss: 13473780.0000, Val Loss: 3506887.7500\n",
      "Epoch [599/1000], Train Loss: 13473966.0000, Val Loss: 3506898.2500\n",
      "Epoch [600/1000], Train Loss: 13473658.0000, Val Loss: 3506834.7500\n",
      "Epoch [601/1000], Train Loss: 13473800.0000, Val Loss: 3506761.2500\n",
      "Epoch [602/1000], Train Loss: 13473502.0000, Val Loss: 3506747.2500\n",
      "Epoch [603/1000], Train Loss: 13473172.0000, Val Loss: 3506820.2500\n",
      "Epoch [604/1000], Train Loss: 13473406.0000, Val Loss: 3506832.0000\n",
      "Epoch [605/1000], Train Loss: 13472610.0000, Val Loss: 3506763.7500\n",
      "Epoch [606/1000], Train Loss: 13472842.0000, Val Loss: 3506684.2500\n",
      "Epoch [607/1000], Train Loss: 13472396.0000, Val Loss: 3506638.5000\n",
      "Epoch [608/1000], Train Loss: 13472319.0000, Val Loss: 3506672.2500\n",
      "Epoch [609/1000], Train Loss: 13472190.0000, Val Loss: 3506717.0000\n",
      "Epoch [610/1000], Train Loss: 13472144.0000, Val Loss: 3506683.7500\n",
      "Epoch [611/1000], Train Loss: 13471562.0000, Val Loss: 3506572.0000\n",
      "Epoch [612/1000], Train Loss: 13471755.0000, Val Loss: 3506483.5000\n",
      "Epoch [613/1000], Train Loss: 13471700.0000, Val Loss: 3506481.0000\n",
      "Epoch [614/1000], Train Loss: 13471344.0000, Val Loss: 3506550.7500\n",
      "Epoch [615/1000], Train Loss: 13470945.0000, Val Loss: 3506584.2500\n",
      "Epoch [616/1000], Train Loss: 13471234.0000, Val Loss: 3506476.0000\n",
      "Epoch [617/1000], Train Loss: 13470994.0000, Val Loss: 3506380.0000\n",
      "Epoch [618/1000], Train Loss: 13470762.0000, Val Loss: 3506391.5000\n",
      "Epoch [619/1000], Train Loss: 13470708.0000, Val Loss: 3506448.7500\n",
      "Epoch [620/1000], Train Loss: 13470742.0000, Val Loss: 3506482.7500\n",
      "Epoch [621/1000], Train Loss: 13470455.0000, Val Loss: 3506455.7500\n",
      "Epoch [622/1000], Train Loss: 13470319.0000, Val Loss: 3506408.2500\n",
      "Epoch [623/1000], Train Loss: 13469858.0000, Val Loss: 3506375.7500\n",
      "Epoch [624/1000], Train Loss: 13469929.0000, Val Loss: 3506377.2500\n",
      "Epoch [625/1000], Train Loss: 13469762.0000, Val Loss: 3506401.2500\n",
      "Epoch [626/1000], Train Loss: 13469609.0000, Val Loss: 3506389.2500\n",
      "Epoch [627/1000], Train Loss: 13469613.0000, Val Loss: 3506345.7500\n",
      "Epoch [628/1000], Train Loss: 13469334.0000, Val Loss: 3506283.0000\n",
      "Epoch [629/1000], Train Loss: 13469462.0000, Val Loss: 3506263.5000\n",
      "Epoch [630/1000], Train Loss: 13469278.0000, Val Loss: 3506261.7500\n",
      "Epoch [631/1000], Train Loss: 13468825.0000, Val Loss: 3506256.5000\n",
      "Epoch [632/1000], Train Loss: 13468758.0000, Val Loss: 3506255.0000\n",
      "Epoch [633/1000], Train Loss: 13468743.0000, Val Loss: 3506223.7500\n",
      "Epoch [634/1000], Train Loss: 13468619.0000, Val Loss: 3506184.5000\n",
      "Epoch [635/1000], Train Loss: 13468611.0000, Val Loss: 3506158.5000\n",
      "Epoch [636/1000], Train Loss: 13468148.0000, Val Loss: 3506157.2500\n",
      "Epoch [637/1000], Train Loss: 13468098.0000, Val Loss: 3506182.2500\n",
      "Epoch [638/1000], Train Loss: 13467996.0000, Val Loss: 3506184.5000\n",
      "Epoch [639/1000], Train Loss: 13468025.0000, Val Loss: 3506154.2500\n",
      "Epoch [640/1000], Train Loss: 13468178.0000, Val Loss: 3506155.5000\n",
      "Epoch [641/1000], Train Loss: 13467720.0000, Val Loss: 3506148.5000\n",
      "Epoch [642/1000], Train Loss: 13467700.0000, Val Loss: 3506127.7500\n",
      "Epoch [643/1000], Train Loss: 13467538.0000, Val Loss: 3506085.7500\n",
      "Epoch [644/1000], Train Loss: 13467358.0000, Val Loss: 3506057.0000\n",
      "Epoch [645/1000], Train Loss: 13467630.0000, Val Loss: 3506077.2500\n",
      "Epoch [646/1000], Train Loss: 13467514.0000, Val Loss: 3506082.5000\n",
      "Epoch [647/1000], Train Loss: 13467251.0000, Val Loss: 3506048.2500\n",
      "Epoch [648/1000], Train Loss: 13467084.0000, Val Loss: 3505967.2500\n",
      "Epoch [649/1000], Train Loss: 13466878.0000, Val Loss: 3505978.5000\n",
      "Epoch [650/1000], Train Loss: 13467003.0000, Val Loss: 3506037.2500\n",
      "Epoch [651/1000], Train Loss: 13466936.0000, Val Loss: 3506074.0000\n",
      "Epoch [652/1000], Train Loss: 13466611.0000, Val Loss: 3506018.5000\n",
      "Epoch [653/1000], Train Loss: 13466419.0000, Val Loss: 3505938.2500\n",
      "Epoch [654/1000], Train Loss: 13466162.0000, Val Loss: 3505896.5000\n",
      "Epoch [655/1000], Train Loss: 13466328.0000, Val Loss: 3505908.5000\n",
      "Epoch [656/1000], Train Loss: 13466068.0000, Val Loss: 3505951.5000\n",
      "Epoch [657/1000], Train Loss: 13466189.0000, Val Loss: 3505930.2500\n",
      "Epoch [658/1000], Train Loss: 13465958.0000, Val Loss: 3505868.5000\n",
      "Epoch [659/1000], Train Loss: 13466083.0000, Val Loss: 3505847.2500\n",
      "Epoch [660/1000], Train Loss: 13465775.0000, Val Loss: 3505845.0000\n",
      "Epoch [661/1000], Train Loss: 13465722.0000, Val Loss: 3505826.0000\n",
      "Epoch [662/1000], Train Loss: 13465602.0000, Val Loss: 3505812.5000\n",
      "Epoch [663/1000], Train Loss: 13465691.0000, Val Loss: 3505841.2500\n",
      "Epoch [664/1000], Train Loss: 13465509.0000, Val Loss: 3505883.5000\n",
      "Epoch [665/1000], Train Loss: 13465394.0000, Val Loss: 3505860.0000\n",
      "Epoch [666/1000], Train Loss: 13465390.0000, Val Loss: 3505807.0000\n",
      "Epoch [667/1000], Train Loss: 13465007.0000, Val Loss: 3505783.2500\n",
      "Epoch [668/1000], Train Loss: 13464564.0000, Val Loss: 3505787.2500\n",
      "Epoch [669/1000], Train Loss: 13464523.0000, Val Loss: 3505814.0000\n",
      "Epoch [670/1000], Train Loss: 13464717.0000, Val Loss: 3505826.2500\n",
      "Epoch [671/1000], Train Loss: 13464970.0000, Val Loss: 3505804.2500\n",
      "Epoch [672/1000], Train Loss: 13464481.0000, Val Loss: 3505747.5000\n",
      "Epoch [673/1000], Train Loss: 13464704.0000, Val Loss: 3505692.7500\n",
      "Epoch [674/1000], Train Loss: 13464381.0000, Val Loss: 3505692.0000\n",
      "Epoch [675/1000], Train Loss: 13464561.0000, Val Loss: 3505758.2500\n",
      "Epoch [676/1000], Train Loss: 13464570.0000, Val Loss: 3505776.2500\n",
      "Epoch [677/1000], Train Loss: 13464372.0000, Val Loss: 3505711.7500\n",
      "Epoch [678/1000], Train Loss: 13464042.0000, Val Loss: 3505662.0000\n",
      "Epoch [679/1000], Train Loss: 13463918.0000, Val Loss: 3505663.0000\n",
      "Epoch [680/1000], Train Loss: 13464307.0000, Val Loss: 3505713.2500\n",
      "Epoch [681/1000], Train Loss: 13463997.0000, Val Loss: 3505759.2500\n",
      "Epoch [682/1000], Train Loss: 13463966.0000, Val Loss: 3505746.5000\n",
      "Epoch [683/1000], Train Loss: 13463761.0000, Val Loss: 3505686.7500\n",
      "Epoch [684/1000], Train Loss: 13463922.0000, Val Loss: 3505643.0000\n",
      "Epoch [685/1000], Train Loss: 13463734.0000, Val Loss: 3505635.5000\n",
      "Epoch [686/1000], Train Loss: 13463567.0000, Val Loss: 3505688.7500\n",
      "Epoch [687/1000], Train Loss: 13463251.0000, Val Loss: 3505747.7500\n",
      "Epoch [688/1000], Train Loss: 13463533.0000, Val Loss: 3505666.0000\n",
      "Epoch [689/1000], Train Loss: 13463374.0000, Val Loss: 3505579.7500\n",
      "Epoch [690/1000], Train Loss: 13463730.0000, Val Loss: 3505577.2500\n",
      "Epoch [691/1000], Train Loss: 13463400.0000, Val Loss: 3505647.0000\n",
      "Epoch [692/1000], Train Loss: 13463214.0000, Val Loss: 3505690.5000\n",
      "Epoch [693/1000], Train Loss: 13463015.0000, Val Loss: 3505629.0000\n",
      "Epoch [694/1000], Train Loss: 13463097.0000, Val Loss: 3505565.7500\n",
      "Epoch [695/1000], Train Loss: 13463028.0000, Val Loss: 3505538.7500\n",
      "Epoch [696/1000], Train Loss: 13462579.0000, Val Loss: 3505575.5000\n",
      "Epoch [697/1000], Train Loss: 13462970.0000, Val Loss: 3505608.0000\n",
      "Epoch [698/1000], Train Loss: 13462676.0000, Val Loss: 3505607.0000\n",
      "Epoch [699/1000], Train Loss: 13462409.0000, Val Loss: 3505571.0000\n",
      "Epoch [700/1000], Train Loss: 13462589.0000, Val Loss: 3505508.7500\n",
      "Epoch [701/1000], Train Loss: 13462606.0000, Val Loss: 3505495.0000\n",
      "Epoch [702/1000], Train Loss: 13462546.0000, Val Loss: 3505507.5000\n",
      "Epoch [703/1000], Train Loss: 13462288.0000, Val Loss: 3505541.2500\n",
      "Epoch [704/1000], Train Loss: 13462451.0000, Val Loss: 3505530.7500\n",
      "Epoch [705/1000], Train Loss: 13462307.0000, Val Loss: 3505481.7500\n",
      "Epoch [706/1000], Train Loss: 13462351.0000, Val Loss: 3505454.0000\n",
      "Epoch [707/1000], Train Loss: 13462081.0000, Val Loss: 3505492.0000\n",
      "Epoch [708/1000], Train Loss: 13462084.0000, Val Loss: 3505569.0000\n",
      "Epoch [709/1000], Train Loss: 13461801.0000, Val Loss: 3505555.2500\n",
      "Epoch [710/1000], Train Loss: 13461735.0000, Val Loss: 3505470.0000\n",
      "Epoch [711/1000], Train Loss: 13462002.0000, Val Loss: 3505427.2500\n",
      "Epoch [712/1000], Train Loss: 13461736.0000, Val Loss: 3505444.2500\n",
      "Epoch [713/1000], Train Loss: 13461836.0000, Val Loss: 3505502.0000\n",
      "Epoch [714/1000], Train Loss: 13461789.0000, Val Loss: 3505492.5000\n",
      "Epoch [715/1000], Train Loss: 13461562.0000, Val Loss: 3505449.7500\n",
      "Epoch [716/1000], Train Loss: 13461585.0000, Val Loss: 3505429.7500\n",
      "Epoch [717/1000], Train Loss: 13461705.0000, Val Loss: 3505440.0000\n",
      "Epoch [718/1000], Train Loss: 13461217.0000, Val Loss: 3505426.5000\n",
      "Epoch [719/1000], Train Loss: 13461185.0000, Val Loss: 3505406.5000\n",
      "Epoch [720/1000], Train Loss: 13461424.0000, Val Loss: 3505391.2500\n",
      "Epoch [721/1000], Train Loss: 13461209.0000, Val Loss: 3505388.2500\n",
      "Epoch [722/1000], Train Loss: 13460944.0000, Val Loss: 3505384.7500\n",
      "Epoch [723/1000], Train Loss: 13461299.0000, Val Loss: 3505388.5000\n",
      "Epoch [724/1000], Train Loss: 13461202.0000, Val Loss: 3505384.5000\n",
      "Epoch [725/1000], Train Loss: 13461073.0000, Val Loss: 3505379.0000\n",
      "Epoch [726/1000], Train Loss: 13460898.0000, Val Loss: 3505375.7500\n",
      "Epoch [727/1000], Train Loss: 13460893.0000, Val Loss: 3505383.0000\n",
      "Epoch [728/1000], Train Loss: 13461046.0000, Val Loss: 3505385.0000\n",
      "Epoch [729/1000], Train Loss: 13461206.0000, Val Loss: 3505381.0000\n",
      "Epoch [730/1000], Train Loss: 13461046.0000, Val Loss: 3505369.5000\n",
      "Epoch [731/1000], Train Loss: 13460663.0000, Val Loss: 3505374.7500\n",
      "Epoch [732/1000], Train Loss: 13460617.0000, Val Loss: 3505395.5000\n",
      "Epoch [733/1000], Train Loss: 13460302.0000, Val Loss: 3505385.7500\n",
      "Epoch [734/1000], Train Loss: 13460621.0000, Val Loss: 3505319.5000\n",
      "Epoch [735/1000], Train Loss: 13460738.0000, Val Loss: 3505308.5000\n",
      "Epoch [736/1000], Train Loss: 13460570.0000, Val Loss: 3505377.7500\n",
      "Epoch [737/1000], Train Loss: 13460768.0000, Val Loss: 3505417.5000\n",
      "Epoch [738/1000], Train Loss: 13460411.0000, Val Loss: 3505392.7500\n",
      "Epoch [739/1000], Train Loss: 13460676.0000, Val Loss: 3505290.2500\n",
      "Epoch [740/1000], Train Loss: 13460379.0000, Val Loss: 3505266.0000\n",
      "Epoch [741/1000], Train Loss: 13460138.0000, Val Loss: 3505345.5000\n",
      "Epoch [742/1000], Train Loss: 13460181.0000, Val Loss: 3505429.7500\n",
      "Epoch [743/1000], Train Loss: 13459850.0000, Val Loss: 3505378.0000\n",
      "Epoch [744/1000], Train Loss: 13460080.0000, Val Loss: 3505292.5000\n",
      "Epoch [745/1000], Train Loss: 13460028.0000, Val Loss: 3505268.2500\n",
      "Epoch [746/1000], Train Loss: 13459720.0000, Val Loss: 3505331.5000\n",
      "Epoch [747/1000], Train Loss: 13460136.0000, Val Loss: 3505402.0000\n",
      "Epoch [748/1000], Train Loss: 13459920.0000, Val Loss: 3505368.7500\n",
      "Epoch [749/1000], Train Loss: 13460028.0000, Val Loss: 3505289.7500\n",
      "Epoch [750/1000], Train Loss: 13459963.0000, Val Loss: 3505274.2500\n",
      "Epoch [751/1000], Train Loss: 13459613.0000, Val Loss: 3505305.5000\n",
      "Epoch [752/1000], Train Loss: 13459828.0000, Val Loss: 3505345.5000\n",
      "Epoch [753/1000], Train Loss: 13459517.0000, Val Loss: 3505346.7500\n",
      "Epoch [754/1000], Train Loss: 13459567.0000, Val Loss: 3505297.7500\n",
      "Epoch [755/1000], Train Loss: 13459475.0000, Val Loss: 3505246.2500\n",
      "Epoch [756/1000], Train Loss: 13459623.0000, Val Loss: 3505232.7500\n",
      "Epoch [757/1000], Train Loss: 13459714.0000, Val Loss: 3505272.2500\n",
      "Epoch [758/1000], Train Loss: 13459681.0000, Val Loss: 3505302.2500\n",
      "Epoch [759/1000], Train Loss: 13459800.0000, Val Loss: 3505313.2500\n",
      "Epoch [760/1000], Train Loss: 13459142.0000, Val Loss: 3505269.7500\n",
      "Epoch [761/1000], Train Loss: 13459164.0000, Val Loss: 3505249.7500\n",
      "Epoch [762/1000], Train Loss: 13459436.0000, Val Loss: 3505283.7500\n",
      "Epoch [763/1000], Train Loss: 13459551.0000, Val Loss: 3505316.0000\n",
      "Epoch [764/1000], Train Loss: 13459178.0000, Val Loss: 3505296.2500\n",
      "Epoch [765/1000], Train Loss: 13459534.0000, Val Loss: 3505254.2500\n",
      "Epoch [766/1000], Train Loss: 13458971.0000, Val Loss: 3505228.7500\n",
      "Epoch [767/1000], Train Loss: 13459128.0000, Val Loss: 3505253.5000\n",
      "Epoch [768/1000], Train Loss: 13458920.0000, Val Loss: 3505294.7500\n",
      "Epoch [769/1000], Train Loss: 13459230.0000, Val Loss: 3505302.5000\n",
      "Epoch [770/1000], Train Loss: 13459149.0000, Val Loss: 3505266.2500\n",
      "Epoch [771/1000], Train Loss: 13458779.0000, Val Loss: 3505223.5000\n",
      "Epoch [772/1000], Train Loss: 13458666.0000, Val Loss: 3505224.7500\n",
      "Epoch [773/1000], Train Loss: 13459068.0000, Val Loss: 3505245.7500\n",
      "Epoch [774/1000], Train Loss: 13458481.0000, Val Loss: 3505274.0000\n",
      "Epoch [775/1000], Train Loss: 13458966.0000, Val Loss: 3505290.2500\n",
      "Epoch [776/1000], Train Loss: 13458536.0000, Val Loss: 3505249.5000\n",
      "Epoch [777/1000], Train Loss: 13458647.0000, Val Loss: 3505218.0000\n",
      "Epoch [778/1000], Train Loss: 13458229.0000, Val Loss: 3505237.0000\n",
      "Epoch [779/1000], Train Loss: 13458844.0000, Val Loss: 3505256.2500\n",
      "Epoch [780/1000], Train Loss: 13458748.0000, Val Loss: 3505257.7500\n",
      "Epoch [781/1000], Train Loss: 13458671.0000, Val Loss: 3505243.7500\n",
      "Epoch [782/1000], Train Loss: 13458417.0000, Val Loss: 3505213.7500\n",
      "Epoch [783/1000], Train Loss: 13458885.0000, Val Loss: 3505190.7500\n",
      "Epoch [784/1000], Train Loss: 13458203.0000, Val Loss: 3505202.7500\n",
      "Epoch [785/1000], Train Loss: 13458356.0000, Val Loss: 3505223.2500\n",
      "Epoch [786/1000], Train Loss: 13458360.0000, Val Loss: 3505265.7500\n",
      "Epoch [787/1000], Train Loss: 13458337.0000, Val Loss: 3505254.2500\n",
      "Epoch [788/1000], Train Loss: 13458064.0000, Val Loss: 3505174.2500\n",
      "Epoch [789/1000], Train Loss: 13458323.0000, Val Loss: 3505152.0000\n",
      "Epoch [790/1000], Train Loss: 13458511.0000, Val Loss: 3505215.0000\n",
      "Epoch [791/1000], Train Loss: 13458127.0000, Val Loss: 3505267.5000\n",
      "Epoch [792/1000], Train Loss: 13458117.0000, Val Loss: 3505248.7500\n",
      "Epoch [793/1000], Train Loss: 13457562.0000, Val Loss: 3505198.5000\n",
      "Epoch [794/1000], Train Loss: 13458121.0000, Val Loss: 3505170.5000\n",
      "Epoch [795/1000], Train Loss: 13458090.0000, Val Loss: 3505208.7500\n",
      "Epoch [796/1000], Train Loss: 13458007.0000, Val Loss: 3505247.7500\n",
      "Epoch [797/1000], Train Loss: 13457775.0000, Val Loss: 3505241.5000\n",
      "Epoch [798/1000], Train Loss: 13457830.0000, Val Loss: 3505203.2500\n",
      "Epoch [799/1000], Train Loss: 13457936.0000, Val Loss: 3505186.5000\n",
      "Epoch [800/1000], Train Loss: 13458008.0000, Val Loss: 3505198.5000\n",
      "Epoch [801/1000], Train Loss: 13457882.0000, Val Loss: 3505226.7500\n",
      "Epoch [802/1000], Train Loss: 13457975.0000, Val Loss: 3505215.0000\n",
      "Epoch [803/1000], Train Loss: 13458040.0000, Val Loss: 3505170.2500\n",
      "Epoch [804/1000], Train Loss: 13457554.0000, Val Loss: 3505140.0000\n",
      "Epoch [805/1000], Train Loss: 13457887.0000, Val Loss: 3505146.2500\n",
      "Epoch [806/1000], Train Loss: 13457460.0000, Val Loss: 3505182.2500\n",
      "Epoch [807/1000], Train Loss: 13457517.0000, Val Loss: 3505217.2500\n",
      "Epoch [808/1000], Train Loss: 13457400.0000, Val Loss: 3505227.2500\n",
      "Epoch [809/1000], Train Loss: 13457488.0000, Val Loss: 3505183.7500\n",
      "Epoch [810/1000], Train Loss: 13457460.0000, Val Loss: 3505149.2500\n",
      "Epoch [811/1000], Train Loss: 13457470.0000, Val Loss: 3505136.7500\n",
      "Epoch [812/1000], Train Loss: 13457537.0000, Val Loss: 3505179.2500\n",
      "Epoch [813/1000], Train Loss: 13457810.0000, Val Loss: 3505207.2500\n",
      "Epoch [814/1000], Train Loss: 13457646.0000, Val Loss: 3505192.2500\n",
      "Epoch [815/1000], Train Loss: 13457475.0000, Val Loss: 3505193.0000\n",
      "Epoch [816/1000], Train Loss: 13457342.0000, Val Loss: 3505171.7500\n",
      "Epoch [817/1000], Train Loss: 13457263.0000, Val Loss: 3505153.2500\n",
      "Epoch [818/1000], Train Loss: 13457364.0000, Val Loss: 3505170.5000\n",
      "Epoch [819/1000], Train Loss: 13457466.0000, Val Loss: 3505219.2500\n",
      "Epoch [820/1000], Train Loss: 13457418.0000, Val Loss: 3505223.0000\n",
      "Epoch [821/1000], Train Loss: 13457200.0000, Val Loss: 3505176.5000\n",
      "Epoch [822/1000], Train Loss: 13457031.0000, Val Loss: 3505148.5000\n",
      "Epoch [823/1000], Train Loss: 13457010.0000, Val Loss: 3505160.2500\n",
      "Epoch [824/1000], Train Loss: 13457218.0000, Val Loss: 3505222.7500\n",
      "Epoch [825/1000], Train Loss: 13457116.0000, Val Loss: 3505207.7500\n",
      "Epoch [826/1000], Train Loss: 13457205.0000, Val Loss: 3505164.0000\n",
      "Epoch [827/1000], Train Loss: 13457140.0000, Val Loss: 3505115.5000\n",
      "Epoch [828/1000], Train Loss: 13456826.0000, Val Loss: 3505127.5000\n",
      "Epoch [829/1000], Train Loss: 13456814.0000, Val Loss: 3505185.0000\n",
      "Epoch [830/1000], Train Loss: 13456781.0000, Val Loss: 3505208.7500\n",
      "Epoch [831/1000], Train Loss: 13457075.0000, Val Loss: 3505176.2500\n",
      "Epoch [832/1000], Train Loss: 13457130.0000, Val Loss: 3505139.2500\n",
      "Epoch [833/1000], Train Loss: 13457004.0000, Val Loss: 3505124.0000\n",
      "Epoch [834/1000], Train Loss: 13456934.0000, Val Loss: 3505158.7500\n",
      "Epoch [835/1000], Train Loss: 13457012.0000, Val Loss: 3505214.7500\n",
      "Epoch [836/1000], Train Loss: 13456620.0000, Val Loss: 3505225.7500\n",
      "Epoch [837/1000], Train Loss: 13456772.0000, Val Loss: 3505184.7500\n",
      "Epoch [838/1000], Train Loss: 13456574.0000, Val Loss: 3505128.0000\n",
      "Epoch [839/1000], Train Loss: 13456901.0000, Val Loss: 3505121.0000\n",
      "Epoch [840/1000], Train Loss: 13456981.0000, Val Loss: 3505167.5000\n",
      "Epoch [841/1000], Train Loss: 13456951.0000, Val Loss: 3505200.5000\n",
      "Epoch [842/1000], Train Loss: 13456670.0000, Val Loss: 3505147.7500\n",
      "Epoch [843/1000], Train Loss: 13456523.0000, Val Loss: 3505073.0000\n",
      "Epoch [844/1000], Train Loss: 13456894.0000, Val Loss: 3505053.0000\n",
      "Epoch [845/1000], Train Loss: 13456571.0000, Val Loss: 3505139.2500\n",
      "Epoch [846/1000], Train Loss: 13456800.0000, Val Loss: 3505239.7500\n",
      "Epoch [847/1000], Train Loss: 13456597.0000, Val Loss: 3505217.2500\n",
      "Epoch [848/1000], Train Loss: 13456848.0000, Val Loss: 3505123.5000\n",
      "Epoch [849/1000], Train Loss: 13456298.0000, Val Loss: 3505043.0000\n",
      "Epoch [850/1000], Train Loss: 13456772.0000, Val Loss: 3505072.7500\n",
      "Epoch [851/1000], Train Loss: 13456835.0000, Val Loss: 3505175.7500\n",
      "Epoch [852/1000], Train Loss: 13456481.0000, Val Loss: 3505222.7500\n",
      "Epoch [853/1000], Train Loss: 13456547.0000, Val Loss: 3505144.0000\n",
      "Epoch [854/1000], Train Loss: 13456560.0000, Val Loss: 3505049.5000\n",
      "Epoch [855/1000], Train Loss: 13456716.0000, Val Loss: 3505050.7500\n",
      "Epoch [856/1000], Train Loss: 13456403.0000, Val Loss: 3505131.2500\n",
      "Epoch [857/1000], Train Loss: 13456509.0000, Val Loss: 3505194.2500\n",
      "Epoch [858/1000], Train Loss: 13456452.0000, Val Loss: 3505160.7500\n",
      "Epoch [859/1000], Train Loss: 13456169.0000, Val Loss: 3505114.5000\n",
      "Epoch [860/1000], Train Loss: 13456207.0000, Val Loss: 3505094.7500\n",
      "Epoch [861/1000], Train Loss: 13456605.0000, Val Loss: 3505110.7500\n",
      "Epoch [862/1000], Train Loss: 13456168.0000, Val Loss: 3505161.7500\n",
      "Epoch [863/1000], Train Loss: 13456179.0000, Val Loss: 3505210.7500\n",
      "Epoch [864/1000], Train Loss: 13456167.0000, Val Loss: 3505170.7500\n",
      "Epoch [865/1000], Train Loss: 13456272.0000, Val Loss: 3505129.5000\n",
      "Epoch [866/1000], Train Loss: 13456290.0000, Val Loss: 3505078.2500\n",
      "Epoch [867/1000], Train Loss: 13456328.0000, Val Loss: 3505089.2500\n",
      "Epoch [868/1000], Train Loss: 13456139.0000, Val Loss: 3505131.7500\n",
      "Epoch [869/1000], Train Loss: 13456068.0000, Val Loss: 3505154.0000\n",
      "Epoch [870/1000], Train Loss: 13456099.0000, Val Loss: 3505124.2500\n",
      "Epoch [871/1000], Train Loss: 13456335.0000, Val Loss: 3505055.5000\n",
      "Epoch [872/1000], Train Loss: 13455694.0000, Val Loss: 3505049.0000\n",
      "Epoch [873/1000], Train Loss: 13456000.0000, Val Loss: 3505108.5000\n",
      "Epoch [874/1000], Train Loss: 13456028.0000, Val Loss: 3505165.0000\n",
      "Epoch [875/1000], Train Loss: 13455898.0000, Val Loss: 3505158.2500\n",
      "Epoch [876/1000], Train Loss: 13455867.0000, Val Loss: 3505119.2500\n",
      "Epoch [877/1000], Train Loss: 13455954.0000, Val Loss: 3505114.0000\n",
      "Epoch [878/1000], Train Loss: 13456327.0000, Val Loss: 3505149.2500\n",
      "Epoch [879/1000], Train Loss: 13455612.0000, Val Loss: 3505176.7500\n",
      "Epoch [880/1000], Train Loss: 13455894.0000, Val Loss: 3505164.7500\n",
      "Epoch [881/1000], Train Loss: 13455960.0000, Val Loss: 3505128.7500\n",
      "Epoch [882/1000], Train Loss: 13456069.0000, Val Loss: 3505109.2500\n",
      "Epoch [883/1000], Train Loss: 13455714.0000, Val Loss: 3505124.2500\n",
      "Epoch [884/1000], Train Loss: 13456012.0000, Val Loss: 3505119.7500\n",
      "Epoch [885/1000], Train Loss: 13455842.0000, Val Loss: 3505107.2500\n",
      "Epoch [886/1000], Train Loss: 13455521.0000, Val Loss: 3505124.0000\n",
      "Epoch [887/1000], Train Loss: 13455640.0000, Val Loss: 3505154.7500\n",
      "Epoch [888/1000], Train Loss: 13455955.0000, Val Loss: 3505156.2500\n",
      "Epoch [889/1000], Train Loss: 13455889.0000, Val Loss: 3505121.2500\n",
      "Epoch [890/1000], Train Loss: 13455715.0000, Val Loss: 3505113.0000\n",
      "Epoch [891/1000], Train Loss: 13455656.0000, Val Loss: 3505122.2500\n",
      "Epoch [892/1000], Train Loss: 13455746.0000, Val Loss: 3505112.5000\n",
      "Epoch [893/1000], Train Loss: 13455853.0000, Val Loss: 3505112.7500\n",
      "Epoch [894/1000], Train Loss: 13455517.0000, Val Loss: 3505113.7500\n",
      "Epoch [895/1000], Train Loss: 13455415.0000, Val Loss: 3505117.0000\n",
      "Epoch [896/1000], Train Loss: 13455672.0000, Val Loss: 3505124.5000\n",
      "Epoch [897/1000], Train Loss: 13455501.0000, Val Loss: 3505143.7500\n",
      "Epoch [898/1000], Train Loss: 13455500.0000, Val Loss: 3505143.0000\n",
      "Epoch [899/1000], Train Loss: 13455637.0000, Val Loss: 3505146.0000\n",
      "Epoch [900/1000], Train Loss: 13455723.0000, Val Loss: 3505143.7500\n",
      "Epoch [901/1000], Train Loss: 13455502.0000, Val Loss: 3505126.7500\n",
      "Epoch [902/1000], Train Loss: 13455278.0000, Val Loss: 3505119.0000\n",
      "Epoch [903/1000], Train Loss: 13455421.0000, Val Loss: 3505101.0000\n",
      "Epoch [904/1000], Train Loss: 13455572.0000, Val Loss: 3505079.5000\n",
      "Epoch [905/1000], Train Loss: 13455525.0000, Val Loss: 3505049.0000\n",
      "Epoch [906/1000], Train Loss: 13455534.0000, Val Loss: 3505053.5000\n",
      "Epoch [907/1000], Train Loss: 13455544.0000, Val Loss: 3505100.0000\n",
      "Epoch [908/1000], Train Loss: 13455658.0000, Val Loss: 3505148.7500\n",
      "Epoch [909/1000], Train Loss: 13455686.0000, Val Loss: 3505140.5000\n",
      "Epoch [910/1000], Train Loss: 13455565.0000, Val Loss: 3505117.0000\n",
      "Epoch [911/1000], Train Loss: 13455415.0000, Val Loss: 3505120.7500\n",
      "Epoch [912/1000], Train Loss: 13455416.0000, Val Loss: 3505140.0000\n",
      "Epoch [913/1000], Train Loss: 13455378.0000, Val Loss: 3505199.7500\n",
      "Epoch [914/1000], Train Loss: 13455534.0000, Val Loss: 3505178.7500\n",
      "Epoch [915/1000], Train Loss: 13455388.0000, Val Loss: 3505096.7500\n",
      "Epoch [916/1000], Train Loss: 13455378.0000, Val Loss: 3505046.5000\n",
      "Epoch [917/1000], Train Loss: 13455453.0000, Val Loss: 3505092.5000\n",
      "Epoch [918/1000], Train Loss: 13455258.0000, Val Loss: 3505188.0000\n",
      "Epoch [919/1000], Train Loss: 13455703.0000, Val Loss: 3505215.7500\n",
      "Epoch [920/1000], Train Loss: 13455452.0000, Val Loss: 3505147.7500\n",
      "Epoch [921/1000], Train Loss: 13455007.0000, Val Loss: 3505080.5000\n",
      "Epoch [922/1000], Train Loss: 13455031.0000, Val Loss: 3505103.7500\n",
      "Epoch [923/1000], Train Loss: 13455278.0000, Val Loss: 3505160.7500\n",
      "Epoch [924/1000], Train Loss: 13455182.0000, Val Loss: 3505173.7500\n",
      "Epoch [925/1000], Train Loss: 13455036.0000, Val Loss: 3505132.2500\n",
      "Epoch [926/1000], Train Loss: 13455040.0000, Val Loss: 3505100.5000\n",
      "Epoch [927/1000], Train Loss: 13455120.0000, Val Loss: 3505079.5000\n",
      "Epoch [928/1000], Train Loss: 13455206.0000, Val Loss: 3505093.2500\n",
      "Epoch [929/1000], Train Loss: 13455033.0000, Val Loss: 3505132.0000\n",
      "Epoch [930/1000], Train Loss: 13455278.0000, Val Loss: 3505130.7500\n",
      "Epoch [931/1000], Train Loss: 13455120.0000, Val Loss: 3505111.7500\n",
      "Epoch [932/1000], Train Loss: 13454755.0000, Val Loss: 3505114.7500\n",
      "Epoch [933/1000], Train Loss: 13454992.0000, Val Loss: 3505159.5000\n",
      "Epoch [934/1000], Train Loss: 13454967.0000, Val Loss: 3505166.7500\n",
      "Epoch [935/1000], Train Loss: 13455115.0000, Val Loss: 3505130.0000\n",
      "Epoch [936/1000], Train Loss: 13455140.0000, Val Loss: 3505106.5000\n",
      "Epoch [937/1000], Train Loss: 13454929.0000, Val Loss: 3505084.5000\n",
      "Epoch [938/1000], Train Loss: 13455184.0000, Val Loss: 3505109.2500\n",
      "Epoch [939/1000], Train Loss: 13455020.0000, Val Loss: 3505112.0000\n",
      "Epoch [940/1000], Train Loss: 13454756.0000, Val Loss: 3505105.2500\n",
      "Epoch [941/1000], Train Loss: 13454758.0000, Val Loss: 3505094.2500\n",
      "Epoch [942/1000], Train Loss: 13455185.0000, Val Loss: 3505121.7500\n",
      "Epoch [943/1000], Train Loss: 13454811.0000, Val Loss: 3505157.7500\n",
      "Epoch [944/1000], Train Loss: 13454936.0000, Val Loss: 3505152.2500\n",
      "Epoch [945/1000], Train Loss: 13455000.0000, Val Loss: 3505155.5000\n",
      "Epoch [946/1000], Train Loss: 13454910.0000, Val Loss: 3505174.0000\n",
      "Epoch [947/1000], Train Loss: 13455026.0000, Val Loss: 3505174.5000\n",
      "Epoch [948/1000], Train Loss: 13454855.0000, Val Loss: 3505142.5000\n",
      "Epoch [949/1000], Train Loss: 13454658.0000, Val Loss: 3505108.5000\n",
      "Epoch [950/1000], Train Loss: 13454998.0000, Val Loss: 3505112.7500\n",
      "Epoch [951/1000], Train Loss: 13454697.0000, Val Loss: 3505172.7500\n",
      "Epoch [952/1000], Train Loss: 13454986.0000, Val Loss: 3505195.7500\n",
      "Epoch [953/1000], Train Loss: 13454791.0000, Val Loss: 3505143.2500\n",
      "Epoch [954/1000], Train Loss: 13455025.0000, Val Loss: 3505071.7500\n",
      "Epoch [955/1000], Train Loss: 13454766.0000, Val Loss: 3505070.0000\n",
      "Epoch [956/1000], Train Loss: 13455207.0000, Val Loss: 3505125.2500\n",
      "Epoch [957/1000], Train Loss: 13454515.0000, Val Loss: 3505173.2500\n",
      "Epoch [958/1000], Train Loss: 13454960.0000, Val Loss: 3505159.2500\n",
      "Epoch [959/1000], Train Loss: 13454833.0000, Val Loss: 3505135.5000\n",
      "Epoch [960/1000], Train Loss: 13454593.0000, Val Loss: 3505143.5000\n",
      "Epoch [961/1000], Train Loss: 13454872.0000, Val Loss: 3505158.2500\n",
      "Epoch [962/1000], Train Loss: 13454690.0000, Val Loss: 3505131.5000\n",
      "Epoch [963/1000], Train Loss: 13454827.0000, Val Loss: 3505107.0000\n",
      "Epoch [964/1000], Train Loss: 13454803.0000, Val Loss: 3505135.0000\n",
      "Epoch [965/1000], Train Loss: 13454444.0000, Val Loss: 3505183.7500\n",
      "Epoch [966/1000], Train Loss: 13454688.0000, Val Loss: 3505168.2500\n",
      "Epoch [967/1000], Train Loss: 13454744.0000, Val Loss: 3505099.0000\n",
      "Epoch [968/1000], Train Loss: 13454948.0000, Val Loss: 3505058.7500\n",
      "Epoch [969/1000], Train Loss: 13454558.0000, Val Loss: 3505101.0000\n",
      "Epoch [970/1000], Train Loss: 13454568.0000, Val Loss: 3505170.7500\n",
      "Epoch [971/1000], Train Loss: 13454611.0000, Val Loss: 3505159.5000\n",
      "Epoch [972/1000], Train Loss: 13454752.0000, Val Loss: 3505123.0000\n",
      "Epoch [973/1000], Train Loss: 13454769.0000, Val Loss: 3505113.0000\n",
      "Epoch [974/1000], Train Loss: 13454531.0000, Val Loss: 3505144.0000\n",
      "Epoch [975/1000], Train Loss: 13454837.0000, Val Loss: 3505196.2500\n",
      "Epoch [976/1000], Train Loss: 13454388.0000, Val Loss: 3505195.2500\n",
      "Epoch [977/1000], Train Loss: 13454576.0000, Val Loss: 3505129.7500\n",
      "Epoch [978/1000], Train Loss: 13454444.0000, Val Loss: 3505088.7500\n",
      "Epoch [979/1000], Train Loss: 13454481.0000, Val Loss: 3505112.5000\n",
      "Epoch [980/1000], Train Loss: 13454729.0000, Val Loss: 3505168.0000\n",
      "Epoch [981/1000], Train Loss: 13454747.0000, Val Loss: 3505200.5000\n",
      "Epoch [982/1000], Train Loss: 13454800.0000, Val Loss: 3505205.5000\n",
      "Epoch [983/1000], Train Loss: 13454426.0000, Val Loss: 3505149.0000\n",
      "Epoch [984/1000], Train Loss: 13454438.0000, Val Loss: 3505118.7500\n",
      "Epoch [985/1000], Train Loss: 13454719.0000, Val Loss: 3505126.2500\n",
      "Epoch [986/1000], Train Loss: 13454563.0000, Val Loss: 3505158.0000\n",
      "Epoch [987/1000], Train Loss: 13454529.0000, Val Loss: 3505155.7500\n",
      "Epoch [988/1000], Train Loss: 13454200.0000, Val Loss: 3505142.7500\n",
      "Epoch [989/1000], Train Loss: 13454468.0000, Val Loss: 3505138.0000\n",
      "Epoch [990/1000], Train Loss: 13454615.0000, Val Loss: 3505123.2500\n",
      "Epoch [991/1000], Train Loss: 13454423.0000, Val Loss: 3505125.2500\n",
      "Epoch [992/1000], Train Loss: 13454156.0000, Val Loss: 3505100.0000\n",
      "Epoch [993/1000], Train Loss: 13454274.0000, Val Loss: 3505091.2500\n",
      "Epoch [994/1000], Train Loss: 13454241.0000, Val Loss: 3505123.0000\n",
      "Epoch [995/1000], Train Loss: 13454453.0000, Val Loss: 3505166.2500\n",
      "Epoch [996/1000], Train Loss: 13454484.0000, Val Loss: 3505188.0000\n",
      "Epoch [997/1000], Train Loss: 13454430.0000, Val Loss: 3505171.7500\n",
      "Epoch [998/1000], Train Loss: 13454316.0000, Val Loss: 3505161.5000\n",
      "Epoch [999/1000], Train Loss: 13454101.0000, Val Loss: 3505139.5000\n",
      "Epoch [1000/1000], Train Loss: 13454499.0000, Val Loss: 3505132.5000\n"
     ]
    }
   ],
   "source": [
    "# 初始化损失列表\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, Y_train)\n",
    "    # loss = custom_loss(output, Y_train)  # 如果使用自定义损失函数\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # 在验证集上评估模型\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, Y_val)\n",
    "        # val_loss = custom_loss(val_output, Y_val)  # 如果使用自定义损失函数\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T09:42:39.480892400Z",
     "start_time": "2024-07-17T04:55:04.910653300Z"
    }
   },
   "id": "d6c33bff53ebcf1b"
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Train Loss: 21720032.0000, Val Loss: 5433821.0000\n",
      "Epoch [2/1000], Train Loss: 21720878.0000, Val Loss: 5433821.0000\n",
      "Epoch [3/1000], Train Loss: 21719954.0000, Val Loss: 5433821.0000\n",
      "Epoch [4/1000], Train Loss: 21721182.0000, Val Loss: 5433821.0000\n",
      "Epoch [5/1000], Train Loss: 21721248.0000, Val Loss: 5433821.0000\n",
      "Epoch [6/1000], Train Loss: 21719896.0000, Val Loss: 5433821.0000\n",
      "Epoch [7/1000], Train Loss: 21721418.0000, Val Loss: 5433821.0000\n",
      "Epoch [8/1000], Train Loss: 21720572.0000, Val Loss: 5433821.0000\n",
      "Epoch [9/1000], Train Loss: 21723592.0000, Val Loss: 5433821.0000\n",
      "Epoch [10/1000], Train Loss: 21723378.0000, Val Loss: 5433821.0000\n",
      "Epoch [11/1000], Train Loss: 21720880.0000, Val Loss: 5433821.0000\n",
      "Epoch [12/1000], Train Loss: 21721544.0000, Val Loss: 5433821.0000\n",
      "Epoch [13/1000], Train Loss: 21719988.0000, Val Loss: 5433821.0000\n",
      "Epoch [14/1000], Train Loss: 21721168.0000, Val Loss: 5433821.0000\n",
      "Epoch [15/1000], Train Loss: 21720612.0000, Val Loss: 5433821.0000\n",
      "Epoch [16/1000], Train Loss: 21722988.0000, Val Loss: 5433821.0000\n",
      "Epoch [17/1000], Train Loss: 21721680.0000, Val Loss: 5433821.0000\n",
      "Epoch [18/1000], Train Loss: 21723640.0000, Val Loss: 5433821.0000\n",
      "Epoch [19/1000], Train Loss: 21720924.0000, Val Loss: 5433821.0000\n",
      "Epoch [20/1000], Train Loss: 21719040.0000, Val Loss: 5433821.0000\n",
      "Epoch [21/1000], Train Loss: 21722062.0000, Val Loss: 5433821.0000\n",
      "Epoch [22/1000], Train Loss: 21720492.0000, Val Loss: 5433821.0000\n",
      "Epoch [23/1000], Train Loss: 21723944.0000, Val Loss: 5433821.0000\n",
      "Epoch [24/1000], Train Loss: 21721950.0000, Val Loss: 5433821.0000\n",
      "Epoch [25/1000], Train Loss: 21723308.0000, Val Loss: 5433821.0000\n",
      "Epoch [26/1000], Train Loss: 21723232.0000, Val Loss: 5433821.0000\n",
      "Epoch [27/1000], Train Loss: 21718968.0000, Val Loss: 5433821.0000\n",
      "Epoch [28/1000], Train Loss: 21721524.0000, Val Loss: 5433821.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[209], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, Y_train)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# loss = custom_loss(output, Y_train)  # 如果使用自定义损失函数\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     14\u001B[0m train_losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\ECE6258\\lib\\site-packages\\torch\\_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    491\u001B[0m     )\n\u001B[1;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\ECE6258\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 初始化损失列表\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, Y_train)\n",
    "    # loss = custom_loss(output, Y_train)  # 如果使用自定义损失函数\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # 在验证集上评估模型\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, Y_val)\n",
    "        # val_loss = custom_loss(val_output, Y_val)  # 如果使用自定义损失函数\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T04:54:56.412346700Z",
     "start_time": "2024-07-17T04:46:19.553467100Z"
    }
   },
   "id": "9f85774aa636541e"
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10752\\1265932812.py:10: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# 绘制损失曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.legend()\n",
    "plt.savefig('Test_Result_non_zero_50/Test_Case_2/loss_curve_non_zero_50.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T13:44:40.669832100Z",
     "start_time": "2024-07-22T13:44:40.523458100Z"
    }
   },
   "id": "1adc8bc1dd956494"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 将test case结果输出到csv文件中"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9a9ad263cf139e8"
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 200, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10752\\1567859702.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  in_seq = np.array([int_to_array(int(row[i])) for i in range(1, 1 + in_seq_length)])  # 控制每个in_seq的长度\n"
     ]
    }
   ],
   "source": [
    "in_seq_length = 10\n",
    "df = pd.read_csv('Test_Instance/in_seq.csv')\n",
    "in_seqs = []\n",
    "for idx, row in df.iterrows():\n",
    "    in_seq = np.array([int_to_array(int(row[i])) for i in range(1, 1 + in_seq_length)])  # 控制每个in_seq的长度\n",
    "    in_seqs.append(in_seq)\n",
    "in_seqs = np.array(in_seqs)\n",
    "in_seqs = np.transpose(in_seqs, (1, 0, 2))\n",
    "print(in_seqs.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T13:44:55.924786Z",
     "start_time": "2024-07-22T13:44:55.833208600Z"
    }
   },
   "id": "64749c84b34ab5d6"
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 200, 5, 2, 60])\n"
     ]
    }
   ],
   "source": [
    "test_data = torch.tensor(in_seqs, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    test_output = model(test_data)\n",
    "print(test_output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T13:45:00.058140300Z",
     "start_time": "2024-07-22T13:44:59.980484900Z"
    }
   },
   "id": "ddf163d93e0586ef"
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "test_output_np = test_output.numpy().flatten()\n",
    "for k in range(10):\n",
    "    df = pd.DataFrame(test_output_np[120000*k:120000*(k+1)])\n",
    "    filename = f'Test_Result_non_zero_50/Test_Case_2/test_output_{k+1:03}.csv'\n",
    "    df.to_csv(filename, index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T13:45:02.061886100Z",
     "start_time": "2024-07-22T13:45:00.501436300Z"
    }
   },
   "id": "7bac46013871d0a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 将test case求解结果输出到csv文件"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5c8c2dbbbf4a1c9"
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 200, 5, 2, 60)\n"
     ]
    }
   ],
   "source": [
    "out_seq_length = in_seq_length\n",
    "out_seqs = []\n",
    "for i in range(out_seq_length):\n",
    "    file_path = f\"Test_Instance/Npy_Output/result_{i+1:03}.npy\"\n",
    "    target_shape = (600, 2, 60)\n",
    "    data = load_and_pad_npy(file_path, target_shape)\n",
    "    order = 0\n",
    "    out_seq = []\n",
    "    # 根据 in_seqs中的值决定加载的数据\n",
    "    for j in range(200):\n",
    "        tmp_seq = []\n",
    "        for k in range(5):\n",
    "            tmp = in_seqs[i, j, k]\n",
    "            if tmp == 1:\n",
    "                # tmp_seq.append(torch.tensor(data[order]))\n",
    "                tmp_seq.append(data[order])\n",
    "                order = order + 1\n",
    "            else:\n",
    "                # tmp_seq.append(torch.tensor(np.zeros((2, 60))))\n",
    "                tmp_seq.append(np.zeros((2, 60)))\n",
    "        out_seq.append(tmp_seq)\n",
    "    out_seqs.append(out_seq)\n",
    "# out_seqs = np.array(out_seqs, dtype=object)\n",
    "out_seqs = np.array(out_seqs)\n",
    "print(out_seqs.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T13:45:10.189577500Z",
     "start_time": "2024-07-22T13:45:10.131857100Z"
    }
   },
   "id": "323825b6675692d1"
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000,)\n",
      "(120000,)\n",
      "(120000,)\n",
      "(120000,)\n",
      "(120000,)\n",
      "(120000,)\n",
      "(120000,)\n",
      "(120000,)\n",
      "(120000,)\n",
      "(120000,)\n"
     ]
    }
   ],
   "source": [
    "for k in range(in_seq_length):\n",
    "    file_path1 = f\"Test_Result_non_zero_50/Test_Case_2/test_output_{k+1:03}.csv\"\n",
    "    existing_df = pd.read_csv(file_path1, header=None)\n",
    "    file_path2 = f\"Test_Instance/Npy_Output/result_{k+1:003}.npy\"\n",
    "    target_shape = (600, 2, 60)\n",
    "    data = load_and_pad_npy(file_path2, target_shape)\n",
    "    out_seq = np.array(out_seqs[k]).flatten()\n",
    "    print(out_seq.shape)\n",
    "    existing_df[1] = out_seq\n",
    "    existing_df.to_csv(file_path1, header=False, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T13:45:31.779669800Z",
     "start_time": "2024-07-22T13:45:29.174647500Z"
    }
   },
   "id": "6a4942c2f2d636c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "712ac57591b4b396"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
